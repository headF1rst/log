# JustAnotherBlog - Complete Content

> This file contains all blog posts in full markdown format.
> Optimized for AI systems and language models.
> Generated: 2026-02-05T13:27:26.068Z

---


# Korean (한국어) Posts

## Netty 내부 동작 원리로 파헤친 WebClient 초기 지연 이슈

- **URL**: https://headf1rst.github.io/log/ko/post33
- **Date**: 2025-12-26
- **Tags**: `Netty`

### Content


물류 인프라를 보유하고 있는 회사들은 3PL이라는 서비스를 제공합니다. 3PL이란 물류 인프라를 갖춘 회사가 그렇지 못한 판매처로부터 배송 업무를 위탁받아 제공하는 서비스를 말합니다.

판매처는 배송이 필요한 주문 목록을 3PL 시스템에 등록하게 되는데, 이 과정에서 입력된 주문이 유효한 주문인지 확인하기 위해서 여러 시스템과 소통하게 됩니다.

외부 API 호출을 위한 도구로는 WebClient를 사용하고 있었는데요. 아래 지표에서 보듯이, 외부 API 호출까지의 지연 시간이 원인을 알 수 없이 길어지는 현상이 '간헐적'으로 발견되었습니다.

![APM](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/yb2pzcn03325b8pk9z9p.png)

이번 포스트에서는 해당 현상의 원인을 파악하며 알게 된 WebClient의 내부 동작 원리와 Reactor Netty의 아키텍처, 그리고 해결책을 공유하고자 합니다.

## 원인 파악을 위한 가정

지연이 발생한다는 것은 요청이 어딘가에서 즉시 처리되지 못하고 대기하고 있었을 가능성이 높다는 의미입니다.

이전 사진에서 빨간 박스로 표시된 메서드는 객체 생성과 WebClient 호출만을 담당하고 있었습니다. WebClient 내부의 어느 처리 단계에서 병목이 발생할 수 있는지 명확히 식별하려면 아키텍처에 대한 이해가 선행되어야 했습니다. 이를 위해 WebClient의 요청 처리 방식과 Reactor Netty의 아키텍처를 분석했습니다.

## WebClient 실행 메커니즘

먼저 문제가 발생한 코드의 구조를 살펴보겠습니다.

**OrderRegistrationService.java**

```java
private final static int CONCURRENCY_CALL = 10;

List<RefineResult> results = Flux.fromIterable(registerDtos)
    .flatMap(dto -> Mono.defer(() ->
       omsService.refineAddress(dto.getPrimaryAddress(), dto.getSecondaryAddress())
       .defaultIfEmpty(ApiResponse.failure("NO_RESPONSE", false))
    ).map(resp -> new RefineResult(dto, resp)), CONCURRENCY_CALL)
    .collectList()
    .block();
```

**OmsService.java**

```java
@Override
public Mono<ApiResponse<RefineAddressDto>> refineAddress(String primaryAddress, String secondaryAddress) {
    RefineAddressInput request = RefineAddressInput.builder()
        .primaryAddress(primaryAddress)
        .secondaryAddress(secondaryAddress)
        .build();

    return omsClient.post()
        .uri("/refine-address")
        .bodyValue(request)
        .retrieve()
        .bodyToMono(RefineAddressOutput.class)
        .timeout(Duration.ofSeconds(5))
        .retryWhen(RetryPolicy.fixedDelay(1, Duration.ofMillis(100), "[OMS 주소정제] 재시도 요청: " + request))
        .map(output -> output.isSuccess() ? ApiResponse.success(RefineAddressDto.from(output))
            : ApiResponse.<RefineAddressDto>failure("응답 결과에 데이터가 없음", false))
        .onErrorResume(ex -> ExternalErrorHandler.handleError(ex, extractOmsErrorMessage(ex), "OMS 주소정제"));
}
```

## Cold Sequence와 구독 시점

위 코드에서 실제 HTTP 요청이 언제 발생하는지 이해하려면 WebClient의 Cold Sequence 특성을 먼저 이해해야 합니다.

WebClient의 리액티브 체인은 Cold Sequence로 동작합니다. 구독이 발생하기 전까지는 파이프라인만 정의될 뿐, 실제 실행은 일어나지 않습니다. HTTP 요청 발송 시점은 `subscribe()`가 호출되는 순간이며, 코드상의 `block()`이 내부적으로 이를 트리거합니다.

```java
List<RefineResult> results = Flux.fromIterable(registerDtos)
    .flatMap(dto -> Mono.defer(() -> ...))
    .collectList()
    .block();  // ← 구독 시작점
```

`block()`의 구독 신호는 **역방향(upstream)**으로 전파됩니다

```text
block() → collectList() → flatMap() → Mono.defer() → WebClient 체인
```

`flatMap(Function, int concurrency)`은 인자로 전달된 concurrency 수 만큼의 Mono를 동시에 구독합니다. `Mono.defer()`는 각 구독 시점마다 내부 람다를 실행하여 새로운 Mono를 생성하므로, 각 DTO마다 독립적인 HTTP 요청 파이프라인이 생성됩니다.

```java
// 구독될 때마다 새로운 WebClient 체인 생성
Mono.defer(() -> omsService.refineAddress(...))
```

## TaskQueue로의 전달

`omsService.refineAddress(...)`가 반환하는 Mono가 구독되면 요청 설정을 빌드하고, `.retrieve()` 이후 체인이 구독되면서 쓰기 요청이 **TaskQueue**에 저장됩니다. 

```java
omsClient.post()
    .uri("/refine-address")
    .bodyValue(request)
    .retrieve()
    .bodyToMono(RefineAddressOutput.class)
```

POST 요청이 NioEventLoop의 TaskQueue에 저장되면, WebClient를 호출한 스레드의 역할은 여기서 끝납니다. 이후 작업은 EventLoop 스레드가 담당합니다.

## Netty EventLoop 스레드의 동작 원리

WebClient의 HTTP 요청이 TaskQueue에 저장되는 이유는 Netty의 **이벤트 루프 기반 비동기 처리 모델** 때문입니다. 이 모델을 이해하려면 먼저 네트워크 통신의 기본 개념을 짚고 넘어가야 합니다.

### User Space와 Kernel Space

서로 다른 머신의 애플리케이션이 통신하려면 시스템 콜로 유저 모드와 커널 모드를 오가며 커널 내 소켓 버퍼에 데이터를 읽거나 써야 합니다.

![Web protocol](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qjtark4qo45ybfnj0lwk.png)

소켓 버퍼에 데이터를 어떻게 읽고 쓰느냐에 따라 Blocking I/O와 Non-blocking I/O로 나뉩니다. 둘의 차이는 스레드가 시스템 콜 후 응답을 기다리는지 여부입니다.

- `Blocking I/O`: 데이터가 준비될 때까지 스레드가 대기
- `Non-blocking I/O`: 데이터가 없으면 즉시 반환, 스레드는 다른 작업 수행 가능

효율적인 Non-blocking I/O를 구현하려면 특정 이벤트를 등록해 놓고 해당 이벤트가 발생했을 때만 처리하는 방식이 필요합니다. 이렇게 하면 하나의 스레드로 여러 채널을 관리할 수 있습니다.

### Multiplexing I/O와 Selector

이벤트 기반 소켓 통신에서는 **하나의 Selector가 여러 소켓 채널의 변화를 감지**하며 이벤트가 발생했을 때만 처리합니다. 이를 `Multiplexing I/O`라고 합니다.

![Multiplexing I/O](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7sqinsru5nse3xrks1f3.png)

Linux에서 이 Multiplexing I/O는 `epoll` 시스템 콜로 구현됩니다. Java NIO의 Selector는 내부적으로 이 epoll을 사용합니다.

## Selector.select()의 실제 동작

OS 커널이 능동적으로 I/O 이벤트를 Selector에 알려주는 것처럼 보이지만, 실제로는 그렇지 않습니다.

Selector.select()가 호출되면 유저 모드에서 커널 모드로 전환되고, 내부적으로 `epoll_wait()` 시스템 콜이 호출되면서 호출 스레드는 커널에서 블로킹 상태로 대기합니다.

![How Selector work](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rokpi9s20m4thyx0akjp.png)

`epoll_wait`을 호출하면, OS 커널은 이전에 `epoll_ctl`로 등록된 파일 디스크립터(소켓)들을 모니터링하다가, 네트워크 카드에 데이터가 도착하거나 소켓 버퍼에 쓰기가 가능해지는 등의 I/O 이벤트가 발생하면 이를 감지합니다. I/O가 발생한 소켓은 커널 내 Ready Queue에 추가되고, `epoll_wait()`이 반환되어 대기 중이던 스레드가 깨어납니다.

즉, User Space가 커널에 요청하고 시스템 콜로 응답받는 pull 구조입니다.

`select()` 자체는 블로킹 호출이지만, 하나의 스레드가 여러 소켓을 감시하고 이벤트가 발생한 소켓들만 골라서 처리합니다. 따라서 각 소켓 입장에서는 전용 스레드 없이도 비동기적으로 처리되는 것과 같은 효과를 얻게 됩니다.

## NioEventLoop의 구조

이제 Netty의 EventLoop가 Selector를 어떻게 활용하는지 살펴보겠습니다.

EventLoop의 구현체인 NioEventLoop는 `1 Thread + 1 Selector + 1 TaskQueue`로 구성됩니다.

![NioEventLoop Structure](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c1xl9zcj1h0pk3eml8sr.png)

EventLoop 스레드는 기본적으로 CPU 코어 수만큼 생성됩니다. `Math.max(Runtime.getRuntime().availableProcessors(), 4)`

각 EventLoop 스레드는 전용 NioEventLoop 인스턴스를 실행하며, 단일 스레드가 무한 루프를 돌면서 두 가지 작업을 수행합니다.

1. I/O 이벤트 처리 (네트워크 읽기/쓰기)
2. TaskQueue의 작업 처리 (사용자가 등록한 Runnable)

```java
// 개념적인 코드
while (true) {
    // 1. 네트워크에서 뭔가 일어났는지 확인
    네트워크_이벤트_확인();
    // 2. 일어난 일들 처리
    이벤트들_처리();
    // 3. 누가 시켜놓은 작업들 처리
    작업큐에서_작업꺼내서_실행();
}
```

실제 Netty 코드를 보면 (Netty 4.2 기준)

```java
// SingleThreadIoEventLoop.java:153-164
protected void run() {
    do {
        runIo();                    // ← 1+2: I/O 확인 및 처리
        runAllTasks(maxTasksPerRun); // ← 3: 작업큐 처리
    } while (!confirmShutdown());
}
```
 
`runIo()`는 내부적으로 `NioIoHandler.run()`을 호출합니다.

```java
// NioIoHandler.java:420-485
public int run(IoExecutionContext runner) {
    // 1단계: select - I/O 이벤트 존재 여부 확인
    select(runner, wakenUp.getAndSet(false));

    // 2단계: 있으면 처리
    return processSelectedKeys();
}
```

이제 각 단계를 자세히 살펴보겠습니다.

### 1. select() - I/O 이벤트 감지

EventLoop는 I/O 이벤트 처리와 TaskQueue에 쌓인 작업 처리, 두 가지 역할을 수행합니다. 이때 `Selector.select()`를 사용하여 처리할 I/O 이벤트가 있는지 확인합니다.

`select()` 메서드는 TaskQueue에 작업이 존재하는지 여부에 따라 적절한 select 방식을 결정합니다.

```java
// NioIoHandler.java
private void select(IoExecutionContext runner, boolean oldWakenUp) {
    Selector selector = this.selector;

    for (;;) {
        // 태스크가 있으면 즉시 확인하고 넘어감
        if (!runner.canBlock() && wakenUp.compareAndSet(false, true)) {
            selector.selectNow();   // 작업 있으면 바로 확인
            break;
        }

        // 태스크가 없으면 이벤트 올 때까지 대기
        int selectedKeys = selector.select(timeoutMillis);
    }
}
```

```java
@Override
public boolean canBlock() {
   assert inEventLoop();
   return !hasTasks() && !hasScheduledTasks();
}
```

#### TaskQueue가 비었을 때

`TaskQueue`가 비어있으면 Netty는 `select(timeout)`을 호출하여 커널로 부터 I/O 이벤트 신호를 받거나 타임아웃이 될 때까지 블로킹 상태로 대기하여 CPU 사용을 줄입니다.

만약 대기 중 `TaskQueue`에 새 task가 들어오면, `wakeup` 메커니즘을 통해 `select()`의 블로킹을 깨워서 즉시 반환시키고, 루프를 돌며 TaskQueue를 처리할 수 있게 합니다.

#### TaskQueue가 있을 때

`TaskQueue`에 작업이 있으면 `selectNow()`를 호출하여 I/O 이벤트가 있는지 빠르게 확인하고, 곧바로 테스크 실행으로 넘어가 작업 지연을 줄입니다.

만약 TaskQueue에 작업이 있는 상황에서 `select(timeout)`을 호출해 블로킹되면, EventLoop 스레드가 잠들어 테스크 처리가 지연되고 응답성이 떨어지게 됩니다. 반대로 `selectNow()`만 계속 수행하면 준비된 I/O 이벤트가 없어도 계속 확인하므로 불필요한 반복으로 busy-wait(CPU 낭비)이 발생할 수 있습니다.

즉, Netty의 `select()`는 상황에 따라 적절한 방식을 선택하여 CPU를 낭비하지 않고 효율적으로 I/O 이벤트를 대기합니다.

#### select() 호출 이후의 내부 동작

앞서 Netty가 상황에 따라 `select(timeout)` 또는 `selectNow()`를 선택적으로 호출한다는 것을 살펴보았습니다. 이제 이 호출이 실제로 어떤 과정을 거쳐 커널까지 도달하고, 다시 돌아오는지 살펴보겠습니다.

`Selector.select()`를 호출하면 JDK 내부의 `SelectorImpl` 클래스가 이를 처리합니다.

```java
// SelectorImpl.java
@Override
    public final int select(long timeout) throws IOException {
        return lockAndDoSelect(null, (timeout == 0) ? -1 : timeout);
    }
```

`lockAndDoSelect()`는 동기화를 수행한 뒤 Multiplexing I/O를 담당하는 `doSelect()`를 호출합니다.

```java
// SelectorImpl.java
private int lockAndDoSelect(Consumer<SelectionKey> action, long timeout)
      throws IOException
{
    synchronized (this) {
        ensureOpen();
        if (inSelect)
            throw new IllegalStateException("select in progress");
        inSelect = true;
        try {
            synchronized (publicSelectedKeys) {
                return doSelect(action, timeout);
            }
        } finally {
            inSelect = false;
        }
    }
}
```

여기서 `doSelect()`는 추상 메서드입니다. 운영체제마다 효율적인 Multiplexing I/O 메커니즘이 다르기 때문에, JDK는 플랫폼별로 다른 구현체를 제공합니다.

| OS      | 구현 클래스            | 시스템 콜      |
|---------|---------------------|--------------|
| Linux   | EPollSelectorImpl   | epoll_wait() |
| macOS   | KQueueSelectorImpl  | kevent()     |
| Windows | WindowsSelectorImpl | IOCP         |

이 글에서는 서버 환경에서 가장 많이 사용되는 **Linux의 epoll 기반 구현**을 중심으로 살펴보겠습니다. (JDK 21 기준)

### EPollSelectorImpl 인스턴스는 언제 생성되는가?

EPollSelectorImpl 인스턴스는 `Selector.open()` 호출 시점에 초기화됩니다.

1. 애플리케이션에서 new NioEventLoopGroup(n) 호출
2. 내부적으로 n개의 NioIoHandler 생성
3. 각 NioIoHandler 생성자에서 provider.openSelector() 호출
4. Linux 환경에서는 EPollSelectorImpl 인스턴스 생성

EPollSelectorImpl 생성 시 다음과 같은 초기화가 이루어집니다.

```java
EPollSelectorImpl(SelectorProvider sp) throws IOException {
   super(sp);

   // 1. epoll 인스턴스 생성 (epoll_create 시스템 콜)
   this.epfd = EPoll.create();

   // 2. epoll_wait 결과를 저장할 네이티브 메모리 할당
   this.pollArrayAddress = EPoll.allocatePollArray(NUM_EPOLLEVENTS);

   // 3. wakeup용 EventFD 생성
   this.eventfd = new EventFD();
   IOUtil.configureBlocking(IOUtil.newFD(eventfd.efd()), false);

   // 4. EventFD를 epoll에 EPOLLIN으로 등록
   EPoll.ctl(epfd, EPOLL_CTL_ADD, eventfd.efd(), EPOLLIN);
}
```

즉, 하나의 EventLoop마다 하나의 epoll 인스턴스가 매핑됩니다.

#### epoll의 세 가지 시스템 콜

epoll은 세 가지 시스템 콜을 제공합니다

- `epoll_create`: epoll 인스턴스(채널 감시 저장소) 생성
- `epoll_ctl`: 감시할 FD 추가/수정/삭제
- `epoll_wait`: 이벤트(read/write)가 발생할 때까지 대기하고, 이벤트가 발생한 FD 목록을 반환

JDK의 `EPoll.wait()`는 JNI를 통해 커널의 `epoll_wait()` 시스템 콜을 직접 호출합니다.

`epoll_wait()`는 미리 할당된 네이티브 메모리의 `epoll_event` 구조체 배열에 준비된 이벤트 정보를 채우고, 준비된 이벤트 개수를 반환합니다. 이 배열에는 각 FD와 발생한  이벤트 타입(EPOLLIN/EPOLLOUT/EPOLLERR 등)이 담겨 있습니다.

EPollSelectorImpl.doSelect()에서 이 메서드들이 실제로 호출되는 흐름을 보면:

```java
// EpollSelectorImpl.java
@Override
protected int doSelect(Consumer<SelectionKey> action, long timeout) throws IOException {
    int to = (int) Math.min(timeout, Integer.MAX_VALUE);

    int numEntries;
    processUpdateQueue();      // epoll_ctl로 관심 이벤트 변경 반영
    processDeregisterQueue();

    try {
        begin(blocking);
        // epoll_wait 시스템 콜 호출
        numEntries = EPoll.wait(epfd, pollArrayAddress, NUM_EPOLLEVENTS, to);
    } finally {
        end(blocking);
    }

    // 반환된 이벤트 처리
    return processEvents(numEntries, action);
}
```

### 2. processSelectedKeys() - I/O 이벤트 처리

`EPoll.wait()`가 이벤트 개수를 반환하면, `EPollSelectorImpl.processEvents()`가 해당 개수만큼 이벤트 배열을 순회하며 각 FD에 연결된 SelectionKey를 찾아 selectedKeys에 추가합니다. 이후 Netty의 `NioIoHandler.processSelectedKeys()`가 `seletedKeys`를 순회하며 각 채널의 이벤트를 처리합니다.

```java
 // NioIoHandler.java
  private int processSelectedKeysOptimized() {
      int handled = 0;
      for (int i = 0; i < selectedKeys.size; ++i) {
          SelectionKey k = selectedKeys.keys[i];
          selectedKeys.keys[i] = null;  // GC를 위해 null 처리

          processSelectedKey(k);  // 각 이벤트 처리
          ++handled;
      }
      return handled;
  }
```

```java
  private void processSelectedKey(SelectionKey k) {
      final DefaultNioRegistration registration = (DefaultNioRegistration) k.attachment();

      // 준비된 이벤트를 핸들러에 전달
      // OP_READ  → 데이터 수신
      // OP_WRITE → 데이터 송신
      // OP_CONNECT → 연결 완료
      // OP_ACCEPT → 새 연결 요청
      registration.handle(k.readyOps());
  }
```

`registration.handle()`은 내부적으로 `AbstractNioChannel.AbstractNioUnsafe.handle()`을 호출합니다. 이 메서드는 이벤트 타입에 따라 적절한 처리를 수행합니다.

```java
// AbstractNioChannel.java:420-450
@Override
public void handle(IoRegistration registration, IoEvent event) {
    NioIoOps nioReadyOps = ((NioIoEvent) event).ops();

    // 1. OP_CONNECT: 연결 완료 처리 (가장 먼저 처리)
    if (nioReadyOps.contains(NioIoOps.CONNECT)) {
        removeAndSubmit(NioIoOps.CONNECT);
        unsafe().finishConnect();
    }

    // 2. OP_WRITE: 쓰기 가능 상태 - 대기 중인 버퍼 전송
    if (nioReadyOps.contains(NioIoOps.WRITE)) {
        forceFlush();
    }

    // 3. OP_READ / OP_ACCEPT: 데이터 수신 또는 새 연결 수락
    if (nioReadyOps.contains(NioIoOps.READ_AND_ACCEPT) || nioReadyOps.equals(NioIoOps.NONE)) {
        read();
    }
}
```

### 3. runAllTasks() - Non-I/O Task 처리

I/O 이벤트 처리가 끝나면 runAllTasks()가 호출되어 TaskQueue에 쌓인 작업들을 처리합니다. WebClient의 HTTP 요청도 바로 이 단계에서 실제로 전송됩니다.

```java
// SingleThreadEventExecutor.java
protected boolean runAllTasks(long timeoutNanos) {
     // 스케줄 큐에서 실행 가능한 태스크를 TaskQueue로 이동
     fetchFromScheduledTaskQueue();
     Runnable task = pollTask();

     final long deadline = timeoutNanos > 0 ? getCurrentTimeNanos() + timeoutNanos : 0;
     long runTasks = 0;

     for (;;) {
         safeExecute(task); // 테스크 실행

         runTasks ++;

         task = pollTask();
         if (task == null) {
             lastExecutionTime = getCurrentTimeNanos();
             break;
          }
      }

     afterRunningAllTasks();
     return true;
}
```

### 전체 흐름 요약

지금까지 살펴본 내용을 하나의 다이어그램으로 정리하면 다음과 같습니다.

![overall](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/s5f9g86wsuwo500ky37d.png)

병목이 발생한 인스턴스의 vCPU 수는 2개였습니다. Netty의 EventLoop 스레드 수는 기본적으로 `Math.max(availableProcessors(), 4)`로 결정되므로, 이 환경에서는 EventLoop 스레드가 **총 4개** 존재합니다.

지금까지 살펴본 바와 같이 Netty의 EventLoop는 Multiplexing I/O 방식으로 동작하기 때문에 적은 수의 스레드로도 많은 동시 요청을 처리할 수 있습니다. `epoll_wait()`은 수천 개의 채널이 등록되어 있어도 실제로 I/O 이벤트가 발생한 채널만 반환하므로, 동시 요청 수가 EventLoop 스레드 수보다 많다고 해서 병목이 발생하지는 않습니다.

따라서 **"동시 요청 10개 > EventLoop 스레드 4개"는 병목의 원인이 아닙니다.**

### 또 다른 가설: Parallel Scheduler 스레드 경합

그렇다면 무엇이 문제였을까요? 문제가 발생한 코드를 다시 살펴보겠습니다.

```java
return omsClient.post()
   .uri("/refine-address")
   .bodyValue(request)
   .retrieve()
   .bodyToMono(RefineAddressOutput.class)
   .timeout(Duration.ofSeconds(5)) // ← 여기
   .retryWhen(RetryPolicy.fixedDelay(1, Duration.ofMillis(100), "...")) // ← 여기
   // ...
```

`timeout()`과 `retryWhen()`의 fixedDelay()는 내부적으로 **Schedulers.parallel()**을 사용하여 타이머를 스케줄링합니다. 그리고 Parallel Scheduler의 스레드 수 역시 CPU 코어 수에 비례합니다.

vCPU 2개 환경에서 동시 요청 수가 10개인 환경에서 할당된 Parallel Scheduler 스레드 수보다 많은 요청을 처리하면서 병목이 발생했을 가능성이 있습니다.

## 검증

실제로 CPU 코어 수 제한이 WebClient 동시 호출 성능에 미치는 영향을 측정하기 위해 `JMH(Java Microbenchmark Harness)`를 사용하여 벤치마크를 수행했습니다.

### 테스트 환경

**공통 설정**

- 동시 호출 수 (Concurrency): 10
- 총 요청 수 (totalRequests): 50
- 서버 응답 지연 (serverLatencyMs): 200m
- 측정 방식: 10회 반복 측정 후 평균값 산출

### Case 1: CPU 코어 10개 (refineAddress_concurrency10_cpu10)

- JVM 옵션: `-XX:ActiveProcessorCount=10`
- Available Processors: 10
- Reactor Schedulers DefaultPoolSize: 10

![Benchmark result1](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/izmlmaiex478fbxj28hm.png)

![Benchmark result2](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/lnz16df329xde31f8r3k.png)

### Case 2: CPU 코어 2개 (refineAddress_concurrency10_cpu2)

- JVM 옵션: `-XX:ActiveProcessorCount=2`
- Available Processors: 2
- Reactor Schedulers DefaultPoolSize: 2

![Benchmark result3](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ma6r5otdoiguigptt1y8.png)

![Benchmark result4](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/d26n2gg4dthjtbew5m8c.png)

### 결과

![JMH Benchmark result](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/640a2hz9kqcccv395blw.png)

| 지표    | CPU 10 코어            | CPU 2 코어      |
|---------|---------------------|--------------|
| 평균 처리 시간   | 1035.498 ms   | 1049.652 ms |
| 안정성 (Stdev)   | 7.496  | 18.554  |
| 신뢰구간 폭 | 22.667 ms | 56.103 ms         |
| Reactor 워커 수   | 10  |  2  |

예상과 달리, CPU 코어 수(Parallel Scheduler Pool Size)에 따른 처리 시간 차이는 약 14ms(1.4%) 로 거의 없었습니다.

스레드 수가 2개에서 10개로 증가해도 성능 향상이 미미합니다. timeout/retry의 스케줄링 자체는 매우 가벼운 작업이므로, 스레드 수가 적어도 큰 영향을 주지 않는것을 확인할 수 있었습니다.

**벤치 마크 코드**

```java
public class RunBenchmark {

    public static void main(String[] args) throws RunnerException {

        Options options = new OptionsBuilder()
            .include(OmsWebClientConcurrencyBenchmark.class.getSimpleName())
            .forks(1)
            .warmupIterations(10) // 각 벤치 마크 실행 전 최적화를 위한 웜업
            .warmupTime(TimeValue.seconds(5))
            .measurementIterations(10) // 각 벤치마크 측정. 10회 반복
            .measurementTime(TimeValue.seconds(5))
            .timeUnit(TimeUnit.MILLISECONDS)
            .addProfiler(JavaFlightRecorderProfiler.class)  // JFR 프로파일러 추가
            .resultFormat(ResultFormatType.JSON)
            .result("jmh-results.json")
            .build();

        new Runner(options).run();
    }
}
```

```java
@BenchmarkMode(Mode.AverageTime)
public class OmsWebClientConcurrencyBenchmark {

    private static final int CONCURRENCY_CALL = 10;

    @State(Scope.Benchmark)
    public static class BenchState {

        @Param({"50"})
        public int totalRequests; // 한 번의 벤치마크 호출당 요청 총 개수

        @Param({"200"})
        public int serverLatencyMs; // 목 서버가 응답 전 인위적으로 대기할 지연(ms)

        private DisposableServer server;
        private OmsService omsService;

        @Setup(Level.Trial)
        public void setup() {
            int availableProcessors = Runtime.getRuntime().availableProcessors();
            String reactorPoolSize = System.getProperty("reactor.schedulers.defaultPoolSize", String.valueOf(availableProcessors));
            System.out.println("\n========================================");
            System.out.println("Benchmark started: OMS WebClient concurrency(10) under different CPU core caps");
            System.out.println("Params: totalRequests=" + totalRequests + ", serverLatencyMs=" + serverLatencyMs);
            System.out.println("Available processors (current JVM): " + availableProcessors);
            System.out.println("Reactor Schedulers DefaultPoolSize: " + reactorPoolSize);
            System.out.println("========================================");

            // 목 응답 JSON
            String json = "{" +
                "\"jibeonAddress\":{\"primaryAddress\":\"서울시 강남구\",\"secondaryAddress\":\"테스트로 123\",\"zipCode\":\"06200\"}," +
                "\"roadAddress\":{\"primaryAddress\":\"서울시 강남구\",\"secondaryAddress\":\"테스트로 123\",\"zipCode\":\"06200\"}," +
                "\"sigungu\":\"강남구\",\"dong\":\"역삼동\",\"provider\":\"OMS\",\"hcode\":\"11110\",\"bcode\":\"1111010100\",\"buildingNumber\":\"123\"" +
                "}";

            this.server = HttpServer.create()
                .host("127.0.0.1")
                .port(0)
                .route(routes -> routes.post("/refine-address", (req, res) ->
                    res.header("Content-Type", "application/json")
                        .sendString(Mono.delay(Duration.ofMillis(serverLatencyMs))
                            .then(Mono.just(json)))))
                .bindNow();

            int port = server.port();
            WebClient omsClient = WebClient.builder().baseUrl("http://127.0.0.1:" + port).build();
            WebClient fbkClient = WebClient.builder().baseUrl("http://127.0.0.1:" + port).build();
            this.omsService = new OmsServiceImpl(omsClient, fbkClient);
        }

        @TearDown(Level.Trial)
        public void tearDown() {
            if (server != null) {
                server.disposeNow();
            }
        }
    }

    private List<ApiResponse<RefineAddressDto>> invokeRefineAddressBatch(OmsService omsService, int totalRequests) {
        return Flux.range(0, totalRequests)
            .flatMap(i -> Mono.defer(() -> omsService.refineAddressIgnore("서울시 강남구", "테스트로 123")
                .defaultIfEmpty(ApiResponse.failure("NO_RESPONSE", false))), CONCURRENCY_CALL)
            .collectList()
            .block();
    }

    // 코어 수 2로 실행
    @Benchmark
    @Fork(value = 1, jvmArgsAppend = {
        "-XX:ActiveProcessorCount=2",
        "-XX:FlightRecorderOptions=threadbuffersize=16k,globalbuffersize=10m,memorysize=50m",
        "-XX:StartFlightRecording=settings=profile"
    })
    public List<ApiResponse<RefineAddressDto>> refineAddress_concurrency10_cpu2(BenchState state, Blackhole blackhole) {
        var result = invokeRefineAddressBatch(state.omsService, state.totalRequests);
        blackhole.consume(result);
        return result;
    }

    // 코어 수 10으로 실행
    @Benchmark
    @Fork(value = 1, jvmArgsAppend = {
        "-XX:ActiveProcessorCount=10",
        "-XX:FlightRecorderOptions=threadbuffersize=16k,globalbuffersize=10m,memorysize=50m",
        "-XX:StartFlightRecording=settings=profile"
    })
    public List<ApiResponse<RefineAddressDto>> refineAddress_concurrency10_cpu10(BenchState state, Blackhole blackhole) {
        var result = invokeRefineAddressBatch(state.omsService, state.totalRequests);
        blackhole.consume(result);
        return result;
    }
}
```

## 지연의 원인: Cold Start

원인을 분석하는 동안 유사한 지표를 가진 Trace를 추가로 수집할 수 있었는데요. 지표에서 한 가지 공통된 패턴을 발견할 수 있었습니다.

지연이 발생한 모든 케이스가 애플리케이션 배포 직후 첫 번째 외부 API 호출 시점에 발생한다는 사실을 발견했습니다.

Netty의 리소스는 **Lazy Initialization** 방식으로 동작합니다. 즉, `WebClient`를 생성하는 시점이 아니라 **실제로 첫 번째 HTTP 요청을 보내는 시점**에 초기화가 이루어집니다.

## Cold Start 해결책

두 가지 해결책을 검토했습니다.

1. `Connection Pool에 사전 커넥션 맺기`: 미리 연결을 생성해 대기
2. `Warmup 옵션`: 리소스를 사전 로드하되 실제 연결은 필요 시점에 생성

### Connection Pool에 사전 커넥션 생성

처음에는 애플리케이션 시작 시점에 미리 커넥션을 생성하여 풀에 대기시키는 방법을 고려하였습니다. 커넥션은 `maxIdleTime`, `maxLifeTime` 만큼 살아있다가 종료되는데, 기본값은 `-1`로 무제한입니다. 즉, 별도로 설정하지 않으면 커넥션이 시간 제한 없이 풀에 유지됩니다.

단, 두 설정값을 기본값(-1)으로 두면 서버 측의 `keepAliveTimeout` 설정과 충돌할 수 있습니다. 

서버에서 커넥션을 먼저 끊으면 클라이언트는 이미 닫힌 커넥션으로 요청을 보내게 되어 `"Connection reset by peer"` 오류가 발생할 수 있습니다. 따라서 실무에서는 **서버의 keepAliveTimeout보다 작은 값으로 maxIdleTime을 설정**하는 것이 권장됩니다.


결국 애플리케이션 시작 시점에 미리 커넥션을 생성해 놓더라도 일정 시간 요청이 없으면 유휴 커넥션이 자동 해제되므로 트래픽이 간헐적인 '주문 등록'과 같은 케이스에는 적합하지 않습니다.

> Netty 이슈에서도 ConnectionPool을 웜업하는건 해결책이 아니며 고려사항이 아니라는것을 확인할 수 있습니다. ([Add warmup functionality for the servers/clients #1455](https://github.com/reactor/reactor-netty/pull/1455))

![Connection warmup](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/umgeh6s5sbz9pi13lwf2.png)

### Warmup 옵션

반면 warmup은 실제 TCP 커넥션을 맺지 않고, 이후 요청에서 재사용되는 네트워크 리소스들을 애플리케이션 시작 시점에 미리 초기화 합니다. 따라서 커넥션이 해제되더라도 EventLoop, DNS 리졸버 등은 이미 로드되어 있어, 후속 요청에서 초기화 비용이 발생하지 않습니다.

```java
@Configuration
public class WebClientConfig {

   @Bean
   public WebClient omsWebClient() {
      HttpClient httpClient = HttpClient.create()
         .baseUrl("https://oms-api.example.com");

       // 애플리케이션 시작 시 warmup 수행
       httpClient.warmup().block();

       return WebClient.builder()
           .clientConnector(new ReactorClientHttpConnector(httpClient))
           .build();
   }
}
```

**Warmup으로 미리 준비되는 리소스**

Warmup을 호출하면, HttpClient/TcpClient 내부에서 다음 리소스들이 구성에 따라 사전에 초기화됩니다.

- **EventLoopGroup**: EventLoop 스레드 풀 생성
- **DNS Resolver**: 비동기 DNS 리졸버 초기화
- **Native transport 라이브러리**: epoll 등 네이티브 루프 및 관련 라이브러리 로드
- **SSL Context**: TLS 핸드셰이크용 SSL 엔진 (HTTPS인 경우)

## 마무리

지금까지 WebClient의 간헐적 지연 문제를 추적하며 Netty EventLoop가 Selector와 TaskQueue를 관리하는 방식, Linux epoll의 Multiplexing I/O 메커니즘, 그리고 실제 원인이었던 Netty의 Lazy Initialization과 warmup 해결책까지 살펴보았습니다.

이 과정을 통해 단순히 라이브러리를 사용하는 것을 넘어, 내부 동작 원리를 이해하는 것이 얼마나 중요한지 다시 한번 깨달았습니다. 표면적인 증상만 보고 '코어 수를 느리자'거나 '타임아웃을 조정하자'는 식의 접근 대신, 각 레이어를 단계별로 파고들면서 병목이 발생할 수 있는 지점을 하나씩 가시화하고 범위를 좁혀나갈 수 있었습니다.


---

## AI 브라우저를 활용한 PR 메세지 자동화

- **URL**: https://headf1rst.github.io/log/ko/post34
- **Date**: 2025-12-14
- **Tags**: `AI`

### Content


개발자에게 "코드 작성"과 "PR(Pull Request) 작성" 중 무엇이 더 고통스러운지 묻는다면, 의외로 많은 분이 후자를 택합니다. 구현은 즐겁지만, 그것을 남에게 글로 설명하는데는 생각보다 상당한 시간이 소모되기 때문입니다.

최근 Copilot이나 IntelliJ AI 등 다양한 도구가 등장하며 PR 작성을 돕고 있지만, 여전히 '2%' 부족함을 느낍니다. 오늘은 그 부족한 2%를 채우고, 리뷰어와 작성자 모두가 행복해지는 PR 자동화 워크플로우를 공유하고자 합니다.

## 1. 좋은 Pull Request 란?

기술적인 자동화를 논하기 전에, 우리가 작성해야 할 '좋은 PR'의 정의부터 명확히 해야 합니다.

좋은 PR의 핵심은 **'리뷰어에 대한 공감'**입니다. 리뷰어는 바쁜 시간을 쪼개서 코드를 봅니다. 리뷰어의 시간을 아껴주고, 코드의 의도를 명확하게 전달하는것이 PR의 가장 중요한 목표입니다.

### 좋은 PR의 구성요소

1.  **명확한 맥락:** 단순히 코드가 '무엇'이 변했는지는 diff만 봐도 알 수 있습니다. 중요한 것은 **'왜 이 변경이 필요했는가'**입니다.

2. **직관적 시각 자료:** 백 마디 말보다 한 장의 스크린샷이나 다이어그램이 훨씬 빠른 이해를 돕습니다.

3. **작고 집중된 단위:** 하나의 PR은 하나의 이슈만 다루어야 리뷰와 병합의 리스크가 줄어듭니다.

코드 리뷰를 하는 이유 중 하나는 팀원 간의 작업 방향 정렬과 개발 과정에서의 실수 방지입니다.

작업 배경을 상세히 기술하면 리뷰어는 단순히 코드 변경사항만 보는 것이 아니라, 왜 이런 변경이 필요했는지 맥락을 이해할 수 있습니다. 이러한 컨텍스트 공유는 리뷰어가 더 넓은 시야에서 코드를 바라볼 수 있게 하여, 단순 문법적 오류나 컨벤션 위반을 넘어 아키텍처적 개선점, 잠재적 사이드 이펙트, 대안적 접근 방식 등 다양한 관점의 피드백을 제공할 수 있게 됩니다.

결과적으로 배경 설명이 잘 된 PR은 리뷰의 질을 높이고, 팀 전체의 도메인 지식과 시스템 이해도를 향상시키는 효과적인 지식 공유의 수단이 됩니다.

## 2. 현재 AI Assistant의 한계: '맥락'의 부재

최근의 생성형 AI 도구들(GitHub Copilot, IntelliJ AI 등)은 코드 변경 사항(Diff)을 요약하는 데 탁월한 능력을 보여줍니다. 버튼 하나만 누르면 아래와 같은 요약을 순식간에 만들어냅니다.

> Copilot Agent가 생성한 PR

<img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/nof7n7t65c4zkp49cpim.png" alt="Copilot PR" width="400">

그러나 AI가 만든 PR 메시지는 초안으로는 충분하지만, 작업의 배경까지는 담아내지 못해 코드 수정의 근본적인 이유를 파악하기 어렵다는 한계가 있습니다.

* **배경 정보 누락:** AI는 코드 자체만 보고 요약하므로, "사용자가 특정 상황에서 겪은 불편함"이나 "기획 의도" 같은 외부 맥락을 알지 못합니다.

* **근본적 이유 부재:** 코드를 '수정했다'는 사실은 알지만, '왜 수정해야만 했는지'에 대한 비즈니스적/기술적 배경은 설명하지 못합니다.

결국, 리뷰어가 가장 궁금해하는 **'작업 배경'**을 채우는 일은 여전히 사람의 몫으로 남게 됩니다. 그리고 배경을 간결하고 핵심적으로 정리하는 것은 PR 작성에서 가장 많은 시간을 잡아먹습니다.

이 과정을 AI가 대신할 수 있다면, 리뷰어에게 코드의 의도를 명확하게 전달하면서 PR 작성에 드는 시간을 크게 줄일 수 있을 것입니다.

## 3. Jira 티켓에서 맥락 가져오기

그렇다면 어디서 맥락을 가져올 수 있을까요? 저는 개발을 시작하기 전에 티켓에 해결하고자 하는 문제와 목표를 명확히 정의합니다.

티켓에는 작업을 한눈에 파악할 수 있도록 "무엇을", "왜" 하는지를 간결하게 담습니다. 예를 들어 "주문 취소 시 재고 복구 실패 이슈 수정"처럼 핵심 문제와 해결 방향을 제목에서부터 드러냅니다.

그 다음 작업의 배경과 목적을 상세히 기술합니다. 현재 어떤 문제가 발생하고 있는지(AS-IS), 왜 이 작업이 필요한지, 어떤 비즈니스 영향이 있는지를 구체적으로 작성하고, 작업을 통해 달성하고자 하는 상태(TO-BE)와 명확한 완료 기준을 함께 정의합니다.

"PR 쓰기도 귀찮은데 Jira까지 자세히 쓰라고요?"라고 반문하실 수 있습니다. 하지만 **문제 정의는 AI가 아닌 사람이 해야 할 영역**입니다. 티켓을 충실히 작성하면 다음과 같은 이점이 있습니다.

### 작업 전 Jira 티켓을 풍부하게 작성하면 좋은점

1. **명확한 목표 설정과 작업 집중**: "로그인 버그 수정"이라는 한 줄짜리 티켓은 '어떤 상황에서', '어떤 사용자가', '어떻게' 버그를 겪는지 알려주지 않습니다. 상세한 설명, 재현 방법, 기대 결과가 담긴 티켓은 내가 무엇을 해야 하는지 명확히 알려주어 불필요한 고민 없이 작업에만 집중하게 해줍니다.

2. **정확한 계획 수립과 예측 가능성 확보**: 티켓이 상세할수록 작업의 규모와 복잡도를 더 정확하게 예측할 수 있습니다.

3. **미래의 나를 위한 기록**: 몇 달 뒤 내가 작성한 코드를 다시 보게 될 때, Jira 티켓은 "내가 왜 이 코드를 이렇게 작성했더라?"에 대한 완벽한 답변이 되어 줍니다. 유지보수와 기능 확장이 훨씬 쉬워집니다.

## 4. AI 브라우저로 워크플로우 완성하기

우리의 목표는 **"PR 작성에 드는 시간과 노력을 최소화하면서, 퀄리티는 극대화하는 것"**입니다. 이를 위해 AI 브라우저인 Commet(혹은 Dia)의 기능을 활용할 수 있습니다.

Commet은 단순한 브라우징을 넘어, 사용자가 자주 사용하는 프롬프트를 **'Shotcuts'** 형태로 저장하고 `/Shotcuts`로 호출할 수 있는 기능을 제공합니다. 이를 통해 매번 긴 프롬프트를 작성할 필요 없이, Jira의 문맥과 코드 변경 사항을 결합할 수 있습니다.

### 자동화 워크플로우

1.  **Jira 티켓 작성:** 작업 전, 이슈의 배경(Why)과 해결 방안(What)을 Jira에 명확히 기록합니다.

2. **Commet Shortcuts 등록:** Commet 브라우저에 PR 자동 생성을 위한 Shortcuts를 등록해줍니다.
![shortcuts](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n2mgw687c31cvmbf0t44.png)

3. PR 수정 화면에서 Commet의 사이드 패널을 열고 미리 등록해 둔 **PR 작성 Shortcuts**을 호출
합니다.

이렇게 하면 AI는 Jira에서 **'작업의 의도(Why)'**를 가져오고, Git Diff에서 **'구현 내용(What)'**을 가져와 PR 메시지를 생성해 줍니다.

## PR 자동 생성에 사용한 프롬프트

마지막으로 PR 자동 생성에 사용한 프롬프트를 공유드리며 글을 마무리하도록 하겠습니다.

```text
당신은 명확하고 규칙적인 Pull Request 설명을 잘 작성하는 숙련된 소프트웨어 엔지니어입니다.  
당신의 주요 목표는 주어진 Jira 티켓과 PR 세부 정보를 기반으로, 우리 팀의 컨벤션을 **엄격히 준수하는 PR 메시지**를 생성하는 것입니다.

이 페이지의 브랜치 정보를 참고하고, “ABCD”로 시작하는 관련 Jira 티켓을 열어 그 안에 작성된 맥락(context)을 활용해 Pull Request 설명을 작성하세요.  
아래 링크를 통해 Jira 보드에 접속한 뒤, 해당 DPDD 티켓을 검색하여 열 수 있습니다.

Jira 보드 링크:  
`https://jiraboard.atlassian.net/jira/software/c/projects/DPDD/boards/5218?assignee=712020%3A8ef336bd-458e-4630-a79f-d0ea76930601`

PR 메시지는 **한국어로 작성하세요.**

---

### 따라야 할 PR 메시지 템플릿

## 요약 ✍️  
*Jira 티켓과 PR 정보를 기반으로 간결한 요약을 작성하세요.*  
먼저 이번 변경의 **배경과 목적(Why)** 을 설명하고,  
그 다음 **구체적인 변경 내용(What, How)** 을 불릿 포인트로 정리하세요.

이 문서에 포함된 클래스나 주요 구성요소 간의 관계, 역할, 상호작용을 설명하기 위해  
`mermaid` 다이어그램을 코드 블록 안에 렌더링하세요.  
다이어그램은 문서의 변경점과 새롭게 추가된 부분에 초점을 맞추어야 합니다.

**형식(Format):**
- 익숙한 ASCII 문자만 사용합니다. 결과는 고정폭(monospaced) 폰트로 렌더링됩니다.  
- 문서의 **첫 줄과 마지막 줄에는 60개의 “_” 문자로 된 수평선**을 추가합니다.  
- 각 줄은 **최대 60자 이내**로 제한합니다.  
- 파일 관계를 설명할 필요가 있다면 디렉터리/계층 구조를 사용하고,  
  그렇지 않다면 흐름도(Flowchart) 또는 시각적인 형태로 표현합니다.  
- 문서나 내용이 제공되지 않았다면, 내용을 요청하세요.

---

## 리뷰어가 참고하면 좋은 링크 🔗  
- Jira url: [여기에 Jira URL 삽입]

---

## 먼저 읽기 좋은 진입점 🚘️  
*PR 세부 정보를 기반으로, 리뷰 시 가장 먼저 보면 좋은 주요 진입 파일(Controller, Consumer 등)을 나열하세요.*

---

## 리뷰어에게 🙏  
*PR의 복잡한 로직, 트레이드오프, 또는 집중해서 봐주었으면 하는 부분을 짧게 설명하세요.*  
너무 세부적이지 않게, 상위 레벨에서 작성합니다.

---

## 체크 리스트 ✅  
- [x] 테스트 코드를 작성했습니다. (코드 변경이 있는 경우)  
- [x] 커밋 전 SonarLint 분석을 수행했습니다.  
- [x] 셀프 리뷰를 진행했습니다.  
- [x] Jira 티켓을 업데이트했습니다.
```


---

## Swagger의 사실과 오해: API-First Development

- **URL**: https://headf1rst.github.io/log/ko/post32
- **Date**: 2025-12-01
- **Tags**: `API`

### Content


개발에서 가장 중요하게 생각하는것 중 하나는 '인터페이스'입니다.
인터페이스를 잘 정의하는 것은 시스템의 일관성과 확장성을 보장하고 변화에 유연하게 대응할 수 있는 기반이 되어줍니다.

새로운 API를 개발할때 가장 먼저 마주하게 되는 인터페이스는 바로 API 명세인데요. API 명세를 잘 정의하기 위해서는 프론트엔드 개발자와 백엔드 개발자간의 효율적인 소통이 필수적입니다.

그렇다면 어떻게 해야 효율적으로 API 명세에 대해 소통할 수 있을까요?
저의 경우, 컨플루언스나 지라 티켓에 API 명세를 작성하거나 임의의 Controller를 구현한 Mock API를 개발환경에 배포한 뒤 Swagger 문서를 공유하는 방식으로 프론트와 소통하였는데요.

이러한 소통 방식은 평소에는 문제가 없지만 짧은 개발 일정 속에 제공해야할 API 수가 많을 경우, 문서와 실제 API가 불일치하거나 다른 작업의 영향으로 인해 빠르게 Mock API를 개발 환경에 제공하지 못하는 상황이 발생했습니다.

## 무엇이 문제인가

문제의 근본적인 원인은 Swagger를 단순히 코드 작성 후 자동으로 문서를 생성해주는 도구로만 생각하고 사용했기 때문입니다. 즉, 코드를 작성해야 Swagger 문서가 만들어지다 보니 API 디자인과 소통이 **Code-First** 방식으로 진행되었습니다.

코드와 동기화된 Swagger 문서는 서버가 배포된 이후에 갱신되기 때문에 API 논의 시점에 딜레이가 발생합니다. 이러한 딜레이를 해결하기 위해 별도의 문서를 만들어 공유해야 했습니다. 하지만 작성자마다 문서 포맷과 표현 방식이 달라 일관성을 유지하기 어려웠습니다. 뿐만 아니라 Swagger 문서가 생성되고 나서는 제대로 관리되지 않았기 때문에 신뢰할 수 없는 문서가 되어갔습니다.

결국 API 소통을 위해서는 코드보다 먼저 문서가 선행되어야 합니다. 이러한 접근 방식을 실현하기 위해 등장한 것이 바로 OpenAPI Specification을 활용한 **API-First** 개발 방식입니다.

## OpenAPI Specification이란?

OpenAPI Specification(OAS)에 대해 알아보기 전에 OAS가 어떠한 문제를 해결하기 위해 등장한 기술인지 먼저 알아보도록 하겠습니다.

사실 OAS의 기원은 Swagger에서 시작되었습니다. 

> The Swagger API project was created in 2011 by Tony Tam, technical co-founder of the dictionary site Wordnik. During the development of Wordnik's products, the need for automation of API documentation and client SDK generation became a major source of frustration. Tam designed a simple JSON representation of the API...
https://en.wikipedia.org/wiki/Swagger_(software)

Swagger의 시작점은 단순한 문서 자동화 툴이 아니었습니다. 2010년대 초, 온라인 사전 서비스 Wordnik를 개발하던 Tony Tam은 API 문서화와 클라이언트 SDK 반복 생성에 점점 지쳐가던 중, “API의 동작을 사람이 아닌 명세(specification)로 정의할 수 있다면 서버와 클라이언트가 동일한 계약을 공유할 수 있지 않을까?”라는 질문에서 Swagger를 고안하게 되었습니다.

Tony Tam은 API를 간결하게 JSON(YAML)으로 기술하는 방식을 설계했고, "코드보다 먼저 계약을 정의하자"라는 단순하지만 강력한 철학을 내세웠습니다. Swagger 명세는 단순한 문서 형식을 넘어, API의 요청-응답 구조와 스키마, 보안, 상태 코드 등 동작의 규약을 포괄하는 '계약(contract)' 그 자체를 기술하는 수단이었습니다. 즉, Swagger의 본래 목적은 API-First(Contract-First) 개발 문화의 정착에 있었던 셈입니다. 

실제로 많은 개발자가 말하는 Swagger는 Swagger 그 자체가 아니라 Spring 진영에서 구현된 Springdoc 혹은 Springfox 라이브러리 입니다. 

```gradle
dependencies {
    implementation 'org.springdoc:springdoc-openapi-starter-webmvc-ui:2.6.0'
    implementation 'io.springfox:springfox-boot-starter:3.0.0'
}
```

Springdoc은 코드에 `@Tag`, `@Operation` 같은 어노테이션을 붙이면 자동으로 Swagger 형식의 JSON 혹은 YAML 명세를 생성해주는 도구입니다. 명세를 작성하는 게 아니라 코드로부터 명세를 추출하는 Code-First 접근인 것입니다.

Code-First 접근의 가장 큰 한계는 명세와 실제 구현 사이의 신뢰가 무너진다는 점입니다. 명세는 항상 이미 작성된 코드를 전제로 생성되기 때문에, 변화가 생길 때마다 명세가 뒤처지게 마련입니다.

반면 API-First 방식은 명세가 먼저 존재하고, 구현은 그 계약을 이행하는 수단일 뿐입니다. API의 요청 구조, 응답 형식, 각종 제약 조건과 오류 모델까지 우선적으로 합의한 뒤, 서버와 클라이언트가 명세를 기준으로 각자 코드를 작성하거나 검증합니다. 이 때 명세서는 단순한 문서가 아니라, 팀 간 신뢰를 담은 실행 가능한 계약이 됩니다.

## OAS를 통한 API-First 실천

이제 API-First 방식을 적용해서 간단한 게시판 서비스를 만들어보겠습니다.

먼저 API 명세를 관리할 별도의 GitHub 저장소를 생성합니다. 클라이언트와 서버 개발자는 각자의 프로젝트에 이 저장소를 서브모듈로 추가해 동일한 명세를 공유하고 효율적으로 관리할 수 있습니다.

### Git 서브모듈 추가 방법

각자의 레포지토리에서 아래 명령어로 서브모듈을 추가합니다.

```bash
git submodule add https://github.com/headF1rst/simple-dash-board-oas.git contract
```

여기서 `contract`는 서브모듈이 생성될 디렉터리 이름입니다. 명령어 실행 후 프로젝트 안에 서브모듈이 정상적으로 추가된 것을 확인할 수 있습니다.

![sub module git](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bpkoy7z9kr7nav1cj2q0.png)

API 명세 작성은 서비스의 전역 정보를 담는 메인 진입점 파일인 `openapi.yaml` 부터 시작합니다.

**openapi.yaml**

```yaml
openapi: 3.0.0
info:
  title: Simple Dashboard API
  description: 게시판 서비스 API
  version: 1.0.0
servers:
  - url: 'https://api.simpledashboard.com'
    description: Production server
  - url: 'http://localhost:8080'
    description: Local development server

paths:
  # 게시글 목록 조회 및 생성 엔드포인트
  /v1/articles:
    $ref: './paths/articles.yaml'
    # 특정 게시글 조회, 수정, 삭제 엔드포인트
  /v1/articles/{id}:
    $ref: './paths/articles-by-id.yaml'

components:
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
```

`$ref` 키워드를 사용해 다른 위치의 스키마 파일을 참조할 수 있습니다. 이를 통해서 진입점 역할을 하는 `openapi.yaml` 파일이 지나치게 비대해지는 것을 막고, 하나의 정의를 여러 곳에서 재사용할 수 있습니다.

`./paths` 디렉터리 하위에 게시글 목록 API 명세를 별도의 YAML 파일로 분리해 관리해 주었습니다.

**articles.yaml**

```yaml
# GET/POST /v1/articles
get:
  summary: 게시글 목록 조회
  description: 게시글 목록을 조회합니다. 페이징을 지원합니다.
  operationId: listArticles
  tags:
    - Article
  parameters:
    - $ref: '../components/parameters/BoardIdQuery.yaml'
    - $ref: '../components/parameters/WriterIdQuery.yaml'
    - $ref: '../components/parameters/PageQuery.yaml'
    - $ref: '../components/parameters/SizeQuery.yaml'
    - $ref: '../components/parameters/SortQuery.yaml'
  responses:
    '200':
      description: 조회 성공
      content:
        application/json:
          schema:
            $ref: '../components/schemas/ArticleListResponse.yaml'
    '400':
      $ref: '../components/responses/400BadRequest.yaml'
    '500':
      $ref: '../components/responses/500ServerError.yaml'

post:
  summary: 새로운 게시글 생성
  operationId: createArticle
  tags:
    - Article
  requestBody:
    required: true
    content:
      application/json:
        schema:
          $ref: '../components/schemas/ArticleCreateRequest.yaml'
  responses:
    '201':
      description: 생성 성공
      content:
        application/json:
          schema:
            $ref: '../components/schemas/ArticleResponse.yaml'
    '400':
      $ref: '../components/responses/400BadRequest.yaml'
```

전체 디랙토리 구조는 다음과 같습니다.

```text
contract/
├── openapi.yaml                      # 메인 진입점 (다른 파일들을 $ref로 참조)
├── paths/                            # API 엔드포인트 정의
│   ├── articles.yaml                 # GET/POST /v1/articles
│   └── articles-by-id.yaml           # GET/PUT/DELETE /v1/articles/{id}
├── components/
│   ├── schemas/                      # 데이터 모델 (DTO)
│   ├── parameters/                   # 공통 파라미터
│   ├── responses/                    # 공통 응답
│   └── examples/                     # 예제 데이터
├── openapi-templates/spring/         # 커스텀 Mustache 템플릿
│   ├── api.mustache                  # API 인터페이스 템플릿
│   ├── generatedAnnotation.mustache  # @Generated 제거용
│   └── licenseInfo.mustache          # 라이선스 헤더
└── build.gradle                      # OpenAPI Generator 설정
```

yaml 파일을 작성할 때는 IntelliJ Plugin인 [OpenAPI Specifications](https://plugins.jetbrains.com/plugin/14394-openapi-specifications)이나 [Swagger Editor](https://editor.swagger.io/)를 활용하면 즉각적인 피드백을 받을 수 있어, 명세 작성과 검증을 훨씬 수월하게 진행할 수 있습니다.

![openapi specification plugin](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5ook4h6nva8pr7po2n78.png)

### OpenAPI Generator 인터페이스 자동 생성

OpenAPI Generator를 사용하면 애플리케이션 빌드 시점에, yaml로 정의한 API 명세를 기반으로 인터페이스를 자동 생성할 수 있습니다. 이렇게 생성된 인터페이스를 기반으로 Controller 구현체를 작성하면, “코드로부터 명세를 생성하는 방식”이 아니라 “명세로부터 코드를 생성하는 방식”으로 개발할 수 있습니다.

먼저 OpenAPI Generator를 사용하기 위한 Gradle 설정을 추가합니다.

**build.gradle**

```
plugins {
    id 'org.openapi.generator' version '7.17.0' apply false
}
```

**build.gradle (:contract)**

```gradle
apply plugin: 'org.openapi.generator'

openApiGenerate {
    generatorName = 'spring'
    
    // 입력 스펙 경로 (번들링된 파일 사용)
    inputSpec = layout.buildDirectory.file("openapi.bundle.yaml").get().asFile.absolutePath
    
    // 출력 경로
    outputDir = layout.buildDirectory.dir("generated/openapi").get().asFile.absolutePath
    
    // 커스텀 Mustache 템플릿 디렉터리
    templateDir = "$projectDir/openapi-templates/spring"
    
    // 패키지 설정
    apiPackage = 'simple.board.contract.api'
    modelPackage = 'simple.board.contract.model'
    
    // API/Model 파일만 생성 (supporting 파일 제외)
    globalProperties = [
        apis           : '',
        models         : '',
        supportingFiles: ''
    ]
    
    // 코드 생성 옵션
    configOptions = [
        interfaceOnly          : 'true',   // 인터페이스만 생성 (구현체 X)
        useTags                : 'true',   // 태그 기반 API 분리
        addGeneratedAnnotation : 'false',  // @Generated 어노테이션 제거
        dateLibrary            : 'java8',  // java.time.* 사용
        serializableModel      : 'true',   // 모델 직렬화 가능
        documentationProvider  : 'none',   // Swagger/SpringDoc 어노테이션 제거
        useBeanValidation      : 'true',   // Bean Validation 사용
        useJakartaEe           : 'true',   // Jakarta EE 사용 (javax → jakarta)
        skipDefaultInterface   : 'true',    // default 메서드 본문 제거
        useSpringBoot3         : 'true'
    ]
}

// 이 모듈은 라이브러리이므로 bootJar 비활성화
tasks.named('bootJar') { enabled = false }
tasks.named('jar') { enabled = true }

// 생성된 소스를 컴파일에 포함
sourceSets {
    main {
        java {
            srcDir layout.buildDirectory.dir("generated/openapi/src/main/java").get().asFile
        }
    }
}

// 컴파일 전에 코드 생성 실행
tasks.named('compileJava') {
    dependsOn tasks.named('openApiGenerate')
}

// OpenAPI 스펙 번들링 태스크
// $ref로 분리된 YAML 파일들을 build 디렉터리로 복사하여 참조 해소
tasks.register('bundleOpenApi') {
    group = 'openapi'
    description = 'Prepare OpenAPI spec files for code generation'
    
    inputs.file("$projectDir/openapi.yaml")
    inputs.dir("$projectDir/paths")
    inputs.dir("$projectDir/components")
    
    outputs.file(layout.buildDirectory.file("openapi.bundle.yaml"))
    outputs.dir(layout.buildDirectory.dir("paths"))
    outputs.dir(layout.buildDirectory.dir("components"))
    
    doLast {
        copy {
            from "$projectDir/openapi.yaml"
            into layout.buildDirectory.get().asFile
            rename { 'openapi.bundle.yaml' }
        }
        if (file("$projectDir/paths").exists()) {
            copy {
                from "$projectDir/paths"
                into layout.buildDirectory.dir("paths").get().asFile
            }
        }
        if (file("$projectDir/components").exists()) {
            copy {
                from "$projectDir/components"
                into layout.buildDirectory.dir("components").get().asFile
            }
        }
    }
}

// 코드 생성 태스크 설정
tasks.named('openApiGenerate') {
    dependsOn tasks.named('bundleOpenApi')
    
    // 불필요한 생성 파일 삭제
    doLast {
        delete layout.buildDirectory.file("generated/openapi/README.md").get().asFile
        delete layout.buildDirectory.file("generated/openapi/.openapi-generator").get().asFile
    }
}

dependencies {
    // 생성된 코드의 컴파일 의존성 (런타임은 소비 모듈에서 제공)
    compileOnly 'org.springframework:spring-web'
    compileOnly 'jakarta.validation:jakarta.validation-api'
    compileOnly 'com.fasterxml.jackson.core:jackson-annotations'
    compileOnly 'org.openapitools:jackson-databind-nullable:0.2.6'
}
```

빌드를 실행하면 아래와 같이 API 인터페이스와 스펙 기반의 모델 객체가 자동으로 생성된 것을 확인할 수 있습니다.

<img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8c88yzs7f9d64ppz8ttv.png" alt="api generated" width="450"/>

## 커스텀 Mustache 템플릿

OpenAPI Generator가 생성한 API 인터페이스를 확인해보면, 저희의 관심사와는 거리가 먼 다양한 보일러플레이트 코드가 함께 생성되어 다소 장황하게 느껴질 수 있습니다.

<img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dv5rkf83nbofpon44mxm.png" alt="OpenAPI Generated Code" style="width: 85%; max-width: 1000px; display: block; margin: 20px auto;"/>

Mustache 템플릿을 직접 커스터마이징하면, 프로젝트 상황에 맞춰 생성 코드를 한층 더 최적화할 수 있습니다.

Mustache는 다양한 프로그래밍 언어를 지원하는 Logic-less 템플릿 엔진으로, 복잡한 로직을 최소화하고 간단한 조건문과 반복문만을 지원합니다. 이러한 특성 덕분에 데이터 표현에만 집중할 수 있으며, View와 서버 로직이 명확하게 분리되어 코드가 단순하고 가독성이 높아집니다. OpenAPI Generator는 기본적으로 Mustache 템플릿을 사용해 코드를 생성하기 때문에, 이 템플릿을 직접 수정하여 생성 결과물을 원하는 형태로 제어할 수 있습니다.

다음과 같이 mustache로 불필요한 보일러플레이트를 제거하거나 프로젝트에 필요한 커스텀 어노테이션과 메서드를 추가할 수 있습니다.

```mustache
package {{package}};

import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RequestHeader;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.RequestParam;
{{#useBeanValidation}}
import jakarta.validation.Valid;
{{/useBeanValidation}}
import {{modelPackage}}.*;

/**
 * {{classname}} - API interface
 */
public interface {{classname}} {
{{#operations}}
{{#operation}}

    @RequestMapping(method = RequestMethod.{{httpMethod}}, value = "{{{path}}}")
    ResponseEntity<{{#returnType}}{{returnType}}{{/returnType}}{{^returnType}}Object{{/returnType}}> {{operationId}}({{#allParams}}{{^-first}}
            {{/-first}}{{#isPathParam}}@PathVariable("{{baseName}}") {{/isPathParam}}{{#isQueryParam}}@RequestParam(value = "{{baseName}}", required = {{required}}) {{/isQueryParam}}{{#isHeaderParam}}@RequestHeader(value = "{{baseName}}", required = {{required}}) {{/isHeaderParam}}{{#isBodyParam}}{{#useBeanValidation}}@Valid {{/useBeanValidation}}@RequestBody {{/isBodyParam}}{{dataType}} {{paramName}}{{^-last}},{{/-last}}{{/allParams}});
{{/operation}}
{{/operations}}
}
```

장황했던 `ArticleApi` 인터페이스가 간결하게 개선된 것을 볼 수 있습니다.

![Article Api](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5bu0pl719fxbyfw1acfs.png)

## 마무리

지금까지 API-First 방식으로 OpenAPI Specification을 활용한 효율적인 API 개발 프로세스에 대해 알아보았습니다. Git 서브 모듈을 통해 명세를 공유하고, OpenAPI Generator로 백엔드 인터페이스를 자동 생성하며, Mustache 템플릿을 커스터마이징하여 프로젝트에 최적화하는 방법까지 살펴보았습니다.

이러한 접근 방식의 진정한 가치는 프론트엔드 개발자와의 협업에서 더욱 빛을 발합니다. OpenAPI 명세를 공유하면 프론트엔드에서도 `typescript-axios`나 `typescript-fetch` 같은 Generator를 사용해 API 호출 코드와 타입을 자동으로 생성할 수 있습니다.

더 나아가 Prism 같은 Mock 서버를 활용하면 백엔드 API 구현이 완료되기 전에도 프론트엔드에서 명세 기반의 Mock 테스트를 진행할 수 있어, 진정한 의미의 병렬 개발이 가능해집니다.

다만, API-First 방식이 도입 초기부터 즉각적인 생산성 향상을 가져다주진 않을 것으로 생각됩니다. OpenAPI Specification 문법에 익숙해지는 과정, YAML 파일 구조화 방법, 그리고 Mustache 템플릿 커스터마이징까지 생각보다 학습 곡선이 가팔랐습니다. 

무엇보다 아직 OpenAPI Specification 관련 라이브러리들이 전반적으로 미성숙하다는 느낌을 받았습니다. 자잘한 버그들이 존재했고, 이를 해결하는 과정에서 생각보다 많은 시간이 소비되었습니다.

하지만 이러한 초기 투자는 장기적으로 충분한 가치가 있다고 생각합니다. OpenAPI Specification이 단일 진실 공급원이 되면 "문서와 실제 동작이 다르다", "이 필드가 필수인지 선택인지 모르겠다"라는 식의 불필요한 오해와 소통 비용이 사라지고, 팀 전체가 동일한 명세를 기준으로 대화할 수 있게 될 것이라 생각합니다.


---

## Resilience4j Bulkhead 패턴: 대용량 데이터 처리 안정성 높이기

- **URL**: https://headf1rst.github.io/log/ko/post31
- **Date**: 2025-10-24
- **Tags**: `Java`

### Content


물류 시스템을 개발하다 보면, 대량의 데이터를 엑셀 파일로 내보내야 하는 요구사항을 자주 마주합니다. 사용자가 지정한 기간의 데이터를 엑셀로 제공하려면, 먼저 데이터를 조회한 뒤 Apache POI로 엑셀 형식으로 변환하는 과정이 필요합니다.

다만 이 과정을 한 번에 처리하면 메모리 사용량이 급증해 `OutOfMemoryException`이 발생할 수 있습니다. 따라서 데이터를 N개 단위로 나누어 조회하고 변환하는 작업을 반복하는 것이 안전합니다. 특히 POI의 SXSSF는 슬라이딩 윈도우 방식으로 설정된 행 수만 메모리에 유지하고, 초과분은 임시 파일로 디스크에 플러시 하여 메모리 상한을 넘지 않도록 합니다.

하지만 요청이 동시에 몰리는 상황에서는 얘기가 달라집니다. 동시 다운로드 건수와 윈도우 크기가 곱해지면서 `요청 수 × 윈도우 크기`에 해당하는 데이터가 동시에 메모리에 올라가 OOM 위험이 다시 커집니다.

이를 방지하는 방법으로는

> - 서비스 레벨에서 동시 요청 수를 제한하는 방법,
> - 요청마다 독립 프로세스를 띄워 비동기로 처리해 격리하는 방법

이 있습니다.

이번 포스트에서는 `Bulkhead 패턴`을 활용해 동시 요청을 효과적으로 제한하는 방법을 중심으로 정리해 보도록 하겠습니다.

## Bulkhead 패턴이란

![Bulkhead](https://upload.wikimedia.org/wikipedia/commons/1/11/Compartments_and_watertight_subdivision_of_a_ship%27s_hull_%28Seaman%27s_Pocket-Book%2C_1943%29_%28cropped%29.jpg)

Bulkhead는 본래 선박 구조에서 온 용어로, 선체 내부를 여러 개의 격벽으로 나누어 한 구획에 물이 차도 다른 구획으로 침수 피해가 번지지 않게 하는 설계를 뜻합니다. 이러한 선박 설계에서 착안한 Bulkhead 패턴은 시스템 자원을 영역별로 분리하여, 한 영역에 과부하나 장애가 발생해도 전체로 확산하지 않도록 합니다.

구체적으로는 특정 기능이나 엔드포인트를 논리/물리적으로 구분하고, 각 구획에 허용할 동시 처리 수와 대기열 상한을 고정합니다. 이렇게 하면 한 구획의 트래픽이 급증하거나 해당 구획이 리소스를 모두 소진하더라도, 다른 API 처리를 위한 자원은 보존되어 한 작업 때문에 다른 작업이 지연되는 상황을 예방할 수 있습니다.

엑셀 파일 생성은 메모리 소모가 크고 일반적인 API보다 처리 시간이 길기 때문에, 동일 서버가 다른 중요한 요청까지 함께 처리하는 환경에서는 엑셀 다운로드가 몰릴 때 톰캣 스레드나 메모리 같은 공용 리소스를 잠식하여 다른 API까지 느려지는 등 문제가 발생할 수 있습니다. Bulkhead 패턴은 작업별로 리소스를 분리하고 상한을 설정해 이러한 장애 전파를 차단합니다.

## Resilience4j를 사용한 Bulkhead 패턴 적용

이러한 Bulkhead 패턴은 직접 구현할 필요 없이 라이브러리를 사용하여 간단하게 구현할 수 있습니다.

대표적인 라이브러리에는 `Resilience4j`, `Netflix Hystrix`, `Alibaba Sentinel` 등이 있는데 가장 활발하게 개발되고 있고, Spring 생태계와의 통합도 잘 되어있는 [Resilience4j](https://resilience4j.readme.io/)를 사용하도록 하겠습니다.
(Resilience4j는 이 외에도 서킷 브레이커나 재시도 등 시스템의 회복 탄력성을 위한 다양한 기능들을 제공합니다)

Resilience4j를 사용하여 Bulkhead 패턴을 적용하는 방법은 간단합니다.

### 의존성 추가

아래와 같이 의존성을 추가해 줍니다.

```gradle
// Resilience4j
implementation 'io.github.resilience4j:resilience4j-spring-boot3:2.3.0'
```

참고: https://mvnrepository.com/artifact/io.github.resilience4j

그리고 `application.yml`에 아래와 같이 추가합니다

```yml
resilience4j:
  bulkhead:
    instances:
      excelStream: # 벌크헤드의 이름
        maxConcurrentCalls: 5   # 동시에 5건까지만 스트리밍 생성
        maxWaitDuration: 100ms    # 100ms 초과 시 거절
```

`maxConcurrentCalls`는 허용할 최대 동시 호출 수(기본값: 25), `maxWaitDuration`은 최대 동시 호출 수에 도달했을 때 추가 요청이 들어온 경우 얼마나 대기할 것인지를 나타냅니다. 기본값인 0s로 설정할 경우 대기 없이 즉시 거절하게 됩니다.

![default value](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dkfxicw3isiauf3943u9.png)

`application.yml`에 옵션을 추가하지 않고 Bulkhead 인스턴스를 만들고 주입해서 사용하는 방법도 가능합니다.

**BulkheadSemaphoreConfig.java**

```java
@Configuration
public class BulkheadSemaphoreConfig {

    @Bean
    public BulkheadConfigCustomizer excelStreamSemaphore() {
        return BulkheadConfigCustomizer.of("excelStream", builder -> builder
                .maxConcurrentCalls(5)
                .maxWaitDuration(Duration.ofMillis(100))
            );
    }
}
```

**BulkheadThreadPoolConfig.java**

```java
@Configuration
public class BulkheadThreadPoolConfig {

    @Bean
    public ThreadPoolBulkheadConfigCustomizer excelStreamThreadPool() {
        return ThreadPoolBulkheadConfigCustomizer.of("excelStream", builder -> builder
                .coreThreadPoolSize(4)
                .maxThreadPoolSize(8)
                .queueCapacity(16)
            );
    }
}
```

### 구현 예시

Bulkhead를 적용한 간단한 Controller와 검증을 위한 테스트 코드를 만들어보겠습니다.

**ExcelDownloadController.java**

```java
@RestController
@RequiredArgsConstructor
public class ExcelDownloadController {

    @Bulkhead(name = "excelStream", type = Type.SEMAPHORE, fallbackMethod = "excelStreamFallback")
    @PostMapping(value = "/orders/download")
    public void download(
        @RequestBody @Valid OrderDownloadRequest request,
        HttpServletResponse response
    ) {
        try {
            // 엑셀 다운로드 처리 시뮬레이션 (500ms 소요)
            Thread.sleep(500);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }

    public void excelStreamFallback(
        OrderDownloadRequest request,
        HttpServletResponse response,
        BulkheadFullException ex
    ) {
        throw new TooManyRequestsException("Too many concurrent requests");
    }
}
```

`@Bulkhead`의 ‎`type`에는 ‎`Bulkhead.Type.SEMAPHORE`와 ‎`Bulkhead.Type.THREADPOOL` 두 가지가 있으며, 기본값은 ‎`SEMAPHORE`입니다. 두 방식의 차이는 아래에서 자세히 설명하겠습니다.

실패 시 호출할 ‎`fallbackMethod` 또한 지정할 수 있습니다.

**주의할 점:**
>  • ‎`fallback` 메서드의 시그니처는 원본 컨트롤러 메서드와 호환되어야 합니다. 즉, 원본 메서드의 파라미터를 동일한 순서로 모두 받아야 하며, 마지막 인자로 예외 타입을 추가할 수 있습니다.
 • 반환 타입도 일치해야 합니다. 원본이 ‎`void`면 ‎`fallback`도 ‎`void`여야 하고, ‎`ResponseEntity<T>`를 반환하면 ‎`fallback` 역시 같은 타입을 반환해야 합니다.


**ExcelDownloadControllerBulkheadTest.java**

```java
@SpringBootTest(classes = SampleApplication.class)
@AutoConfigureMockMvc
@ActiveProfiles("test")
class ExcelDownloadControllerBulkheadTest {

    @Autowired
    private MockMvc mockMvc;

    @Test
    void bulkhead가_최대_동시_호출_수를_제한한다() throws InterruptedException {
        // given
        int totalRequests = 10;
        int maxConcurrentCalls = 5;
        CountDownLatch startLatch = new CountDownLatch(1);
        CountDownLatch doneLatch = new CountDownLatch(totalRequests);
        ExecutorService executorService = Executors.newFixedThreadPool(totalRequests);
        
        AtomicInteger successCount = new AtomicInteger(0);
        AtomicInteger rejectedCount = new AtomicInteger(0);

        // when
        for (int i = 0; i < totalRequests; i++) {
            executorService.submit(() -> {
                try {
                    startLatch.await(); // 모든 스레드가 동시에 시작하도록 대기
                    
                    mockMvc.perform(post("/orders/download")
                            .contentType(MediaType.APPLICATION_JSON)
                            .content("{}"))
                        .andExpect(status().isOk());
                    
                    successCount.incrementAndGet();
                } catch (Exception e) {
                    // TooManyRequestsException이 발생하면 거절된 것으로 카운트
                    if (e.getCause() instanceof TooManyRequestsException ||
                        (e.getMessage() != null && e.getMessage().contains("Too many concurrent requests"))) {
                        rejectedCount.incrementAndGet();
                    } else {
                        // 다른 예외도 거절로 간주 (Bulkhead 관련)
                        rejectedCount.incrementAndGet();
                    }
                } finally {
                    doneLatch.countDown();
                }
            });
        }

        startLatch.countDown(); // 모든 요청 동시 시작
        doneLatch.await(5, TimeUnit.SECONDS); // 모든 요청 완료 대기
        executorService.shutdown();

        // then
        System.out.println("[DEBUG_LOG] Success count: " + successCount.get());
        System.out.println("[DEBUG_LOG] Rejected count: " + rejectedCount.get());
        
        // maxConcurrentCalls(5)를 초과하는 요청은 거절되어야 함
        assertThat(successCount.get()).isLessThanOrEqualTo(maxConcurrentCalls);
        // 일부 요청은 거절되어야 함 (10개 요청 중 5개 초과분)
        assertThat(rejectedCount.get()).isGreaterThan(0);
        // 전체 처리된 요청 수 확인
        assertThat(successCount.get() + rejectedCount.get()).isEqualTo(totalRequests);
    }
```

**application-test.yml**

```yml
resilience4j:
  bulkhead:
    instances:
      excelStream: # 벌크헤드의 이름
        maxConcurrentCalls: 5   # 동시에 5건까지만 스트리밍 생성
        maxWaitDuration: 100ms    # 100ms 초과시 거절
```

테스트를 실행해 보면 다음과 같이 테스트가 성공하고, 10건의 동시 요청 중 5건만 수행되고 나머지 5건은 거절된 걸 확인할 수 있습니다.

![Bulkhead test result](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0hj1my13497fwex16t4m.png)

## SEMAPHORE vs THREADPOOL

Resilience4j의 `Bulkhead`는 내부적으로 두 가지 실행 모델을 제공합니다. 

`SEMAPHORE`와 `THREADPOOL` 두 방식 모두 동시 처리량을 제한하고 격리하는 목적은 같지만, 격리 수준과 적용 시점에서 차이가 있습니다.

### SemaphoreBulkhead

`SemaphoreBulkhead`는 `java.util.concurrent.Semaphore`를 내부적으로 사용하여 동시 호출 수를 제어하며, 현재 스레드에서 동기적으로 코드를 실행합니다.

자바의 세마포어는 `permit`이라는 내부 카운터를 기반으로 동시 접근을 제어하는 동기화 메커니즘입니다. Resilience4j의 Bulkhead는 `maxConcurrentCalls`에 설정된 최대 동시 호출 수만큼 permit이 초기화됩니다. 

![Semaphore](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/apiqg6dl2liqonzodw6s.png)

![new Semaphore](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0pmv6ci7zedqn40b4jb4.png)

스레드가 호출을 시도할 때, 설정된 동시 호출 수 한도 내라면 즉시 실행되고 한도를 초과했다면 `maxWaitDuration` 동안 대기하게 됩니다. 이 시간 내에 permit을 얻지 못하면 `BulkheadFullException`이 발생하게 됩니다.

![try lock](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pp3zihqvlwq5q6bguvv3.png)


![try lock 2](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/i74x9dyeahihiszbjoda.png)

세마포어 방식은 다양한 스레딩 모델과 I/O 모델에서 잘 동작하며, 호출 응답이 빠를고 예측 가능한 동기 작업에 효과적입니다. 별도의 스레드 풀을 관리하지 않으므로 오버헤드가 적고, 단순한 동시성 제어가 필요한 경우에 유용합니다.

### ThreadPoolBulkhead

`ThreadPoolBulkhead`는 내부적으로 `ArrayBlockingQueue`와 `ThreadPoolExecutor`를 사용하여 작업을 비동기적으로 실행하며, 
특정 작업을 전용 스레드 풀로 격리하여 동시 호출을 관리합니다.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7pbudt2v42pntvnrtbjm.png)

스레드 풀에 사용 가능한 스레드가 있다면 작업이 즉시 실행되고, 모든 스레드가 사용 중이면 작업은 큐에 추가되어 스레드가 사용 가능해질 때까지 대기합니다. 만약 큐까지 가득 차면 `BulkheadFullException`이 발생하여 요청이 거부됩니다.

이러한 스레드 풀 방식은 응답 시간이 길거나 예측 불가능한 I/O 바운드 작업에 적합합니다. 비동기 처리가 필요하거나, 느린 외부 서비스 호출로부터 메인 스레드 풀을 보호해야 하는 경우에 적합합니다.

### 언제 뭘 사용해야 할까

세마포어는 호출 스레드에서 직접 동기적으로 실행되기 때문에 별도의 스레드 생성이나 컨텍스트 스위칭 등의 오버헤드가 발생하지 않습니다. 만약 수행시간이 적게 걸리는 작업이라면 스레드 풀로 작업을 위임하고 컨텍스트 스위칭하는 데 드는 비용이 오히려 더 클 수 있습니다.

반면 느린 작업에 세마포어를 사용할 경우 호출

단순히 동시 호출 수만 제한하는 것이 목적이라면 세마포어 방식이 효율적입니다. 하지만 외부 API 호출처럼 작업 시간이 길고 완전한 리소스 격리가 필요하다면 전용 스레드 풀을 할당하는 스레드 풀 방식이 더 적합합니다.

## 마무리

지금까지 Bulkhead 패턴과 Resilience4j를 활용하여 시스템 전체의 안정성을 확보하는 방법을 알아보았습니다.

촉박한 개발 일정 속에서 장애 방어 로직을 추가하는 것이 때로는 오버엔지니어링처럼 느껴질 수 있습니다. 하지만 리소스 소모가 많고 장애 전파 위험이 큰 기능에서 발생하는 한 번의 장애는, 시스템 전체를 마비시키며 훨씬 더 큰 비용을 초래할 수 있습니다.

특히 Resilience4j는 Bulkhead 뿐만 아니라 서킷 브레이커와 같은 강력한 회복탄력성 패턴들을 제공합니다. 복잡한 구현 없이 어노테이션과 간단한 설정만으로 시스템의 안정성을 크게 높일 수 있으니, 이 글이 도입에 도움이 되었으면 합니다.


---

## 10억 달러짜리 실수 해결하기: JSpecify와 NullAway를 사용한 최신 Java Null 안전성

- **URL**: https://headf1rst.github.io/log/ko/post10
- **Date**: 2025-09-24
- **Tags**: `JSpecify`
- **Summary**: JSpecify 어노테이션과 NullAway 정적 분석을 사용하여 현대 Java 애플리케이션에서 NullPointerException을 제거하는 방법 학습

### Content


자바 프로그래밍을 처음 시작한 개발자부터 20년 경력의 시니어 개발자까지, 경력을 불문하고 개발자들이 가장 자주 마주치는 에러는 **NullPointerException**일 것입니다.

![Top Crash Reasons](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v82uppsp39zvfkuuwr1m.png)

실제로 한 통계 자료에 따르면 NullPointerException이 소프트웨어 결함 통계 2위에 해당할 정도로 많은 개발자가 NPE로 고통받고 있다고 하는데요.

"Null 참조를 만든 건 나의 10억 달러짜리 실수다"라고 [토니 호어(Tony Hoare)](https://news.ycombinator.com/item?id=12427069)의 말도 너무 유명하죠.

> "I call it my billion-dollar mistake. It was the invention of the null reference in 1965. … This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years."

이러한 null 안전성을 높이기 위해 Java에서는 다양한 시도들이 있었고, `Optional`, JSR 305의 `@NonNull` 어노테이션을 거쳐, 마침내 **JSpecify**가 표준 어노테이션으로 제정되었습니다.

지금부터 JSpecify를 사용하여 어떻게 프로젝트에 안정성을 높일 수 있는지, 신규 프로젝트뿐만 아니라 기존 시스템에 도입하기에도 부담이 없을지에 대해 소개해 드리고자 합니다.

## NullPointerException: 왜 여전히 문제인가?

토큰을 추출하는 API를 호출하고, 그 결과를 아무런 확인 없이 사용하는 코드가 있습니다.

```java
// TokenExtractor.java
public interface TokenExtractor {
    String extractToken(String authorization);
}

// Main.java
TokenExtractor tokenExtractor = new DefaultTokenExtractor();
String token = tokenExtractor.extractToken("some-auth-header");
System.out.println("Token length: " + token.length()); // <-- NullPointerException 발생!
```

이 코드는 `extractToken` 메서드가 `null`을 반환할 가능성이 있을 때 `NullPointerException`을 발생시킵니다.

`extractToken` 메서드가 `null`을 반환하게 되면 `token`에 `null`값이 들어가고 `token.length()`로 `token` 값에 접근할 때 `NullPointerException`이 발생하게 됩니다.

이처럼 Java의 근본적인 문제는 Null 가능성(Nullability)이 암시적(implicit)이라는 점입니다.

API 문서에 명시되어 있지 않으면 개발자는 반환 값이 null일 수 있는지 아닌지 알기 어렵고, 이는 오해와 버그로 이어지게 됩니다.

## JSpecify: 명시적인 Null 안전성을 향한 표준

이 문제를 해결하기 위해 Google, JetBrains, Spring 등 여러 팀이 협력하여 JSpecify 표준을 만들었습니다.

JSpecify는 단순한 어노테이션 집합이 아니라, Null 안전성에 대한 명확한 명세를 제공하여 다양한 도구(IDE, 정적 분석기)가 일관되게 동작하도록 하는 것을 목표로 합니다.

JSpecify는 Null 가능성을 세 가지 상태로 정의합니다.

1. **미지정(Unspecified)**: Java의 기본 상태로, `null`일 수도 아닐 수도 있습니다.
2. **Nullable (@Nullable)**: 명시적으로 `null` 값을 가질 수 있음을 나타냅니다.
3. **Non-null (@NonNull)**: 절대 `null` 값을 가지지 않음을 보장합니다.

![JSpecify Null](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qp2g74jw6udz7f0x9nbf.png)

### JSpecify 의존성 추가

```gradle
implementation 'org.jspecify:jspecify:1.0.0'
```

이제 `TokenExtractor` 인터페이스에 어노테이션을 추가하여 Null 가능성을 명시할 수 있습니다.

```java
import org.jspecify.annotations.Nullable;

public interface TokenExtractor {
  
    @Nullable // 반환 값이 null일 수 있음을 명시
    String extractToken(String authorization);
}
```

이렇게 하면 IntelliJ IDEA와 같은 IDE는 `token.length()`를 호출하는 부분에서 잠재적인 `NullPointerException`을 경고해 주어, 개발자가 런타임 오류가 발생하기 전에 문제를 해결할 수 있게 도와줍니다.

![IntelliJ IDE Null](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/i34w3oga4na12thlupwp.png)

## 가독성 향상: `@NullMarked`로 기본값 설정하기

대부분의 경우(약 90%) API는 `null`이 아닌 값을 다룹니다. 모든 매개변수와 반환 값에 `@NonNull`을 붙이는 것은 귀찮은 일이며 코드를 지저분하게 만들고 자칫 가독성을 해칠 수 있습니다.

이를 해결하기 위해 JSpecify는 `@NullMarked` 어노테이션을 제공합니다.

`@NullMarked` 어노테이션을 패키지 수준(`package-info.java` 파일)에 적용하면 해당 패키지 내의 모든 타입은 기본적으로 Non-null로 간주됩니다.

```java
// src/main/java/com/example/package-info.java
@NullMarked
package com.example;

import org.jspecify.annotations.NullMarked;
```

이제 명시적으로 `@Nullable`을 붙인 경우를 제외하고는 모두 Non-null로 처리되므로 코드가 훨씬 깔끔하게 관리할 수 있습니다.

## 빌드 시간 검증: NullAway로 Null 안전성 강화하기

IDE의 경고는 유용하지만, 이를 무시하고 코드를 커밋하는 것을 막을 수는 없는데요.

Null 안전성을 강제하기 위해 [NullAway](https://github.com/uber/NullAway)와 같은 정적 분석 도구를 사용할 수 있습니다. `NullAway`는 Error Prone의 플러그인으로, 빌드 과정에서 JSpecify 어노테이션을 분석하여 Null 안전성 위반 시 빌드를 실패시킵니다.

Gradle에 다음과 같이 설정할 수 있습니다.

```gradle
plugins {
  id 'net.ltgt.errorprone' version '4.1.0'
}

dependencies {
    implementation 'org.jspecify:jspecify:1.0.0'
    
    errorprone "com.google.errorprone:error_prone_core:2.37.0"
    errorprone "com.uber.nullaway:nullaway:0.12.6"
}

tasks.withType(JavaCompile).configureEach {
    options.errorprone {
        disableAllChecks = true
        option("NullAway:OnlyNullMarked", "true")
        error("NullAway")
    }
}
```

IDE의 경고를 무시하고 빌드를 하면 다음과 같이 컴파일 시점에 에러가 발생하여 Null 안전성을 위반하는 코드가 배포되는 것을 막을 수 있습니다.

![build error](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/k82l826lqbmlzs5rclpu.png)

## Spring 생태계에서의 JSpecify

Spring Framework 7(Spring Boot 4에 포함)부터는 코드베이스 전체가 JSpecify 어노테이션으로 마이그레이션되었습니다. 이는 Spring 개발자들이 별도의 설정 없이도 Spring API의 Null 안전성 정보를 IDE와 빌드 도구에서 바로 활용할 수 있음을 의미합니다.

예를 들어, `RestClient`의 `.body()` 메서드는 `@Nullable String`을 반환하므로, 개발자는 반환 값이 `null`일 가능성을 인지하고 적절히 처리해야 합니다.

```java
// Spring의 RestClient API
@Nullable
String body = restClient.get().uri("/user").retrieve().body(String.class);

// IDE는 body가 null일 수 있음을 경고
System.out.println(body.length());
```

## 앞으로의 Java의 Null 안전성

장기적으로 Java 언어 자체에 Null 안전성 기능이 도입될 예정이라고 하는데요.

`?` (nullable)와 `!` (non-null) 같은 새로운 구문이 제안되었지만, Java의 하위 호환성 원칙 때문에 기본값은 여전히 "미지정(unspecified)"으로 남을 것이라고 합니다. [(https://openjdk.org/jeps/8303099)](https://openjdk.org/jeps/8303099)

하지만 어디까지나 제안 단계이고 이 기능이 현실화되기까지는 수년이 걸릴 것이므로, 현재로서는 JSpecify와 NullAway가 Java 애플리케이션의 안정성을 높이는 가장 현실적이고 강력한 방법입니다.

## 마무리

JSpecify와 NullAway는 Java의 "10억 달러짜리 실수"를 해결하기 위한 강력한 조합입니다. 명시적인 어노테이션을 통해 코드의 의도를 명확히 하고, IDE와 빌드 도구를 연동하여 컴파일 시간에 잠재적인 NullPointerException을 제거할 수 있습니다.

실제로 팀에 JSpecify를 공유하고 프로젝트에 적용하면서 느낀 경험에 비추어 볼때, 간단한 설정으로 애플리케이션 안정성과 코드 품질이 향상되는 것을 경험하였으며, 패키지 단위의 점진적 도입이 가능하기 때문에 기존 프로젝트에 적용하는 데 부담이 적다는 것 역시 큰 장점으로 느껴졌습니다.

이 글을 읽어주시는 분들께서도 JSpecify를 프로젝트에 적용하여 더 안전하고 견고한 코드를 작성해 보시기 바랍니다.


---

## Anthropic 엔지니어들의 프롬프트 엔지니어링 팁

- **URL**: https://headf1rst.github.io/log/ko/post22
- **Date**: 2025-06-24
- **Tags**: `AI`
- **Summary**: Anthropic 엔지니어들이 공유한 가치 있는 프롬프트 엔지니어링 팁과 통찰

### Content


[youtube (AI prompt engineering: A deep dive)](https://www.youtube.com/watch?v=T9aRN5JkmL8&t=2463s)에서 Anthropic 엔지니어들이 그들의 프롬프트 작성 팁과 경험을 공유했는데, 여기서 인상 깊었던 몇 가지 포인트를 소개합니다.

## 인상 깊었던 프롬프트 작성 팁

* 모델이 실수를 했을 때, 왜 틀렸는지 묻고, 다음에는 어떻게 지시해야 틀리지 않을지 모델에게 직접 물어보는 것이 도움이 될 수 있다.
* 모델이 예상치 못한 입력이나 모호한 상황에 직면했을 때 어떻게 해야 할지 (예: "불확실" 태그 출력) 명확한 지시를 제공하여 잘못된 답변을 내놓는 것을 방지해야 한다.
* 모델이 답변을 제공하기 전에 자신의 추론 과정을 설명하도록 유도하면(Chain of Thought) 모델의 정확도가 향상된다.
* 모델에게 자신을 "인터뷰"하도록 요청할 수 있다. 자신이 원하는 바를 명확히 파악하기 어려울 때, 모델에게 나에게 필요한 정보를 질문하고 이끌어내도록 요청하여 프롬프트를 구성하는 데 도움을 받을 수 있다.

## 프롬프트 엔지니어링 기술 향상 팁

* **다른 사람의 프롬프트와 모델 출력 많이 읽기**: 훌륭한 프롬프트를 보고 그 구조와 의도를 분석하며, 모델의 출력을 면밀히 관찰하면 많은 것을 배울 수 있다.
* **모델과 많은 대화 시도**: 직접 모델과 반복적으로 소통하면서 프롬프트를 개선하는 연습을 해야 한다.
* **타인에게 프롬프트 보여주기**: 자신이 작성한 프롬프트를 그 작업에 대한 사전 지식이 없는 다른 사람에게 보여주면, 명확하지 않은 부분을 발견하는 데 도움이 된다.
* **모델의 한계 시험**: 모델이 할 수 없을 것이라고 생각하는 가장 어려운 작업을 시도하며 모델의 한계를 탐색하는 것이 가장 큰 학습으로 이어진다.


---

## 조인 테이블이 왜 생기지? @JoinColumn으로 해결하는 연관관계 매핑의 비밀

- **URL**: https://headf1rst.github.io/log/ko/post23
- **Date**: 2024-10-10 10:00
- **Tags**: `JPA`
- **Summary**: JoinColumn

### Content


`@JoinColumn`은 외래키를 매핑할 때 사용한다. 즉, 한 엔티티에서 다른 엔티티를 참조(조인)하는데 사용되는 필드를 지정하는 역할을 한다.

```java
@Entity  
@Table(name = "orders")  
@Getter
@NoArgsConstructor  
public class Order {  
  
    @Id  
    @GeneratedValue(strategy = GenerationType.IDENTITY)  
    private Long id;  
  
    @ManyToOne  
    @JoinColumn(name = "customer_id") // 외래 키로 지정
    private Customer customer;
  
    public Order(Customer customer) {  
        this.customer = customer;  
    }  
}
```

```java
@Entity  
@Getter  
@NoArgsConstructor  
public class Customer {  
  
    @Id  
    @GeneratedValue(strategy = GenerationType.IDENTITY)  
    private Long id;  
  
    private String name;  
  
    public Customer(String name) {  
        this.name = name;  
    }  
}
```

`@JoinColumn(name = "customer_id")`는 Order 테이블에서 `customer_id`라는 컬럼을 통해 `Customer` 엔티티를 참조하는 외래 키를 정의한다.

`@ManyToOne`의 기본 FetchType이 EAGER이기 때문에, Order 조회시 연관된 Customer 엔티티도 함께 `@JoinColumn`에 명시된 필드를 통해 참조(조인)하여 가져오는걸 볼 수 있다.

```java
@SpringBootTest  
class OrderTest {  
  
    @Autowired  
    private OrderRepository orderRepository;  
    @Autowired  
    private CustomerRepository customerRepository;  
  
    @Test  
    void joinColumnTest() {  
        var customer = customerRepository.save( new Customer("John Doe"));  
        var order = orderRepository.save(new Order(customer));  
  
        Order result = orderRepository.findById(order.getId()).get();  
    }
}
```

```
Hibernate: 
    select
        o1_0.id,
        c1_0.id,
        c1_0.name 
    from
        orders o1_0 
    left join
        customer c1_0 
            on c1_0.id=o1_0.customer_id 
    where
        o1_0.id=?
```

`@JoinColumn`은 ConstraintMode 옵션 값을 통해서 외래 키 제약 조건을 걸 수 있다.

- **ConstraintMode.PROVIDER_DEFAULT** (기본값): JPA Provider의 외래 키 제약 조건 생성 전략을 따른다. Hibernate는 이 옵션 값에서 외래 키 제약 조건을 설정한다.
- **ConstraintMode.CONSTRAINT**: 해당 외래 키 컬럼에 데이터베이스 레벨에서 외래 키 제약 조건을 설정한다.
- **ConstraintMode.NO_CONSTRAIN**: DB에 외래키 제약조건을 걸지 않는다.
    - 외래 키 컬럼은 생성되지만 DB가 그 컬럼의 값을 검증하거나 무결성을 유지하지는 않는다.

# @JoinColumn은 생략해도 될까

단방향 `@ManyToOne` 에선 `@JoinColumn`을 생략 할 수 있다. `@JoinColumn`을 생략하면 외래 키를 찾을 때 기본 전략을 사용한다. 실제로 앞의 예제 코드에서 `@JoinColumn`을 생략하고 테스트를 실행했을때 동일한 로그가 출력되는걸 확인할 수 있었다.

> 기본 전략: 필드명 + _ + 참조하는 테이블의 컬럼명
> 
> ex) customer + _ + id -> customer_id

그러나 단방향 `@OneToMany` 연관관계에서 `@JoinColumn`을 생략할 경우, JPA는 조인 테이블을 생성하여 엔티티간의 관계를 관리한다.

```java
@Entity  
@Table(name = "orders")  
@Getter  
@NoArgsConstructor  
public class Order {  
  
    @Id  
    @GeneratedValue(strategy = GenerationType.IDENTITY)  
    private Long id;  

	@Column(name = "customer_id")
    private Long customerId;  
  
    public Order(Long customerId) {  
        this.customerId = customerId;  
    }  
}
```

```java
@Entity  
@Getter  
@NoArgsConstructor  
public class Customer {  
  
    @Id  
    @GeneratedValue(strategy = GenerationType.IDENTITY)  
    private Long id;  
  
    @OneToMany(cascade = CascadeType.ALL)  
    private List<Order> orders = new ArrayList<>();
  
    private String name;  
  
    public Customer(String name) {  
        this.name = name;  
    }  
}
```

```java
@SpringBootTest  
class OrderTest {  
  
    @Autowired  
    private OrderRepository orderRepository;  
    @Autowired  
    private CustomerRepository customerRepository;  
  
    @Test  
	@Transactional  
	public void joinColumnTest() {  
	    Customer customer = new Customer("John Doe");  
	    List<Order> orders = List.of(
		    new Order(customer.getId()), 
		    new Order(customer.getId())
		);
	    customer.getOrders().addAll(orders);  
  
	    customerRepository.save(customer);  
	    orderRepository.saveAll(orders);  
	}
}
```

테스트 실행 후 로그를 확인해보면 `customer_orders` 조인 테이블이 생성된것을 확인할 수 있다.

![log](https://i.imgur.com/v332oJ2.png)

# 왜 조인 테이블이 생성될까?

단방향 `@OneToMany` 관계에서는 JPA가 어느 테이블에 외래 키를 두어야 할지 명확하지 않기 때문에, 중간 조인 테이블을 생성하여 관계를 관리한다.

조인 테이블이 생성되는 것을 방지하고 싶다면, `@JoinColumn`을 명시적으로 설정하여 외래 키를 직접 관리하도록 해야 한다.
아래와 같이 `@JoinColumn(name = "customer_id"...)`를 설정하여 외래키를 Order 테이블에 설정할 수 있다.

```java
@Entity  
@Getter  
@NoArgsConstructor  
public class Customer {  
	//...
	
    @OneToMany(cascade = CascadeType.ALL)  
    @JoinColumn(name = "customer_id", foreignKey = @ForeignKey(ConstraintMode.NO_CONSTRAINT))  
    private List<Order> orders = new ArrayList<>();  
  
    //...
}
```

다시 테스트를 실행하고 로그를 확인해 보면 조인 테이블이 생성되지 않은걸 확인할 수 있다.

![log](https://i.imgur.com/1RLHtIp.png)

---

**참고 자료**
- [@ManyToOne을 사용할 때 @JoinColumn 생략](https://hyeon9mak.github.io/omit-join-column-when-using-many-to-one/)



---

## 내부 메서드 호출시 트랜잭션이 적용되지 않는 이슈

- **URL**: https://headf1rst.github.io/log/ko/post29
- **Date**: 2024-10-01 10:00
- **Tags**: `transactional`
- **Summary**: Transactional 내부 메서드 호출시 트랜잭션이 적용되지 않는 이슈

### Content


# @Transactional 동작 원리

## AOP와 프록시 패턴을 통한 트랜잭션 관리

AOP는 엔터프라이즈 애플리케이션 개발에서 객체지향 프로그래밍(OOP)의 한계를 보완해주는 보조적인 프로그래밍 기법이다. 이를 통해 트랜잭션, 캐싱, 로깅 같은 부가적인 관심사를 분리하고 모듈화하여 개발자가 비즈니스 로직에 집중할 수 있게 한다.

스프링은 프록시 패턴을 이용해 AOP를 지원한다. 프록시 객체는 실제 객체로의 메서드 호출을 가로채며, 비즈니스 로직이 실행되기 전후에 공통 기능을 적용하는 방식으로 동작한다.

트랜잭션 관리도 마찬가지로, @Transactional이 적용된 메서드가 호출되기 전후에 프록시 객체가 트랜잭션 시작과 종료를 처리한다. 그렇다면 이 프록시 객체는 어떻게 생성될까?

## Spring AOP의 두 가지 프록시 방식

스프링은 프록시 객체 생성을 위해 JDK 동적 프록시와 CGLIB 프록시, 두 가지 방식을 제공한다.

### 1. JDK 동적 프록시

JDK의 java.lang.reflect.Proxy 클래스를 이용해 인터페이스를 구현하는 프록시 객체를 런타임에 동적으로 생성한다. 타겟 객체는 최소 하나 이상의 인터페이스를 구현해야 하며, 그 인터페이스의 추상 메서드를 기반으로 프록시 객체를 생성한다.

![Drawing 2024-09-25 18.44.13.excalidraw](https://i.imgur.com/V8LdLQg.png)

인터페이스를 구현한 프록시 객체는 InvocationHandler를 통해 타겟 메서드로의 메서드 호출을 가로채고 부가 기능을 처리한다.

만약 트랜잭션 처리 이외에 로깅 처리를 추가로 타겟 메서드에 부여하면 그에 필요한 프록시 객체가 하나 더 생성되며 다음과 같은 순서로 객체가 호출된다.

> ProxyLogging -> ProxyTransaction -> Target

### 2. CGLIB 프록시 (기본 방식)

CGLIB(Code Generation Library)는 바이트코드를 조작해 런타임에 동적으로 클래스를 생성할 수 있는 라이브러리다. JDK 동적 프록시와 달리, 타겟 객체를 상속하는 프록시 객체를 생성하므로 타겟 객체가 인터페이스를 구현하지 않아도 프록시를 생성할 수 있다. 프록시 객체는 타겟 객체의 메서드를 오버라이드하여 호출을 가로채고, 메서드 실행 전후에 부가 기능을 처리한다.

스프링 부트 2.x 이후에는 타깃 객체가 인터페이스를 구현하고 있는지와 상관없이 기본적으로 CGLIB 프록시 방식으로 트랜잭션이 동작한다.

> [AOP in Spring Boot, is it a JDK dynamic proxy or a Cglib dynamic proxy?](https://www.springcloud.io/post/2022-01/springboot-aop/#gsc.tab=0)

# 내부 메서드와 AOP 프록시

@Transactional이 어떤 원리로 동작하는지 알아보았으니, 이제 동일 클래스 내에서 내부 메서드를 호출했을 때 트랜잭션이 적용되지 않는 경우와 그 이유를 살펴보자.

```java
@Service  
@RequiredArgsConstructor  
public class TransportService {  
  
    private final TransportRepository transportRepository;  
  
    public void sendTransportEvents(List<Long> transportIds) {  
        transportRepository.changeStatuses(transportIds);  
        // 전송...  
    }  
}
```

```java
@Repository  
public interface TransportRepository {  
  
    void save(Transport transport);  
  
    default void changeStatuses(List<Long> transportIds) {  
        for (Long transportId : transportIds) {  
            changeStatus(transportId);  
        }  
    }  
  
    void changeStatus(Long transportId);  
  
    Optional<Transport> findById(Long id);  
}
```

```java
@Repository  
@RequiredArgsConstructor  
public class TransportRepositoryImpl implements TransportRepository {  
  
    private final TransportJpaRepository transportJpaRepository;  
  
    @Override  
    public void save(Transport transport) {  
        transportJpaRepository.save(transport);  
    }

    @Transactional(propagation = Propagation.REQUIRES_NEW)
    @Override    
    public void changeStatus(Long transportId) {  
        Transport transport = transportJpaRepository.findById(transportId).orElseThrow();  
        transport.updateStatus(TransportStatus.SENT);  
    }  
  
    @Override  
    public Optional<Transport> findById(Long id) {  
        return transportJpaRepository.findById(id);  
    }  
}
```

```java
@SpringBootTest  
class TransportServiceTest {  
  
    @Autowired  
    private TransportService sut;  
    @Autowired  
    private TransportRepository transportRepository;  
  
    @Test  
    void 운송_이벤트가_전송되면서_상태를_SENT로_변경한다() {  
        // given  
        var transportA = new Transport(1L, TransportStatus.PENDING);  
        var transportB = new Transport(2L, TransportStatus.PENDING);  
        List.of(transportA, transportB).forEach(transportRepository::save);  
  
        // when  
        sut.sendTransportEvents(List.of(1L, 2L));  
  
        // then  
        var result1 = transportRepository.findById(1L).get();  
        var result2 = transportRepository.findById(2L).get();  
  
        assertThat(result1.getStatus()).isEqualTo(TransportStatus.SENT);  
        assertThat(result2.getStatus()).isEqualTo(TransportStatus.SENT);  
    }  
}
```

TransportRepositoryImpl.changeStatus(transportId)에서 JPA의 더티 체킹을 활용해 transport의 상태 값을 PENDING에서 SENT로 변경을 시도하고 있다.

JPA의 더티 체킹이 의도한 대로 동작한다면 테스트가 통과해야 하지만 테스트가 실패하며 상태 값은 그대로 PENDING으로 변하지 않은 것을 확인할 수 있었다.

![스크린샷 2024-10-01 오전 2.05.54.png](https://i.imgur.com/RI75mwO.png)

## 왜 이런 일이 발생할까?

```java
@Repository  
@RequiredArgsConstructor  
@Slf4j  
public class TransportRepositoryImpl implements TransportRepository {  
  
    private final TransportJpaRepository transportJpaRepository;  
    private final EntityManager em;  

	// ...

    @Transactional(propagation = Propagation.REQUIRES_NEW)
    @Override    
    public void changeStatus(Long transportId) {    
        Transport transport = transportJpaRepository.findById(transportId).orElseThrow();  
        transport.updateStatus(TransportStatus.SENT);  
  
        if (em.contains(transport)) {  
            log.info("Entity is managed");  
        } else {  
            log.info("Entity is detached");  
        }
        
        log.info("Transaction active: {}", TransactionSynchronizationManager.isActualTransactionActive());  
    }
}
```

더티 체킹이 정상적으로 동작하려면 엔티티가 영속성 컨텍스트에서 관리되고 있어야 한다. 하지만 아래 로그를 보면 Transport 엔티티가 영속성 컨텍스트에 관리되지 않는 것을 확인할 수 있는데, 이는 해당 메서드에 트랜잭션이 적용되지 않았기 때문이다.

![스크린샷 2024-10-01 오전 2.35.24.png](https://i.imgur.com/BANQI7X.png)

영속성 컨텍스트의 생명주기는 트랜잭션과 동일하다. 트랜잭션이 활성화되지 않으면 영속성 컨텍스트가 생성되지 않고, 따라서 엔티티도 영속성 컨텍스트에서 관리되지 않는다. 결국, 트랜잭션이 적용되지 않은 `changeStatus()` 메서드에서는 영속성 컨텍스트가 생성되지 않기 때문에 더티 체킹이 동작하지 않는 것이다.

스프링은 CGLIB을 사용해 `TransportRepositoryImpl` 객체를 상속하는 프록시 객체를 생성한다. 이 프록시 객체는 `@Transactional`이 붙은 `changeStatus()` 메서드를 오버라이드하여, 트랜잭션 관련 로직을 메서드 호출 전후에 삽입한다.

문제는 `changeStatuses()` 메서드가 같은 클래스 내에서 `changeStatus()`를 직접 호출하는 방식으로 작성되어 발생한다. 스프링 프록시는 클래스 외부에서 호출되는 경우에만 트랜잭션을 처리할 수 있으며, 자기 자신 내에서 호출하는 메서드는 프록시를 거치지 않고 호출되기 때문에 트랜잭션이 적용되지 않는다.

### 왜 내부 호출은 프록시를 거치지 않을까?

스프링 AOP에서 프록시는 외부 클라이언트가 호출할 때 동작한다. 외부에서 프록시 객체를 통해 메서드를 호출하면, 프록시는 이 호출을 가로채고 트랜잭션 등의 부가 기능을 처리한다. 그러나 동일 클래스 내에서 호출하는 경우는 외부 호출이 아니라 자기 자신의 인스턴스를 통해 메서드가 호출되는 것이기 때문에, 프록시를 거치지 않고 실제 메서드가 직접 호출된다.

이를 해결하기 위해 `@Transactional`을 외부에서 호출되는 메서드에 적용하는 것이 권장된다. 혹은 내부 메서드를 별도의 클래스로 분리해 외부에서 호출할 수 있도록 설계하는 방법도 있다.

### 마무리

@Transactional의 동작 원리에 대해 어느 정도 알고 있다고 생각했는데 이번 문제를 겪으면서 아직 명확히 정리되어 있지 않다는 것을 느끼게 되었다.

글을 작성하면서 정리되지 않고 점으로만 존재하던 지식이 선으로 연결되는 듯한 느낌을 받았다. 바쁘다고 글 쓰는 걸 미루지 말고 글로 지식과 경험을 내재화하는 습관을 가질 수 있도록 노력해야겠다.

---

**참고 자료**

- [Proxying Mechanisms](https://docs.spring.io/spring-framework/reference/core/aop/proxying.html)
- [In Java, why are dynamic proxies only allowed to proxy interface classes?](https://www.quora.com/In-Java-why-are-dynamic-proxies-only-allowed-to-proxy-interface-classes)


---

## Gson 라이브러리 InaccessibleObjectException

- **URL**: https://headf1rst.github.io/log/ko/post30
- **Date**: 2024-07-02 10:00
- **Tags**: `gson` `Java`
- **Summary**: gson

### Content


Spring Boot 2.5.x 버전에서 3.2.x 버전으로 마이그레이션 하는 과정에서 InaccessibleObjectException이 발생하였다. Gson 라이브러리를 사용하는 쪽에서 발생한 문제였는데, 이에 대한 트러블 슈팅 과정을 정리해 보고자 한다.

## Gson 라이브러리 란?

구글이 만든 자바 기반 라이브러리로, 자바 객체를 JSON으로 직렬화(Serialization)하거나 JSON을 자바 객체로 역직렬화(Deserialization)하는 작업을 쉽게 할 수 있도록 도와준다.

### 직렬화 과정 내부 동작

`Gson.toJson(Object)` 메서드는 객체를 전달받아, 인자로 전달된 객체를 Json으로 변환하여 반환한다.

```java
Gson gson = new Gson();
String json = gson.toJson(myObject);
```

`toJson(Object src)` 메서드는 내부적으로 객체의 타입을 추론한 후 `toJson(Object src, Type typeOfSrc, Appendable writer)` 메서드를 호출한다.

호출된 `toJson` 메서드는 주어진 객체 타입에 맞는 `TypeAdapter`를 찾고, 이를 사용하여 객체를 JSON으로 변환한다. 이때 객체 타입에 맞는 TypeAdapter를 찾기 위해 `getAdapter(TypeToken)` 메서드를 호출한다.

```java
public void toJson(Object src, Type typeOfSrc, JsonWriter writer) throws JsonIOException {  
    TypeAdapter<Object> adapter = (TypeAdapter<Object>) getAdapter(TypeToken.get(typeOfSrc));
    // ...
}
```

`getAdapter(TypeToken)` 메서드는 주어진 타입에 대한 `TypeAdapter`를 먼저 캐시에서 찾고, 없으면 `TypeAdapterFactory` 리스트를 순회하면서 각 팩토리에 대해 `create` 메서드를 호출하여 주어진 타입에 대한 `TypeAdapter`를 생성할 수 있는지 확인한다.

```java
public <T> TypeAdapter<T> getAdapter(TypeToken<T> type) {  
  TypeAdapter<?> cached = typeTokenCache.get(type);  
  if (cached != null) {  
    TypeAdapter<T> adapter = (TypeAdapter<T>) cached;  
    return adapter;  
  }  

  // ...
  
  TypeAdapter<T> candidate = null;  
  try {  
    FutureTypeAdapter<T> call = new FutureTypeAdapter<>();  
    threadCalls.put(type, call);  
  
    for (TypeAdapterFactory factory : factories) {  
      candidate = factory.create(this, type);  
      if (candidate != null) {  
        call.setDelegate(candidate);  
        // Replace future adapter with actual adapter  
        threadCalls.put(type, candidate);  
        break;  
      }  
    }  
  } 
  // ...
  return candidate;  
}
```

TypeAdapterFactory 리스트를 순회하면서, 먼저 사용자 정의 또는 특수한 TypeAdapterFactory를 확인하고, 적절한 TypeAdapter를 찾지 못하면 **ReflectiveTypeAdapterFactory**를 사용하여 객체의 필드에 접근하고 JSON으로 직렬화 한다.

## 문제 원인

> Unable to make field private final java.time.LocalDate java.time.LocalDateTime.date accessible: module java.base does not "opens java.time" to unnamed module.

로그인 시에 `ValidToken` 객체를 직렬화하여 Redis에 저장하는 과정에서 Gson 라이브러리를 사용하는데, 위와 같은 예외가 발생했다.

Spring Boot 3.2.x로 버전업하면서 Java 버전 또한 11에서 17로 업그레이드 하였다.
Java 17 이상 부터는 모듈화된 애플리케이션에서 리플렉션을 통해 다른 모듈의 private 필드에 접근하는 것이 제한되도록 변경 되었기 때문에, Gson 라이브러리를 사용하여 TypeAdapterFactory 리스트를 순회하는 과정에서 문제가 발생하는것 이었다.

Gson은 먼저 사용자 정의 또는 특수한 `TypeAdapterFactory`를 확인하고, 적절한 `TypeAdapter`를 찾지 못하면 `ReflectiveTypeAdapterFactory`를 사용하게 된다.

`ReflectiveTypeAdapterFactory`는 객체의 필드를 탐색하고, `field.setAccessible(true)`를 호출하여 private 필드를 접근 가능하도록 설정한다. 하지만 Java 17 이상의 모듈 시스템에서는 모듈화된 
애플리케이션에서 이러한 호출이 기본적으로 제한되기 때문에 `ReflectiveTypeAdapterFactory`가 private 필드에 접근할 수 없게 되어, 결과적으로 직렬화 및 역직렬화 과정에서 예외가 발생하게 되는 것이다.

## 해결방법

private 필드를 가진 클래스 타입에 대한 직렬화 및 역직렬화 로직이 구현된 커스텀 TypeAdapter를 구현한면 Gson이 TypeAdapterFactory 리스트를 순회할 때, 정의한 커스텀 TypeAdapter가 ReflectiveTypeAdapterFactory 보다 우선순위를 갖기 때문에 문제를 해결할 수 있다.

```java
public class LocalDateTimeAdapter extends TypeAdapter<LocalDateTime> {

    private static final DateTimeFormatter formatter = DateTimeFormatter.ISO_LOCAL_DATE_TIME;

    @Override
    public void write(JsonWriter jsonWriter, LocalDateTime localDateTime) throws IOException {
        jsonWriter.value(localDateTime.format(formatter));
    }

    @Override
    public LocalDateTime read(JsonReader jsonReader) throws IOException {
        return LocalDateTime.parse(jsonReader.nextString(), formatter);
    }
}
```

## 참고 자료
- [Error reflection JDK 17 and gson](https://github.com/google/gson/issues/1979)
- [Gson 공식 트러블 슈팅 가이드 문서](https://github.com/google/gson/blob/main/Troubleshooting.md#reflection-inaccessible)


---

## Fixture Monkey With Kotlin

- **URL**: https://headf1rst.github.io/log/ko/post20
- **Date**: 2024-03-03 10:00
- **Tags**: `test`
- **Summary**: fixtureMonkey

### Content


테스트를 작성하다보면 프로덕션 코드를 작성하는 시간보다 테스트를 위한 픽스처를 만드는데 더 많은 시간이 소요될 때가 있다.

테스트를 작성하는데 시간이 많이 들고 번거로울 수록 테스트 코드를 생략하게 되고 결국 결함에 취약한 시스템을 구현할 위험이 있다.

주문 관련 로직을 테스트 하기위해서 `Order` 픽스처를 만들어야했었는데 `Order` 객체 내부에 필드가 24개 였을뿐더러 내부 객체의 필드까지 합하면 정의해줘야 하는 필드가 수 없이 많았다.

심지어 케이스마다 다른 상태값을 갖고있는 `Order` 픽스처를 추가로 만들어줘야 했기 때문에 테스트 준비에 많은 시간이 소요됐던 경험이 있다.

그러던중 `Fixture Monkey`라는 PBT(Property Based Testing) 라이브러리를 알게되었고, `Fixture Monkey`를 사용하여 테스트를 편리하게 작성할 수 있었던 경험이 있었는데, 개인적으로 유용하게 사용했던 핵심 내용만 간략하게 소개하고자 한다.

## FixtureMonkey의 주요 기능

- 렌덤하고 복잡한 제약조건을 갖는 객체를 생성해준다.
- 설정한 제약조건을 검증할 수 있다.
- 테스트케이스마다 다르게 객체 제어가 가능하다.

`FixtureMonkey`는 엔티티 필드에 지정된 Bean validation 어노테이션에 따라 유효한 속성값을 갖는 객체를 생성한다.

실패 테스트 작성과 같이 특정 테스트 케이스에서 조건을 추가하거나 조건에 벗어난 필드를 설정해야할 경우에는 `ArbitraryBuilder`를 사용하여 fixture를 제어할 수 있다.

`ArbitraryBuilder`는 빌더 패턴을 사용하여 객체의 필드값을 원하는 값으로 설정하여 객체를 생성하는게 가능하다.

## 예제

### Without FixtureMonkey Test

`FixtureMonkey`의 편리한 기능을 살펴보기에 앞서서 이전 방식으로 테스트를 작성해보았다.

다음은 주문 과정에서 입력된 배송지 주소가 유효한지 검증하는 테스트이다.

```kotlin
data class Order(  
    val product: List<Product>,  
    val purchaserName: String,  
    val receiver: Receiver,  
    val totalPrice: Long,  
    val coupon: List<Coupon>,  
    val delivery: Delivery,  
)
```

```kotlin
class OrderFixture {  
    companion object {  
        fun create(  
            id: Long = 1L,  
            product: List<Product> = listOf(  
                Product(name = "초콜릿", price = 300L),  
                Product(name = "키보드", price = 20000L),  
            ),  
            purchaserName: String = "홍길동",  
            receiver: Receiver = Receiver(name = "홍길동", "01012341234"),  
            totalPrice: Long = 20300L,  
            coupon: List<Coupon> = listOf(Coupon()),  
            delivery: Delivery = Delivery("경기도", "203동 1023호", true),  
        ): Order {  
            return Order(  
                id = id,  
                product = product,  
                purchaserName = purchaserName,  
                receiver = receiver,  
                totalPrice = totalPrice,  
                coupon = coupon,  
                delivery = delivery,  
            )  
        }  
    }  
}
```

```kotlin
class OrderServiceTestWithOutFixtureMonkey : DescribeSpec({  
    val sut = OrderService()  
    val log = LoggerFactory.getLogger(this.javaClass)  
  
    describe("배송 주소 유효성 검사") {  
        it("유효성 검증을 통과한다.") {  
            val order = OrderFixture.create()  
  
            shouldNotThrowAny {  
                sut.validateDeliveryAddress(order)  
            }  
        }  
        it("지번 주소를 입력받았을 경우, 상세 주소가 없으면 안 된다") {  
            val order = OrderFixture.create(delivery = Delivery(baseAddress = "경기도", road = false, detailAddress = null))  
  
            val exception = shouldThrow<IllegalArgumentException> {  
                sut.validateDeliveryAddress(order)  
            }  
  
            exception.message shouldBe "지번 주소에는 상세 주소가 반드시 필요합니다."  
        }  
    }})
```

테스트 성공과 실패 케이스를 위한 `OrderFixture` 객체를 정의하여 사용하였다.
`Order` 객체에 정의된 필드 뿐만 아니라 연관된 객체의 필드값들도 함께 정의해줘야하기 때문에 꽤나 번거로운 작업일 수 있다.

만약 연관된 엔티티가 더 많고 정의해줘야하는 필드 수가 훨씬 많아진다면 테스트를 위한 fixture를 정의하는데 큰 비용이 소모되게 된다.

### FixtureMonkey Test

이번엔 `FixtureMonkey`를 사용하여 간단하게 fixture를 생성하여 테스트를 작성해보도록 하겠다.

먼저 `build.gradle.kts`에 의존성을 추가해주었다.
```gradle
// fixture monkey  
testImplementation("com.navercorp.fixturemonkey:fixture-monkey-starter-kotlin:1.0.14")  
testImplementation("com.navercorp.fixturemonkey:fixture-monkey-jakarta-validation:0.6.3")  
testImplementation("com.navercorp.fixturemonkey:fixture-monkey-jackson:0.6.3")
```

**DefaultMonkeyCreator**
```kotlin
fun monkey() : FixtureMonkey {  
    return FixtureMonkey.builder()  
        .plugin(KotlinPlugin())  
        .build()  
}
```

**FixtureBuilders**
```kotlin
fun <T> defaultFixtureBuilder(clazz: Class<T>): ArbitraryBuilder<T> {  
    return monkey().giveMeBuilder(clazz)  
}
```

```kotlin
class OrderServiceTestWithFixtureMonkey: DescribeSpec({  
    val sut = OrderService()  
    val log = LoggerFactory.getLogger(this.javaClass)  
  
    describe("배송 주소 유효성 검사") {  
        it("유효성 검증을 통과한다.") {  
            val order = defaultFixtureBuilder(Order::class.java)  
                .setExp(  
                    Order::delivery,  
                    Delivery(baseAddress = "경기도", detailAddress = null, road = true))  
                .sample()  
  
            shouldNotThrowAny {  
                sut.validateDeliveryAddress(order)  
            }  
        }  
        it("지번 주소를 입력받았을 경우, 상세 주소가 없으면 안 된다") {  
            val order = defaultFixtureBuilder(Order::class.java)  
                .setExp(  
                    Order::delivery,  
                    Delivery(baseAddress = "경기도", detailAddress = null, road = false))  
                .sample()  
  
            val exception = shouldThrow<IllegalArgumentException> {  
                sut.validateDeliveryAddress(order)  
            }  
  
            exception.message shouldBe "지번 주소에는 상세 주소가 반드시 필요합니다."  
        }  
    }})
```

이처럼 `FixtureMonkey`를 사용하면 랜덤한 필드값을 갖는 fixture 객체를 생성하여 사용할 수 있다.

`setter`를 통해서 객체를 제어할 수 있는데, 테스트하고자 하는 필드를 명확히 표현하기 때문에 테스트 관심사를 바로 파악할 수 있다는 장점도 있다.


---

## 객체지향과 탈 국지화

- **URL**: https://headf1rst.github.io/log/ko/post19
- **Date**: 2024-02-04 10:00
- **Tags**: `객체지향`
- **Summary**: 객체지향과 탈 국지화

### Content


최근 두 권의 책을 병행해서 읽는 중인데 서로 조금 상반되는 내용을 읽게 되어 내 생각을 정리해 보는 시간을 가져보았다.

한 권은 펠리너 헤르만스가 쓴 '[프로그래머의 뇌](https://m.yes24.com/Goods/Detail/105911017)'이고 다른 한 권은 조영호 님이 쓴 '[오브젝트](https://m.yes24.com/Goods/Detail/74219491)'이다.

두 권 모두 읽게 된 계기가 있다. 프로그래머의 뇌는 기능을 추가하거나 수정할 때 코드를 읽고 영향도를 파악하는 역량이 부족하다고 느껴서 읽게 되었고
오브젝트는 작년에 한 번 읽었지만 내용을 다 까먹기도 했고 마침 사내 스터디를 모집해서 읽게 되었다.

두 책의 상반된다고 느꼈던 부분을 먼저 정리해 보았다.

## 코드 분석과 인지 과정

'프로그래머의 뇌'에서는 개발자가 코드를 읽을 때 세 가지 인지 과정이 일어난다고 설명하고 있다.

첫 번째는 `LTM(Long Term Memory)`으로 컴퓨터의 하드디스크같이 장기 기억 저장소라고 할 수 있다.
이는 반복 학습을 통해서 개발자가 습득한 지식으로
이미 알고 있는 지식을 기반으로 코드를 청크내어 읽고 분석할 수 있기 때문에 코드를 파악하는데 드는 인지적 부하를 줄여준다.

다음과 같은 피보나치수열의 경우, 재귀 함수의 기본 구조에 대한 이해가 LTM에 없다면 코드를 파악하고 분석하는데 더 큰 비용과 노력이 필요하게 된다.

```kotlin
fun fibonacci(n: Int): Int {
    return if (n <= 1) n
    else fibonacci(n - 1) + fibonacci(n - 2)
}

fun main() {
    println(fibonacci(10))
}
```

두 번째는 `STM(Short Term Memory)`으로 컴퓨터의 메인 메모리와 유사하게, 한 번에 한정된 정보만을 처리할 수 있다.
새로운 코드를 읽을 때, 개발자는 변수 이름, 함수 호출, 알고리즘의 로직 등과 같은 여러 정보를 STM에 저장한다.
하지만 STM의 용량에는 제한이 있기 때문에 많은 정보를 저장하게 되면 일부를 **잃어버리게** 된다.

세 번째는 `작업 기억 공간`으로 컴퓨터의 프로세서와 같아서 LTM과 STM의 정보를 조합하여 연산하는 등의 사고 작용을 담당한다.

## 객체의 역할, 책임, 협력

'오브젝트'에서는 객체의 역할, 책임, 협력의 중요성에 대해서 강조하고 있다.

하나의 시스템은 여러 객체의 협력을 통해서 만들어지고, 이 과정에서 각 객체는 고유의 역할과 책임을 가지고 다른 객체로부터의 메시지를 처리한다.

이렇게 설계된 객체의 품질을 측정하는 척도로 응집도와 결합도가 사용될 수 있다.
잘 설계된 객체일수록 내부 상태 값 간의 응집도가 높고 다른 객체와의 결합도는 낮다. 이런 설계는 변경에 대해 유연하게 대응할 수 있게 한다. 반면, 응집도가 낮고 결합도가 높은 설계는 유연성이 떨어지게 된다.

이러한 응집도와 결합도를 결정하는 것은 캡슐화다. 즉, 잘 추상화된 객체가 좋은 설계를 의미하며, 객체의 품질은 캡슐화 수준에 따라 결정된다.

확실히 추상화가 가져오는 이점은 많다. 추상화는 복잡한 시스템을 단순화하고, 코드의 재사용성을 높이며, 유지보수를 용이하게 하고, 소프트웨어 설계의 유연성을 향상한다.

그러나 코드의 인지 과정에 대해 고려해 볼 때, 추상화 수준이 높아질수록 코드를 파악하는 데 필요한 노력이 증가할 수 있다는 생각이 들었다. 추상화가 복잡성을 감추는 동시에, 그 이해를 위해 더 많은 맥락을 파악해야 하기 때문이다.

## 코드의 탈 국지화에 의한 인지 과부하

객체 지향적으로 잘 작성된 코드는 SRP 원칙을 준수하며 변경 지점이 한곳이 되어 유지보수하기 좋은 코드가 된다.
하지만 코드가 유지보수하기 좋게 수정되었다고 해서 반드시 가독성까지 좋아지는 것은 아니다.

여러 객체의 협력이 필요한 복잡한 비즈니스 로직을 가정해 보자. 해당 로직을 구현하는 데 필요한 메시지들은 여러 적절한 객체에게 할당될 것이다. 그리고 각 객체는 자신에게 할당된 메시지를 처리하기 위해 필요한 정보를 갖고 있는 전문가에게 또 다른 메시지를 전달할 것이다.

이때 캡슐화를 높이기 위해서 메서드에 전달하는 인자의 수마저 최소화한다면 개발자는 내부 구현을 파악하기 위해서 해당 메서드가 정의되어 있는 객체들을 찾아 찾아 넘나들어야만 한다.

이렇게 탈 국지화된 코드는 여러 군데에서 메서드의 내부 구현을 찾아봐야 하므로 작업 기억 공간에는 어려움을 줄 수 있다.

'프로그래머의 뇌'에서는 유지보수하기 좋은 코드를 작성하기보다는 장기적으로 가독성이 높은 코드로 리팩터링 하는 방식을 `인지적 리팩터링`이라고 소개하고 있다.

그렇다면 객체지향을 통한 `유지보수`와 인지적 리팩터링을 통한 `가독성` 중 무엇을 선택해야 할까?

## 표식을 통한 인지 과부하 개선

변경될 가능성이 거의 없는 복잡한 비즈니스 로직이라면 인지적 리팩터링을 통한 가독성 개선을 고려해 볼 수 있다고 생각한다.

하지만 그 외에 변경될 가능성이 조금이라도 존재하는 영역은 유지보수를 고려한 설계를 해야 한다고 생각한다.

대신 탈 국지화에 의한 인지 과부하를 줄이기 위한 수단이 필요한데, 책에서는 **표식을** 남기는 것이 도움이 된다고 설명하고 있다. 여러 파일을 거쳐서 내부 구현을 파악하고 다니다 보면, 내부 구현을 파악하고 돌아왔을 때 그 내용을 잊어버리는 경우가 있을 수 있는데, 이때 내부 구현에 대한 내용을 함축한 표식을 보게 되면 쉽게 상기할 수 있다.

여기에서 표식은 **주석**이 될 수 있다. 주석 사용 여부가 종종 개발자들 사이에서 논쟁거리가 되곤 하지만 개발자들은 코드를 읽을 때 주석 문에 굉장히 많이 의존하며 고수준의 주석 문은 코드를 청크 단위로 쪼개는 데 도움을 준다고 논문을 기반으로 책에서는 설명하고 있다. (반면 저수준의 주석 문은 오히려 청킹 작업에 부담을 준다)

그렇다고 모든 함수의 결과를 주석으로 다는 것에 대해선 부정적으로 생각한다. 주석도 코드의 일부이기 때문에 코드를 파악하는 데 드는 시간이 늘어나기 때문이다. 코드 자체가 표식의 역할을 할 수 있다면 더 효율적일 것이다.

주석 대신 메서드명과 변수명이 자신의 역할을 잘 표현하고 있다면 코드를 표식으로 사용하는게 가능해진다.
이는 코드의 가독성을 자연스럽게 향상시키며, 개발자가 코드를 읽고 이해하는 데 소요되는 시간을 줄요준다.

즉, 가독성과 유지보수성 사이의 트레이드 오프를 최소화 하기 위해서는, 코드의 구조를 잘 설계하는 것과 더불어, 각 구성 요소의 네이밍에도 신경을 써야한다.

## 마무리

두 책을 읽고 나서의 나의 견해를 정리하자면 변화의 가능성이 없는 로직의 경우 가독성을 우선시해도 괜찮다고 생각한다. 그러나 대부분의 경우 변화를 예측하기란 어려운 일이기 때문에 가능한 모든 영역에서 유지보수를 고려한 객체 지향적 설계를 해야만 한다고 생각한다. 대신 의미 있는 변수명과 메서드명을 부여하여 탈 국지화된 코드를 파악하는 과정에서 표식 역할을 할 수 있도록 하는 게 중요하다.

변수명과 메서드명의 중요성은 OOP에 대해서 알기 이전인 프로그래밍을 처음 배웠을 때부터 들었던 내용이다.

그만큼 중요하다는 것이었는데 다시 한번 기본의 중요성을 느낄 수 있었고, 어느 순간 네이밍의 중요성을 잊고 지냈던 자신을 반성하는 시간을 가지는 계기가 되었다.


---

## JPA 트랜잭션과 영속성 컨텍스트

- **URL**: https://headf1rst.github.io/log/ko/post18
- **Date**: 2024-01-07 10:00
- **Tags**: `JPA`
- **Summary**: JPA 트랜잭션과 영속성 컨텍스트

### Content


최근 구현한 테스트 코드에서 `@Transactional` 여부에 따라 테스트 결과가 달라지는 문제를 만나게 되었다.

타 서비스로부터 송장 접수 결과에 대한 카프카 메세지를 소비한 다음, 송장 접수에 실패했다면 택배 등록 여부를 실패로 변경하는 로직에 대한 테스트 코드였는데 당시 상황을 간단하게 재현해 보았다.

```kotlin
@Entity  
@Table(name = "orders")  
class Order (  
    val shippingLabel: String,  
    var parcelStatus: Boolean = true,  
  
    @Id  
    @GeneratedValue(strategy = GenerationType.IDENTITY)  
    val id: Long? = null,  
) {  
    fun isParcelRegister(): Boolean {  
        return parcelStatus  
    }  
  
    fun updateParcelStatus(registerSuccess: Boolean) {  
        parcelStatus = registerSuccess  
    }  
}
```

```kotlin
@Service  
class OrderStatusService(  
    private val orderRepository: OrderRepository,  
) {  
    
    @Transactional  
    fun checkOrderSubmissionStatus(message: OutSourcingResultMessage) {  
        val order = orderRepository.findByShippingLabel(message.shippingLabel)  
        order.updateParcelStatus(message.registerSuccess)  
    }  
}
```

```kotlin
@SpringBootTest  
class OrderStatusChangeServiceTest @Autowired constructor(  
    private val orderStatusService: OrderStatusService,  
    private val orderRepository: OrderRepository,  
) {  
  
    @Test  
    @DisplayName("송장접수에 실패하면 택배등록 여부를 실패 표시 해야한다.")  
    fun checkOrderSubmissionStatusTest() {  
        val order = Order(shippingLabel = "12345")  
        val savedOrder = orderRepository.save(order)  
        val failureMessage = OutSourcingResultMessage(  
            shippingLabel = savedOrder.shippingLabel,  
            registerSuccess = false  
        )  
  
        orderStatusService.checkOrderSubmissionStatus(failureMessage)  
  
        order.isParcelRegister() shouldBe false  
    }  
}
```

OrderStatusChangeService의 checkOrderSubmissionStatus(failureMessage) 메서드에 `@Transactional`이 걸려있고 order의 `parcelStatus`를 true에서 **false**로 변경한다.

이때 JPA 더티 체킹을 활용하여 데이터베이스 컬럼값을 변경하기 때문에 checkOrderSubmissionStatus 메서드가 종료되는 시점에 checkOrderSubmissionstatus의 트랜잭션이 커밋 되면서 영속성 컨텍스트의 변경 사항이 데이터베이스로 flush 될 것이기 때문에 테스트는 성공할 것이라 예상된다.

하지만 테스트 실행 결과, 테스트가 실패한 것을 확인할 수 있었다.

![[스크린샷 2024-01-06 오후 9.20.22.png]](https://i.imgur.com/lTvSdwQ.png)

이때 checkOrderSubmissionStatus를 호출하는 테스트 코드에 `@Transactional`을 걸어주면 테스트는 성공한다.

왜 테스트 코드의 `@Transactional` 여부에 따라서 테스트 결과가 달라지는 것일까?
## 테스트 코드의 엔티티는 영속성 컨텍스트에 관리되지 않는다

스프링 컨테이너는 기본적으로 트랜잭션 범위의 영속성 컨텍스트 전략을 사용한다.

즉, 트랜잭션 범위와 영속성 컨텍스트의 생명 주기가 같다는 뜻으로 트랜잭션을 시작할 때 영속성 컨텍스트를 생성하고 트랜잭션 커밋 시점에 영속성 컨텍스트를 flush하고 종료한다.

```kotlin
@SpringBootTest  
class OrderStatusChangeServiceTest @Autowired constructor(  
    private val orderStatusService: OrderStatusService,  
    private val orderRepository: OrderRepository,  
    private val entityManager: EntityManager,  
) {  
  
    @Test  
    @DisplayName("송장접수에 실패하면 택배등록 여부를 실패 표시 해야한다.")  
    fun checkOrderSubmissionStatusTest() {  
        val order = Order(shippingLabel = "12345")  
        val savedOrder = orderRepository.save(order)  
        val failureMessage = OutSourcingResultMessage(  
            shippingLabel = savedOrder.shippingLabel,  
            registerSuccess = false  
        )  
  
        println(entityManager.contains(savedOrder)) // false  
        orderStatusService.checkOrderSubmissionStatus(failureMessage)  
  
        order.isParcelRegister() shouldBe false  
    }  
}
```

위의 결과에서 볼 수 있듯 테스트 코드의 엔티티는 트랜잭션 범위 밖이기 때문에 영속성 컨텍스트에 등록이 되어있지 않다. `checkOrderSubmissionStatus` 메서드 내에서 조회해 온 Order는 트랜잭션 범위이기 때문에 영속성 컨텍스트에 의해 관리되고 있지만 테스트 코드에서 검증의 대상이 되는 Order는 트랜잭션 범위 밖에서 조회된, **영속성 컨텍스트에 의해 관리되지 않는** Order를 대상으로 데이터 변경 **검증이 이루어진다**.

때문에 실제 데이터베이스의 컬럼값은 의도한 대로 true에서 false로 변경되었지만, 테스트 코드의 Order는
영속성 컨텍스트의 1차 캐시에 관리되지 않기 때문에 당연하게도 더티 체킹의 효과를 볼 수 없는 것이다.

테스트 코드에 `@Transactional`을 붙여준다면 테스트 코드의 엔티티가 영속성 컨텍스트에 의해 관리되게 되고 1차 캐시에 의해 더티 체킹 대상이 되기 때문에 테스트가 성공하게 된다.
## JpaTransactionManager는 트랜잭션 단위로 EntityManager를 관리한다

위 사례를 통해서 더티 체킹은 영속성 컨텍스트가 관리하는 영속 상태의 엔티티에만 적용되며, 영속성 컨텍스트는 트랜잭션 단위로 생성, 삭제된다는 것을 알 수 있었다.

JPA가 트랜잭션 단위로 영속성 컨텍스트를 생성, 관리하는 부분의 코드를 한번 살펴보았다.

먼저 스프링의 표준 트랙잭션 워크플로우를 구현하는 `AbstractPlatformTransactionManager` 클래스를 살펴보면 `startTransaction()` 메서드에서 
`TransactionManager`의 `doBegin()` 메서드를 호출하는것을 볼 수 있다.

![[스크린샷 2024-01-07 오후 6.59.49.png]](https://i.imgur.com/l4sY5Fz.png)

`doBegin()` 메서드는 `TransactionManager`를 구현하는 `JpaTransactionManager`에서 확인할 수 있다.

![[스크린샷 2024-01-07 오후 7.26.00.png]](https://i.imgur.com/7Qm3pOV.png)

이때  새로 시작된 트랜잭션이라면 EntityManager를 생성하고 `JpaTransactionObject`에 저장한다.

`JpaTransactionObject`는 현재 트랜잭션의 상태를 추적하는 데 사용되며, 트랜잭션 범위 내에서 사용되는 EntityManager를 `EntityManagerHolder`를 통해서 관리하고 있다.

이러한 로직을 통해서 트랜잭션 범위 내에서 동일한 EntityManager가 사용될 수 있던 것이다.

## 마무리

지금까지 트랜잭션 범위와 영속성 컨텍스트의 생명 주기가 어떻게 관리되는지 알아보았다.

사실 영속성 컨텍스트의 생존 범위가 항상 트랜잭션 범위내인 것은 아니다.
OSIV(Open Session In View)를 사용하면 영속성 컨텍스트의 범위를 트랜잭션 범위 밖까지 확장 할 수 있다.

---

## GitHub Actions를 통해 CI/CD 구축하기 (feat. Docker, Jib)

- **URL**: https://headf1rst.github.io/log/ko/post17
- **Date**: 2022-12-06 10:00
- **Tags**: `tech` `프로젝트` `CI/CD` `Docker`

### Content


저희 `Text Me` 서비스의 베타 버전이 배포되고 난 뒤에, 사용자들로 부터 수많은 피드백을 받을 수 있었습니다. 
사용자 피드백을 빠르게 반영하다 보니 프로젝트의 빌드 및 배포 주기가 짧아졌고 이러한 과정이 서서히 번거롭게 느껴지기 시작했습니다.

사용자의 피드백을 빠르게 반영하기 위해서는 하루에 많게는 20번의 배포가 이루어져야했기 때문에 프로젝트의 빌드 및 배포과정을 자동화 하기로 하였습니다. 
(하루에 1,000번 이상의 크고 작은 배포가 이뤄지는 테크 기업에 비하면 귀여운 수준이지만 말이죠… )

이번 글에서는 저희가 CI/CD 도구 중 GitHub Actions를 사용하는 이유와 동작원리에 대해서 공유하겠습니다.

## 1. GitHub Actions의 장점

GitHub Actoins를 CI 솔루션으로 채택하게 된 이유는 다음과 같습니다.

- GitHub와 통일된 환경에서 CI 수행이 가능하다.
- 중앙에서 관리하는 `GitHub Actions Runner` 에 지속적으로 트러블슈팅하여 원활한 CI 환경 구성이 가능하다.
- 프로젝트마다 개별 Runner를 통한 빌드 테스트가 가능하다.
- 친숙한 문법의 YAML 파일로 파이프라인 구성이 간단하다.

- GitHub Actions Runner란?
    - GitHub Actions를 기동하는 Runner
    - GitHub는 퍼블릭 쪽의 GitHub Actions Runner를 클라우드에서 제공해 주고 있다.
        - 덕분에 직접 프로비저닝할 필요 없이 Runner를 바로 사용하는 것이 가능하다.

## 2. Github Action의 구성 요소

- workflow
    - 한개 이상의 `job` 을 실행할 수 있는 자동화된 작업
    - `YAML` 파일로 작성된다.
    - `event` 에 의해서 실행된다.
- event
    - `workflow` 를 실행시키는 특정 활동
    - 깃허브에서 발생하는 대부분의 작업을 event로 정의 가능.
        - ex) `push event` , `pull request event` , `issue event`
- jobs
    - 한가지 `runner` 안에서 실행되는 여러 `step` 들의 모음
    - 각 `step` 들은 일종의 `shell script` 처럼 실행된다.
    - step들은 순서에 따라 실행되며 step끼리 데이터 공유가 가능하다
    - job은 다른 job에 의존관계를 가질 수 있으며 `병렬 실행` 이 가능하다.
- actions
    - 반복 작업을 정의한 커스텀 어플리케이션
    - workflow 파일에서 자주 반복되는 코드를 미리 정의할 수 있다.
        - 코드 양을 줄이는 이점
    - 깃허브 마켓플레이스를 통해 다른 사람이 만든 action 사용 가능

더 자세한 GitHub Actions workflow syntax는 [해당 포스트](https://jinmay.github.io/2020/05/13/git/github-action-syntax/)를 참고하면 도움이 되실 것 같습니다.

![먀](https://i.imgur.com/9tu0sgH.png)

## 3. Github Action으로 CI/CD 파이프라인 구축하기

다음과 같은 순서로 파이프라인이 구동되도록 workflow를 작성하였습니다.

1. Github Action이 트리거되면 jib로 이미지를 빌드한다.
2. 만들어진 이미지를 DockerHub에 push한다.
3. 서버에 접속해서 도커 이미지를 pull 한다.

`.github/workflows` 디렉토리를 프로젝트에 생성하고, 거기에 gradle 빌드를 위한 `build_backend.yml` 을 생성하였습니다.

### 3.1 build_backend.yml

다음 `jobs` 가 실제로 CI를 수행하는 과정이며 `steps` 단계로 jobs가 진행되게 됩니다.

```yaml
name: Build Backend Image

on:
  pull_request:
    branches:
      - production
      - master
    paths:
      - "backend/**"
  workflow_dispatch:

defaults:
  run:
    working-directory: "backend/text-me"

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up JDK 11
        uses: actions/setup-java@v1
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Set environment variables
          run:|
            echo "::set-env name=DB_URL::${{ secrets.DB_URL }}"
            echo "::set-env name=DB_USERNAME::${{ secrets.DB_USERNAME }}"
            echo "::set-env name=DB_PASSWORD::${{ secrets.DB_PASSWORD }}"
            echo "::set-env name=JWT_KEY::${{ secrets.JWT_KEY }}"
            echo "::set-env name=JWT_EXPIRY::${{ secrets.JWT_EXPIRY }}"
            echo "::set-env name=REFRESH_EXPIRY::${{ secrets.REFRESH_EXPIRY }}"
            echo "::set-env name=AWS_ACCESS_KEY_ID::${{ secrets.AWS_ACCESS_KEY_ID }}"
            echo "::set-env name=AWS_SECRET_ACCESS_KEY::${{ secrets.AWS_SECRET_ACCESS_KEY }}"

      - name: Login to Docker Hub
        uses: docker/login-action@v2.1.0
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_PASSWORD }}

      - name: Grant execute permission for gradlew
        run: chmod +x gradlew
        shell: bash

      - name: Build with jib
        run: |
          ./gradlew jib \
          -Djib.to.auth.username=${{ secrets.DOCKERHUB_USERNAME }} \
          -Djib.to.auth.password=${{ secrets.DOCKERHUB_PASSWORD }} \
          -Djib.to.image="${{ secrets.DOCKERHUB_USERNAME }}/text-me-docker-repo:${GITHUB_REF##*/}"

      - name: Get current time
        uses: 1466587594/get-current-time@v2
        id: current-time
        with:
          format: YYYY-MM-DDTHH-mm-ss
          utcOffset: "+09:00"

      - name: Show Current Time
        run: echo "CurrentTime=${{steps.current-time.outputs.formattedTime}}"
        shell: bash
```

빌드 스크립트에 작성된 내용을 좀 더 자세히 살펴보겠습니다.

- `jobs: build: runs-on: ubuntu-latest`
  - 작성한 스크립트가 작동될 OS 환경을 지정합니다.
  - text me 서비스는 우분투 18.04에서 동작하기 때문에 `ubuntu-latest` 로 지정해 주었습니다.
    ![d](https://i.imgur.com/mRZU1aq.png)
    
- `steps: uses`
    - 마켓 플레이스에 사전 정의된 내용을 이용하여 step을 수행합니다.
    - 사전 작업을 위한 환경 설정용.
      ![d](https://i.imgur.com/xZ8nmau.png)

- `steps: run`
    - 개발자가 직접 정의한 커맨드를 수행합니다.
    - 실제 수행용.

- `-Djib.to.image="${{ secrets.DOCKERHUB_USERNAME }}/text-me-docker-repo:${GITHUB_REF##*/}"`
  - Jib를 통해 도커 이미지를 빌드해서 도커 허브에 push 합니다.

**도커 컨테이너 이미지와 JIB**

스프링 프로젝트를 컨테이너 이미지로 만들기 위해서는 다양한 방법들이 존재합니다.

- **로컬 환경에서 jar 파일 빌드 , jar 파일을 이미지에 복사 후 실행**
  - 프로젝트 폴더로 들어가서 gradlew clean build를 통해 grdlew로 jar 파일 빌드
    - `./gradlew clean build`
  - 프로젝트 테스트 코드들이 실행되고 모든 테스트가 통과하면 `./build/libs` 에 실행 가능한 jar 파일이 생성된다.
  - jar 파일을 바탕으로 도커 이미지를 생성하기 위한 Dockerfile을 작성

      ```yaml
      FROM openjdk:11
      ARG JAR_FILE=./build/libs/jpashop-0.0.1-SNAPSHOT.jar
      COPY ${JAR_FILE} app.jar
      ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]
      ```

  - 해당 Dockerfile을 프로젝트 폴더에 넣은 후 docker build . -t 태그 이름 명령어를 실행하면 컨테이너 이미지 생성
  - java11의 실행환경을 제공하는 openjdk:11 이미지 위에서 폴더에 있는 jar 파일을 이미지 내부로 복사 후 java 명령어를 통해 실행

![d](https://cloud.google.com/java/images/docker_build_flow.png)

하지만 jar 파일을 빌드하고 추가로 이미지에 복사하여 실행하는 방법은 소스코드에 조금의 변화만 생기더라도 변경된 부분과 의존성이 연결된 jar 파일 전체가 새로운 이미지로 인식되어 전체 파일이 다시 빌드되기 때문에 Docker layer의 장점을 살릴 수 없습니다.

이를 해결하기 위해서 layer를 나누는 방법도 있지만 Google Cloud에서 제공해주는 jib를 통해서 이미지 빌드를 최적화하는 방법을 선택하였습니다.

- **JIB를 통한 이미지 빌드**
  - Jib는 **Dockerfile, Docker에 의존하지 않고** gradle, maven에서 Jib 플러그인을 사용해서 컨테이너 이미지를 빌드하고 허브에 푸시하는 방법.
  - 어플리케이션을 **(종속 항목, 리소스, 클래스 등)** 별개의 레이어로 구성하고 Docker 이미지 레이어 캐싱을 활용해서 **변경사항만 다시 빌드**함으로써 빌드를 빠르게 유지
  - jib 레이어 구성과 작은 기본 이미지는 전체 이미지 크기를 작게 유지시키며 빌드 속도를 향상 시킴

![d](https://cloud.google.com/java/images/jib_build_flow.png)

**Gradle에 jib 플러그인 추가 및 환경 변수 등록**

```bash
plugins {
    id 'com.google.cloud.tools.jib' version '3.3.1'
}

jib {
	to {
		image = "sanha1998/text-me-docker-repo"
	}
	from {
		image = "eclipse-temurin:11-jre"
	}
	container {
		jvmFlags = ["-Xms128m", "-Xmx128m"]
		environment = [
				'DB_URL': System.getenv('DB_URL'),
				'DB_USERNAME': System.getenv('DB_USERNAME'),
				'DB_PASSWORD': System.getenv('DB_PASSWORD'),
				'JWT_KEY': System.getenv('JWT_KEY'),
				'JWT_EXPIRY': System.getenv('JWT_EXPIRY'),
				'REFRESH_EXPIRY': System.getenv('REFRESH_EXPIRY'),
				'AWS_ACCESS_KEY_ID': System.getenv('AWS_ACCESS_KEY_ID'),
				'AWS_SECRET_ACCESS_KEY': System.getenv('AWS_SECRET_ACCESS_KEY')
		]
	}
}
```

**환경 변수 주입**

스프링 부트 프로젝트의 빌드가 시작되면`application.yml` 혹은 `application.properties` 
내에 설정된 환경변수 값들이 주입됩니다. 이때 AWS 계정 정보와 같은 민감 정보를 application.yml에 노출시키지 않기 위해서 Secrete 환경 변수를 깃허브에 등록해줘야 합니다.

![스크린샷 2022-12-17 오후 9.19.33.png](https://i.imgur.com/VPtkmpm.png)

깃허브에 등록한 secrets 환경 변수들을 사용하기 위해서 다음과 같은 작업을 수행해주었습니다.

```yaml
- name: Set environment variables
  run:|
    echo "::set-env name=DB_URL::${{ secrets.DB_URL }}"
    echo "::set-env name=DB_USERNAME::${{ secrets.DB_USERNAME }}"
    echo "::set-env name=DB_PASSWORD::${{ secrets.DB_PASSWORD }}"
    echo "::set-env name=JWT_KEY::${{ secrets.JWT_KEY }}"
    echo "::set-env name=JWT_EXPIRY::${{ secrets.JWT_EXPIRY }}"
    echo "::set-env name=REFRESH_EXPIRY::${{ secrets.REFRESH_EXPIRY }}"
    echo "::set-env name=AWS_ACCESS_KEY_ID::${{ secrets.AWS_ACCESS_KEY_ID }}"
    echo "::set-env name=AWS_SECRET_ACCESS_KEY::${{ secrets.AWS_SECRET_ACCESS_KEY }}"
```

### 3.2 deploy-backend

이제 운영 서버에서 도커 허브에 올린 이미지를 pull한 다음, 실행시켜 주면 됩니다.

 docker 관련 명령어를 사용하기 위해서 [운영 서버에 docker를 설치](https://headf1rst.github.io/TIL/infra-1)해 주었습니다.

docker가 운영 서버에 정상적으로 설치되었다면, 이미지를 가져와서 실행시키는 스크립트를 작성하여 배포를 자동화 시켜 보도록 하겠습니다.

```yaml
name: Deploy Backend

on:
  push:
    branches:
      - production
  workflow_dispatch:

defaults:
  run:
    working-directory: "backend/text-me"

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Set up JDK 11
        uses: actions/setup-java@v1
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: SSH setting
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.HOST }}
          username: ${{ secrets.USER_NAME }}
          key: ${{ secrets.PRIVATE_KEY }}
          envs: GITHUB_SHA
          script: |
            whoami
            docker pull ${{ secrets.DOCKERHUB_USERNAME }}/text-me-docker-repo:${GITHUB_REF##*/}
            docker tag ${{ secrets.DOCKERHUB_USERNAME }}/text-me-docker-repo:${GITHUB_REF##*/} ${{ secrets.DOCKERHUB_USERNAME }}/text-me-docker-repo:${GITHUB_REF##*/}
            docker stop text-me-api
            docker run -d --rm --name text-me-api -p 8080:8080 ${{ secrets.DOCKERHUB_USERNAME }}/text-me-docker-repo:${GITHUB_REF##*/}
```

배포 스크립트에서 눈여겨 볼 부분은 원격 서버에 접근하기 위한 SSH setting 부분입니다. ([ssh-action](https://github.com/appleboy/ssh-action)) 

로컬 서버를 열고 터미널에 다음 명령어를 입력하여 ssh 키를 생성해 주었습니다.

- `ssh-keygen -t rsa -b 4096 -C "내이메일@gmail.com"`

![d](https://i.imgur.com/s7q725G.png)

ssh 키를 `./authorized_keys2` 경로에 저장하고 다음 명령어를 통해서 ssh 키값을 확인해 줍니다.

- `vim ./ssh/authorized_keys2.pub`

ssh 키를 GITHUB SECRET의 PRIVATE_KEY로 등록해주었습니다.

![d](https://i.imgur.com/wJ7mdpG.png)

모든 과정이 마무리되었다면 직접 docker hub에 접속해서 이미지를 pull 받아올 필요 없이 자동으로 이미지를 가져와서 운영 서버에 띄워주게됩니다.

## 마치며

지금까지 기프터즈팀이 GitHub Actions를 사용하는 이유와 동작원리에 대해 설명드렸습니다.

시간이 된다면 프론트 빌드 및 배포 과정도 포스팅 해보도록 하겠습니다. 감사합니다.

---

**참고 자료** 📚

- [GitHub Action으로 CI/CD 구축하기](https://velog.io/@sgwon1996/GitHub-Action%EC%9C%BC%EB%A1%9C-CICD-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0)

- [카카오엔터프라이즈가 GitHub Actions를 사용하는 이유](https://tech.kakao.com/2022/05/06/github-actions/)

- [Github actions를 이용한 CICD - 2](https://itcoin.tistory.com/685)

- [[Github]깃허브의 CI툴인 Actions의 문법 간단 정리](https://jinmay.github.io/2020/05/13/git/github-action-syntax/)

- [쿠버네티스 환경에 스프링 어플리케이션 배포하기](https://velog.io/@sgwon1996/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%ED%99%98%EA%B2%BD%EC%97%90-%EC%8A%A4%ED%94%84%EB%A7%81-%EC%96%B4%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98-%EB%B0%B0%ED%8F%AC%ED%95%98%EA%B8%B0)


---

## ubuntu 18.04에 Docker 설치하기

- **URL**: https://headf1rst.github.io/log/ko/post16
- **Date**: 2022-12-03 10:00
- **Tags**: `infra`

### Content


프로젝트를 진행하면서 도커 허브에 올라가있는 이미지를 가져와서 배포 환경에서 실행해야 하는 요구사항이 발생하였습니다. 

이를 위해서 ubuntu환경에 docker를 설치했던 과정을 공유해보겠습니다.

## Docker가 지원하는 OS

다음은 docker 공식 문서에서 지원하는 OS입니다.

저는 `ubuntu 18.04` 환경에서 진행하였습니다.

- Ubuntu Impish 21.10
- Ubuntu Hirsute 21.04
- Ubuntu Focal 20.04 (LTS)
- Ubuntu Bionic 18.04 (LTS)

## 설치 과정

- 기본 패키지들이 최신 버전인지 확인하고 갱신
    - `$ sudo apt-get update`
- apt가 HTTPS를 통해 repository를 이용하는 것을 허용할 수 있도록 해주는 패키지들을 설치
    
    ```bash
    $ sudo apt-get install \
    	ca-certificates \
    	curl \
    	gnupg \
    	lsb-release
    ```
    
- Docker 공식 GPG key 추가
    - `$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg`
    - 만약 아래와 같은 경고가 발생한다면 (1), (2) 과정을 순서대로 수행해 줍니다.
        - `gpg: WARNING: unsafe ownership on homedir '/home/vlastimil/.gnupg'`
    - (1) `sudo gpgconf --kill dirmngr`
    - (2) `sudo chown -R $USER ~/.gnupg`
- Docker repository 등록
    
    ```bash
    $ echo \
    	"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] [https://download.docker.com/linux/ubuntu](https://download.docker.com/linux/ubuntu) \
    	$(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    ```
    
- Docker 설치
    - `$ sudo apt-get update`
    - `$ sudo apt-get install docker-ce docker-ce-cli [containerd.i](http://containerd.io/)o`
- Docker가 재대로 설치되었는지 확인
    - `$ sudo docker version`

## Ubuntu 환경에서 sudo 권한 없이 docker 명령어 사용하기

배포 스크립트를 통해서 도커 관련 명령어가 실행되게 하기 위해서는 다음 명령어가 필요합니다.

- `$ sudo usermod -aG docker $USER`
    - usermod : 사용자 속성을 변경하는 명령어
    - G (—groups) : 새로운 그룹을 말한다.
    - a (—append) : 다른 그룹에서 삭제 없이 G 옵션에 따른 그룹에 사용자를 추가한다.
    - $USER : 현재 사용자를 가리키는 환경변수
    - 실행 후 우분투를 재기동해야 함.

---

**참고 자료** 📚

- [Install Docker Engine on Ubuntu](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository)

- [gpg: WARNING: unsafe ownership on homedir '/home/user/.gnupg'](https://unix.stackexchange.com/questions/452020/gpg-warning-unsafe-ownership-on-homedir-home-user-gnupg)


---

## [오브젝트] 13장 - 서브클래싱과 서브타이핑

- **URL**: https://headf1rst.github.io/log/ko/post15
- **Date**: 2022-11-29 10:00
- **Tags**: `객체지향`

### Content


상속이 사용되는 두 가지 용도

- 타입 계층을 구현하는 것
    - 부모 클래스
        - 일반적인 개념을 구현
        - 부모 클래스는 자식 클래스의 일반화
    - 자식 클래스
        - 특수한 개념을 구현
        - 자식 클래스는 부모 클래스의 특수화
- 코드 재사용
    - 부모 클래스의 코드를 재사용하는 것이 가능
    - 단, 부모, 자식 클래스가 강하게 결합되어 변경하기 어려운 코드가 탄생

- 상속의 사용 이유는 `타입 계층을 구현하는 것`이어야 한다.
    - 객체의 행동을 기반으로 타입 계층을 구성해야한다.

## 1. 올바른 타입 계층을 구성하는 원칙

### 개념 관점의 타입

- 타입
    - 공통의 특징을 공유하는  대상들의 분류
    - 사물을 분류하기 위한 틀
    - ex) `자바, 루비, 자바스크립트` - 프로그래밍 언어 타입
- 타입의 인스턴스 (객체)
    - 타입으로 분류되는 대상
    - ex) `자바` - 프로그래밍 언어의 인스턴스

- 타입의 구성 요소
    - 심볼: 타입의 이름
    - 내연: 타임의 정의. 타입에 속한 객체들이 가지는 공통적인 속성, 행동
    - 외연: 타입에 속하는 객체들의 집합

### 프로그래밍 언어 관점의 타입

하드웨어는 데이터를 0과 1로 구성된 일련의 비트 조합으로 취급.
비트 자체에는 타입이라는 개념이 존재하지 않기 때문에 비트에 담긴 데이터를 문자열로 다룰지, 정수로 다룰지는 데이터를 사용하는 어플리케이션에 의해 결정된다.

- 타입
    - 동일한 오퍼레이션을 적용할 수 있는 인스턴스들의 집합
    - 비트 묶음에 의미를 부여하기 위해 정의된 제약과 규칙
- 타입의 두 가지 목적
    - 객체의 타입에 따라 적용 가능한 연산자의 종류를 제한하여 프로그래머의 실수를 방지
    - 타입에 수행되는 오퍼레이션에 대해 미리 약속된 문맥을 제공
        - a와 b에 부여된 타입이 `+` 연산자의 문맥을 정의.
        - new 연산자는 타입에 정의된 만큼 저장 공간을 할당하고 객체를 초기화하기 위해 타입 생성자를 자동으로 호출

### 객체지향 패러다임 관점의 타입

- 타입
    - 객체의 퍼블릭 인터페이스를 정의하는 것
        - 퍼블릭 인터페이스
            - 객체가 수신할 수 있는 메시지의 집합
    - 동일한 퍼블릭 인터페이스를 가지는 객체들은 동일한 타입
        - 객체가 수신할 수 있는 메시지를 기준으로 타입을 분류

## 2. 타입 계층

타입 계층은 포함 관계로 연결되어있다.

퍼블릭 인터페이스 관점에서의 슈퍼타입과 서브타입

- 슈퍼타입
    - 서브타입이 정의한 퍼블릭 인터페이스를 인반화시켜 상대적으로 범용적이고 넓은 의미로 정의한 것.
- 서브타입
    - 슈퍼타입이 정의한 퍼블릭 인터페이스를 특수화시켜 상대적으로 구체적이고 좁은 의미로 정의한 것.
    - 서브타입의 인스턴스는 슈퍼타입의 인스턴스로 간주될 수 있다.

## 3. 서브클래싱과 서브타이핑

타입 계층을 구현할 때 지켜야 하는 제약사항

### 언제 상속을 사용해야 하는가?

상속의 올바른 용도는 `타입 계층`을 구현하는 것.
아래 두 질문에 모두 '예'라고 답할 수 있을때만 상속을 사용.

- 상속 관계가 is-a 관계를 모델링하는가?
    - [자식 클래스]는 [부모 클래스]다 라고 말해도 이상하지 않는 경우
    - 단, 어휘적 정의가 아니라 기대되는 행동에 따라 타입 계층을 구성해야 한다.
- 클라이언트 입장에서 부모 클래스의 타입으로 자식 클래스를 사용해도 무방한가? ⭐️
    - 상속 계층을 사용하는 클라이언트의 입장에서 부모 클래스와 자식 클래스의 차이점을 몰라야 한다. - `자식 클래스와 부모 클래스의 행동 호환성`

`팽귄과 새`는 is-a 관계로 묶을 수 있지만 새와 팽귄의 서로 다른 행동 방식 (날 수 있는지 여부)은 동일한 타입 계층으로 묶어서는 안된다고 경고한다.

### 행동 호환성

- 두 타입 사이에 행동이 호환될 경우에만 타입 계층으로 묶어야 한다.
- 행동의 호환 여부를 판단하는 기준은 클라이언트의 관점
    - 클라이언트가 두 타입이 동일하게 행동할 것이라고 기대하면 두 타입을 타입 계층으로 묶을 수 있다.

### 클라이언트의 기대에 따라 계층을 분리하라.

행동 호환성을 만족시키지 않는 상속 계층을 유지하기란 쉽지 않다.
클라이언트의 기대에 맞게 상속 계층을 분리하라.

- 인터페이스는 클라이언트가 기대하는 바에 따라 분리돼야 한다.
    - 인터페이스 분리 원칙 (ISP)
- 두 클래스 사이에 행동이 호환되지 않는다면 올바른 타입 계층이 아니므로 상속을 사용해서는 안된다.

### 서브클래싱과 서브타이핑

- 서브클래싱
    - 코드 재사용 목적으로 상속을 사용하는 경우
    - 자식 클래스와 부모 클래스의 행동이 호환되지 않는다.
        - 자식 클래스의 인스턴스가 부모 클래스의 인스턴스를 대체할 수 없다.
- 서브타이핑
    - 타입 계층 구성을 위해 상속을 사용하는 경우
    - 자식 클래스와 부모 클래스의 행동이 호환된다.
        - 자식 클래스의 인스턴스가 부모 클래스의 인스턴스를 대체할 수 있다. (LSP)
        - 부모 클래스는 자식 클래스의 `슈퍼타입`이 된다.
        - 자식 클래스는 부모 클래스의 `서브타입`이 된다.

## 4. 리스코프 치환 원칙

상속 관계로 연결한 두 클래스가 서브타이핑 관계를 만족시키며 서브타입은 그것의 기반 타입에 대해 대체 가능해야 한다.

### 리스코프 치환 원칙을 위반하는 is-a 관계 (정사각형, 직사각형)

```java
public class Rectangle {

	private int x, y, width, height;

	public Rectangle(int x, int y, int width, int height) {
		// 생략
	}

	public void setWidth(int width) {
		this.width = width;
	}

	public void setHeigth(int height) {
		this.height = height;
	}
}
```

```java
public class Square extends Rectangle {

	public Square(int x, int y, int size) {
		super(x, y, size, size);
	}

	@Override
	public void setWidth(int width) {
		super.setWidth(width);
		super.setHeight(width);
	}

	@Override
	public void setHeight(int height) {
		super.setWidth(height);
		super.setHeight(height);
	}
}
```

```java
public void resize(Rectangle rectangle, int width, int height) {
	rectangle.setWidth(width);
	rectangle.setHeight(height);
	assert rectangle.getWidth() == width && rectangle.getHeight() == height;
}
```

클라이언트가 `resize()` 메서드를 사용한다고 할때,

- `Rectangle`을 사용하는 클라이언트는
    - Rectangle의 너비와 높이가 다를 수 있다는 가정하에 코드를 개발.
    - 하지만 `Square` 는 너비와 높이가 항상 같다.
    - Ractangle을 Square로 대체할 경우 Rectangle에 세워진 가정을 위반할 확률이 높다.

- is-a 관계의 문장 앞에 `"클라이언트 입장에서"`라는 말이 빠져 있다고 생각하라.
- 클라이언트 입장에서 퍼블릭 인터페이스의 행동 방식이 변경되지 않는다면 클라이언트의 코드를 변경하지 않고도 새로운 자식 클래스와 협력할 수 있게 된다.

### 느낀점

면접 단골 예상 질문이라며 인터페이스와 추상 클래스의 차이점에 대한 질문을 본적이 있습니다. 자바8, 9로 넘어오면서 부터는 인터페이스에서도 디폴트 메서드를 정의하거나 private 메서드를 정의하는것이 가능해 졌습니다.

이처럼 인터페이스가 점점 추상 클래스의 역할까지 담당해 나가고 있고 인프런 강의 영상에서도 점점 추상클래스를 사용하지 않고 있다는 말을 들은적이 있습니다. 때문에 문득 그럼에도 추상클래스가 필요한 이유가 있을까?라는 의문을 갖고 찾아보게 되었습니다.

- 인터페이스에선 `final` 메서드를 정의할 수 없다.
    - 서브클래스에서 어떠한 메서드가 재정의 (오버라이드)되는 것을 의도적으로 막기 위해서는 final 메서드가 필요하다
- 인터페이스는 상태를 갖지 못한다.
- 커뮤니케이션 가치
    - 인터페이스는 구현해야하는 존재
    - 구현될 필요가 없는 인터페이스를 의도하고 작성하였지만 누군가는 인터페이스를 보고 구현해야 할 존재로 바라볼 수 있다. 즉, 사람들의 예상과 다른 코드를 작성하여 오해를 불러 일으키는 코드를 작성하게 된다.
- 생성자를 선언하지 못한다.

https://www.quora.com/Do-we-need-abstract-classes-anymore-when-we-have-Java-8s-interfaces-with-default-and-static-methods


---

## [오브젝트] 9장 - 유연한 설계

- **URL**: https://headf1rst.github.io/log/ko/post14
- **Date**: 2022-11-02 10:00
- **Tags**: `객체지향`

### Content


## 1. 개방-폐쇄 원칙 (Open-Closed Principle)

- 개체는 확장에 대해 열려있어야 하고, 수정에 대해서는 닫혀 있어야한다.
    - 확장에 열려있다.
        - 새로운 동작을 추가해서 어플리케이션의 기능을 확장할 수 있다.
    - 수정에 닫혀있다.
        - 기존 코드를 수정하지 않고도 어플리케이션 동작을 추가하거나 변경할 수 있다.

### OCP원칙을 지키는 방법

- 컴파일타임 의존성을 고정시키고 런타임 의존성을 변경하라.
    - 런타임 의존성
        - 실행시에 협력에 참여하는 객체들 사이의 관계
    - 컴파일타임 의존성
        - 코드에서 드러나는 클래스들 사이의 관계
    - 컴파일타임 의존성을 런타임 의존성으로 대체하라

- 추상화에 의존하라
    - 추상화
        - 핵심적인 부분만 남기고 불필요한 부분을 생략하여 복잡성을 낮추는 기법
    - 추상화 부분은 수정에 닫혀 있다.
    - 추상화를 통해 생략된 부분은 확장의 여지를 남긴다.
    - 변하지 않는 부분을 추상화해서 고정하고 변하는 부분을 생략하는 추상화 메커니즘이 OCP의 기반이 된다.

## 2. 객체에 대한 생성과 사용을 분리하라

- 객체 생성에 대한 지식은 결합도를 증가시킨다.
    - 객체의 타입과 생성자에 전달해야 하는 인자에 대한 과도한 지식은 코드를 문맥에 강하게 결합시킨다.
- 동일 클래스 안에서 객체 생성과 사용이 공존해서는 안된다.

### 사용으로부터 생성을 분리하는 방법

- 객체를 생성할 책임을 클라이언트가 갖도록 하라
    - `Movie`객체에게 어떤 할인 정책을 적용할지를 알고 있는 것은 Movie와 협력할 클라이언트이다.
    - Movie의 클라이언트가 적절한 할인정책 인스턴스를 생성한 후 Movie에게 전달하도록 한다.
    - Movie는 특정 클라이언트에 결합되지 않고 독립적일 수 있게 된다
- 객체 생성만을 담당하는 Factory 객체를 추가하라 ⭐️ (PURE FABRICATION 객체)
    - 객체 생성에 대한 책임만을 담당하는 팩토리 객체를 추가하고 클라이언트는 팩토리 객체를 사용하도록 한다.

   ```java
   public class Factory {

      public Movie createAvatarMovie() {
          return new Movie("아바타",
                          new AmountDiscountPolicy(...));
      }
  }
  ```

    ``` java
    public class Client {
      
            private Factory factory;
                
            public Client(Factory factory) {
                this.factory = factory;
            }
            
            public Money getAvatarFee() {
                Movie avatar = factory.createAvatarMovie();
                return avatar.getFee();
            }
    }
    ```

- 순수한 가공물에게 책임을 할당하라
    - 시스템을 객체로 분해하는 두가지 방식
        - 표현적 분해
            - 도메인 모델의 개념을 이용해 시스템을 분해하는 것.
            - 객체지향 설계를 위한 가장 기본적인 접근법
            - (도메인 모델은 어디까지나 설계를 위한 출발점에 불과하다.)
            - 모든 책임을 도메인 객체에 할당하면 캡슐화가 위반된다.
            - 설계자가 임의로 만들어낸 가공의 객체에 책임을 할당하여 문제를 해결한다.
            - `PURE FABRICATION`: 도메인과 무관한 인공적인 객체
        - 행위적 분해
            - 특정한 행동을 표현하기 위해 시스템을 분해하는 것.
            - 어떤 행동을 책임질 도메인이 존재하지 않는다면 PURE FABRICATION을 추가하고 책임을 할당하라.

    - 설계자의 역할은 도메인 추상화를 기반으로 어플리케이션 로직을 설계하는 동시에 품질 측면에서 균형을 맞추는 데 필요한 객체들을 창조하는 것.

## 3. 의존성 주입 (Dependency Injection)

- 외부에서 의존성의 대상을 해결한 후 이를 사용하는 객체 쪽으로 주입하는 기법

- 생성과 사용을 분리하고 나면, 객체는 인스턴스를 사용하는 책임만 갖게된다.
    - 즉, 외부의 다른 객체가 인스턴스를 생성하여 전달해야 한다.

### 의존성 주입 방법

- 생성자 주입
    - 객체를 생성하는 시점에 생성자를 통한 의존성 해결
    - 객체의 생명주기 전체에 걸쳐 의존성 관계를 유지
- Setter 주입
    - 객체 생성 후 setter 메서드를 통한 의존성 해결
    - 런타임에 의존성의 대상을 변경할 수 있다.
    - 객체가 생성되기 위해 어떤 의존성이 필수적인지 명시할 수 없다.
- 메서드 주입
    - 메서드 실행 시 인자를 이용한 의존성 해결

### Service Locator 패턴 (Anti)

- 의존성을 해결할 객체들을 보관하는 일종의 저장소.
- 객체가 직접 Service Locator에게 의존성 해결을 요청

- 인스턴스를 등록하고 반환할 수 있는 메서드를 구현한 저장소.

```java
public class ServiceLocator {

	private static ServiceLocator soleInstance = new ServiceLocator();
	private DiscountPolicy discountPolicy;

	public static DiscountPolicy discountPolicy() {
		return soleInstance.discountPolicy;
	}

	public static void provide(DiscountPolicy discountPolicy) {
		soleInstance.discountPolicy = discountPolicy;
	}

	private ServiceLocator() {
	}
}
```

- `Movie`의 인스턴스가 `AmountDiscountPolicy`의 인스턴스에 의존하기를 원하는 경우
    - ServiceLocator에 인스턴스를 등록한 후 Movie를 생성

```java
ServiceLocator.provide(new AmountDiscountPolicy(...));
Movie avatar = new Movie("아바타",
						Money.wons(10000));
```

### 숨겨진 의존성은 나쁘다

ServiceLocator 패턴은 의존성을 감춘다는 단점이 있다.

```java
Movie avatar = new Movie("아바타",
						Money.wons(10000));
						
avatar.calculateMovieFee(screening); // NPE!!!
```

개발자는 위 코드가 Movie를 온전히 생성해 줄 것이라고 예상하지만 인스턴스에 접근하는 순간 인스턴스 변수인 discountPolicy가 null이라는 것을 알게 된다.

- 의존성을 구현 내부로 감추는 것의 문제점
    - 의존성 문제가 런타임에 가서야 발견된다.
    - 단위 테스트 작성이 어렵다.
    - 의존성을 이해하기 위해 코드의 내부 구현을 이해할 것을 강요한다.
        - 캡슐화를 위반하게 된다.

- 가능한 의존성 주입을 사용하라.
    - 필요한 의존성은 클래스의 퍼블릭 인터페이스에 명시적으로 드러난다.
    - 코드 내부를 읽을 필요가 없으므로 캡슐화를 지킨다.
    - 가급적 의존성을 객체의 퍼블릭 인터페이스에 노출하라.

- 의존성을 명시적으로 표현할 수 있는 기법을 사용하라 (의존성 주입)

## 4. 의존성 역전 원칙 (Dependency Inversion Principle)

- 협력의 본질을 담고 있는 것은 상위 수준의 정책이다.
    - 협력에서 중요한 의사결정, 비즈니스의 본질을 담고 있는 것은 상위 수준의 클래스다.
- 상위 수준의 클래스는 하위 수준의 클래스에 의존해서는 안된다.
- 모든 의존성의 방향이 추상화를 바라보도록 해라.
    - 하위 수준 클래스의 변경으로 상위 수준 클래스가 영향을 받는것을 방지 할 수 있다.
    - 상위 수준을 재사용할 때 하위 수준의 클래스에 얽매이지 않고도 다양한 컨텍스트에 재사용이 가능

### 의존성 역전 원칙과 패키지

- 객체지향 프로그래밍 언어에서 어떤 구성 요소의 소유권을 결정하는 것은 `모듈` (패키지)이다.

- 인터페이스가 서버 모듈 쪽에 위치하는 전통적 모듈 구조
  (Movie), (DiscountPolicy, AmountDiscountPolicy, PercentDiscountPolicy)
    - Movie는 DiscountPolicy에 의존한다.
        - Movie를 정상적으로 컴파일하기 위해서는 DiscountPolicy가 필요.
    - DiscountPolicy가 포함된 패키지 내, 클래스가 수정되면 패키지 전체가 재배포 돼야한다.
    - DiscountPolicy가 속한 패키지에 의존하는 Movie가 포함된 패키지 역시 재컴파일돼야 한다.
    - Movie에 의존하는 또 다른 패키지도 재컴파일이 되야하며 어플리케이션 전체로 번지게 되며 빌드 시간이 상승한다.

- 인터페이스와 소유권을 역전시킨 객체지향적인 모듈 구조
    - Movie의 재사용을 위해 필요한 것이 DiscountPolicy 뿐이라면 둘을 같은 패키지로 모은다.
    - AmountDiscountPolicy, PercentDiscountPolicy는 별도의 패키지로 하여 의존성 문제를 해결.
    - Movie를 다른 컨텍스트에서 재사용하기 위해서는 Movie와 DiscountPolicy가 포함된 패키지만 재사용하면 된다.

## 5. 그럼에도 역할, 책임, 협력이 가장 중요하다

- 유연한 설계
    - 동일한 컴파일 타임 의존성으로부터 다양한 런타임 의존성을 만들 수 있는 코드 구조를 가지는 설계
- 불필요한 유연성은 불필요한 복잡성을 낳는다.
- 로직을 처리하기 위해 책임을 할당하고 협력의 균형을 맞추는것을 우선시 하라
- 객체 생성 메커니즘을 결정하고 객체 생성 책임을 담당할 객체를 찾는것을 최대한 미뤄라

## 느낀점

"객체지향은 현실 세계의 모방이다."라는 말을 처음 객체지향 프로그래밍을 접했을때 들었던 기억이 있다. 객체지향을 처음 접하는 사람들이 조금은 쉽게 이해할
수 있기 위해 한 표현이라고 생각된다. 이번 장에서 저자가 말하듯, 우리가 애플리케이션을 구축하는 것은 사용자들이 원하는 기능을 제공하기 위함이지,
실세계를 모방하기 위함이 아니다라는 것을 다시 한번 생각하게 되었다.

패키지 구조에 대한 내용도 인상 깊었다. 그동안 찾기 쉽게 비슷한 도메인을 패키지로 묶어서 관리했었다. 이번에 의존성 역전 원칙과 패키지 구조에 대해 알고나서
모듈화에 대해서 조금 더 관심을 갖는 계기가 되었다. 그럼, 이번 장에서 인상 깊었던 구절로 마무리 하도록 하겠다. 우리 모두 훌륭한 설계 역량을 가진 개발자로 성장해보자!

"도메인을 반영하는 어플리케이션의 구조라는 제약 안에서 실용적인 창조성을 발휘할 수 있는 능력은 훌륭한 설계자가 갖춰야 할 기본 역량이다."


---

## 어노테이션 기반 MVC로 리팩터링하기 - MVC 2편

- **URL**: https://headf1rst.github.io/log/ko/post13
- **Date**: 2022-10-17 10:00
- **Tags**: `Spring` `MVC`

### Content


이번 포스트에서는 [이전 포스트](https://headf1rst.github.io/TIL/mvc1)에서 구현한 MVC 프레임워크를 
어노테이션 기반의 MVC로 점진적으로 리팩토링해 나가는 과정에 대해서 다뤄보도록 하겠다.

## 1. 불편함을 감지하기

![스크린샷 2022-08-21 오후 12.54.25.png](https://i.imgur.com/0xMJSgT.png)

지금까지 구현된 MVC 프레임워크는 기능을 추가할 때 마다 Controller 인터페이스를 구현하는 새로운 클래스를 구현하고 mappings에 추가해야 한다. 
또한 HTTP 메서드와 URI가 매핑되어있지 않기 때문에 메서드만 다른 동일 URI를 구분하는것이 불가능 했다.

예를 들어서 다음과 같이 HTTP 메서드만 다른 URI를 구분할 수 없다.

`Post /users` , `Get /users`

이 뿐만 아니라 구현된 컨트롤러의 `execute()` 메서드도 10줄이 넘어가는 경우가 거의 없다.

![스크린샷 2022-08-23 오전 2.57.12.png](https://i.imgur.com/Mzs6jXi.png)

관련된 여러 기능을 하나의 모듈 단위로 묶은 Controller를 구현할 수는 없을까?

`ListUserController`나 `LoginController`와 같이 기능 하나당 하나의 Controller를 추가하는 방식이 아닌, 사용자와 관련된 기능을 처리하는
`UserController`를 구현할 수 있다면 위에서 언급한 문제점들을 해결할 수 있을것이다.

어노테이션 기반의 MVC로 리팩토링을 마치고 난 뒤의 Controller는 다음과 같다. User 도메인에 관련된 모든 기능이 
UserController 객체에 존재하며 동일한 URI 더라도 메서드에 따라 구분되는 것을 볼 수 있다.

![스크린샷 2022-10-20 오후 5.38.33.png](https://i.imgur.com/e1zZaFK.png)

어노테이션 기반 MVC로 리팩토링함으로써 얻는 이점은 다음과 같다.

- 하나의 Controller에 관련 기능이 모두 존재하기 때문에 유지보수에 유리하다.
  - 기능마다 Controller를 생성해서 등록해야 하는 번거로움이 없다.
- @Controller를 설정해주는 것만으로 Controller를 등록할 수 있다.
  - mappings에 Controller를 등록할 필요없다.
- HTTP 메서드에 따른 URI 식별이 가능하다.

하지만 여기서 한가지 고민이 생긴다. 어떻게 하면 @Controller 어노테이션을 설정해주는 것 만으로 Controller를 등록할 수 있을까?

Reflections API를 사용하면 “특정한 어노테이션이 붙은 클래스인가?” 등을 확인 할 수 있다.
혹은 어노테이션에 추가로 제공된 정보를 바탕으로 추가적인 일을 처리하는 것 또한 가능하다.

Reflections을 활용해서 런타임에 동적으로 @Controller 어노테이션의 클래스 정보를 읽어오는 `ControllerScanner` 객체를 구현해 보도록 하자.

## 2. ControllerScanner

어노테이션 기반 MVC 구현에 필요한 핵심 요구사항은 다음과 같다.

- `@Controller` 어노테이션이 설정된 클래스 정보를 읽어와야 한다.
- `@Controller` 가 설정된 클래스 내에 `@RequestMapping` 이 설정된 메서드 정보를 읽어와야 한다.

이와 같은 책임을 `ControllerScanner` 객체를 구현하여 할당해 주었다.

![스크린샷 2022-10-20 오후 5.57.36.png](https://i.imgur.com/qZkw6bT.png)

ControllerScanner 객체의 생성자는 파라미터로 Object 타입의 가변인자를 넘겨받고 이를 통해서 필드값의 `Reflections` 객체를 초기화 한다.

`getControllers()` 메서드는 reflections의 `getTypesAnnotatedWith()` 메서드를 호출하여 basePackage 내에 @Controller 어노테이션이 
설정된 모든 클래스 메타 정보들을 조회해 온다.

[Reflections API](https://www.baeldung.com/reflections-library)에 대해 간략하게 알아보자면 다음과 같다.

- Reflections API란?

  프로그래머가 작성한 모든 클래스는 JVM의 클래스 로더가 클래스 정보를 읽어와서 해당 정보를 메모리에 저장해 놓는다. 
(클래스 정보 - 각 클래스가 어떠한 생성자, 메서드, 필드들을 포함하고 있는지 나타낸다)

    - 컴파일 시간이 아닌 **런타임에** 동적으로 특정 클래스의 정보를 추출할 수 있는 프로그래밍 기법.
    - 구체적인 클래스 타입을 알지 못하더라도 (**접근 제어자가 private 이더라도**) 해당 클래스의 메서드, 타입, 변수들에 접근할 수 있도록 해주는 
  Java API.

getControllers() 메서드가 `instantiateControllers()` 메서드를 호출하며 조회한 클래스들을 파라미터로 전달하여 인스턴스화 시키는 로직을 수행한다.

![스크린샷 2022-10-20 오후 11.58.46.png](https://i.imgur.com/20ccxTJ.png)

- `controllerClass.getConstructor().newInstance();`
    - public으로 선언되어 있으며 매개변수가 없는 생성자에 접근
    - newInstance()를 통해 객체 인스턴스를 생성한다.

생성된 객체의 인스턴스들은 HashMap 형태의 자료구조에 클래스 정보를 키값으로 하여 저장된다.

저장된 인스턴스들은 getControllers() 메서드에 의해 해당 메서드를 호출한 객체에게 전달되어 @Controller 어노테이션이 설정된 Controller 객체들을 특정 패키지내에서 전부 탐색 할 수 있다.

## 3. 레거시 MVC → @MVC

ControllerScanner를 구현하였으니 어노테이션 기반 매핑을 담당할 HandlerMapping 클래스를 추가적으로 구현한 뒤에 초기화해야 한다.

현재의 MVC 구조를 어노테이션 구조로 점진적으로 리팩로링 하기 위해서는 레거시 MVC와 어노테이션 MVC가 공존할 수 있는 환경을 구축해야만 했다.

![스크린샷 2022-10-20 오후 4.22.54.png](https://i.imgur.com/toNV1am.png)

기존의 `RequestMapping` 은 리팩토링 완료시까지 사용되어야 하고 새로운 어노테이션 기반의 `AnnotationHandlerMapping` 을 구현해야 했기 때문에 각 Mapping의 공통된 부분을 추상화한 인터페이스를 만들어 주었다.

![스크린샷 2022-10-20 오후 4.22.54.png](https://i.imgur.com/q8xpHuH.png)

![스크린샷 2022-10-20 오후 4.30.13.png](https://i.imgur.com/xknlOjm.png)

HandlerMapping의 구현체중 RequestMapping은 이전 포스트에서 살펴보았기 때문에 새로 구현한AnnotationHandlerMapping을 살펴보도록 하자.

## 4. AnnotationHandlerMapping

![스크린샷 2022-10-20 오후 5.18.52.png](https://i.imgur.com/L2mVoR5.png)

AnnotationHandlerMapping의 생성자는 파라미터로 가변인자를 전달 받으며 필드값인 `basePackage` 를 초기화 한다. 이 외에도 `HandlerKey` 와 `HandlerExecution` 을 각각 key-value 값으로 저장하는 `handlerExecutions` 멤버 변수를 알고있다.

## 4.1 AnnotationHandlerMapping의 메서드

AnnotationHandlerMapping 객체에 선언되어있는 메서드들을 하나씩 알아보도록 하자.

### 4.1.1 initMapping()

initMapping()은 서블릿 컨테이너가 DispatcherServlet의 init() 메서드를 호출하면서 init() 메서드 내에서 호출 된다.

![스크린샷 2022-10-20 오후 5.18.52.png](https://i.imgur.com/bF34OoX.png)

initMapping() 메서드는 `ControllerScanner` 객체를 생성하고 ControllerScanner의 `getControllers()` 메서드를 호출하여 basePackage 내에 @Controller 어노테이션이 설정된 모든 Controller 인스턴스를 조회해 온다.

그다음 `initHandlerExecution(controllers)` 를 호출하게 된다.

### 4.1.2 initHandlerExecution()

![스크린샷 2022-10-20 오후 11.07.27.png](https://i.imgur.com/rqTBUTB.png)

initHandlerExecution(controllers) 메서드는 가장 먼저 `getMethods(controllers)` 메서드를 호출하여

ControllerScanner를 통해 찾은 Controller 클래스의 메서드 중 `@RequestMapping` 어노테이션이 설정되어 있는 모든 메서드를 Reflections 라이브러리를 활용하여 찾는다.

![스크린샷 2022-10-20 오후 11.09.52.png](https://i.imgur.com/VqmEpzW.png)

그 다음 initHandlerExecution(controllers) 메서드는 반복문을 돌면서 다음의 작업을 수행한다.

- 앞서 getMethods로 찾은 메서드가 선언되어 있는 클래스의 정보를 반환한다.
- 반환된 클래스에 설정된 어노테이션 중 Controller 어노테이션 정보를 annotation 변수에 저장해 놓았다.
- addHandlerExecution() 메서드를 호출한다.

### 4.1.3 addHandlerExecution()

`addHandlerExecution()` 메서드는 @RequestMapping이 선언되어 있는 메서드가 속한 클래스의 인스턴스, 요청 URL 정보, 그리고 @RequestMapping이 설정된 메서드를 인자로 전달받아 `Map<HandlerKey, HandlerExecution> handlerExecutions` 에 값을 저장하는 로직을 수행한다.

![스크린샷 2022-10-21 오전 12.06.55.png](https://i.imgur.com/juikAJ1.png)

![스크린샷 2022-10-21 오전 12.06.55.png](https://i.imgur.com/jiuEOEG.png)

메서드에 설정한 @RequestMapping에 대한 정보를 불러온 다음 `createHandlerKey()` 메서드를 통해서 handlerExecutions의 키값이 될 HandlerKey 객체를 생성해 주었다.

![스크린샷 2022-10-21 오전 12.06.40.png](https://i.imgur.com/oGnvY43.png)

![스크린샷 2022-10-21 오전 12.06.40.png](https://i.imgur.com/5vTnM4o.png)

createHandlerKey() 메서드는 @RequestMapping에 설정한 value(URL)와 method(HttpMethod) 값을 불러온 다음 @Controller에 설정된 공통 URL값과 @RequestMapping URL을 조합한 전체 URL을 HandlerKey의 첫번째 생성자 인자로, method를 두번째 인자로 넘겨서 객체를 생성하여 반환한다.

`HandlerKey` 객체는 요청 url을 저장하는 url과 HttpMethod를 enum 타입으로 관리하는 requestMethod 멤버 변수를 알고 있다. hashCode & equals를 재정의하여 HandlerKey를 키값으로 사용할 수 있도록 하였다.

![스크린샷 2022-10-23 오전 12.08.42.png](https://i.imgur.com/iuA2PBB.png)

![스크린샷 2022-10-23 오전 12.08.42.png](https://i.imgur.com/FhSvs43.png)

![스크린샷 2022-10-23 오전 12.08.42.png](https://i.imgur.com/kSNe4a1.png)

### 4.1.4 getHandler()

AnnotationHandlerMapping에 HttpRequest가 전달되면 해당 요청 URL과 Http 메서드에 해당하는 HandlerExecution을 반환해야 하며 이를 `getHandler()` 메서드를 통해 수행한다.

![스크린샷 2022-10-23 오전 12.38.08.png](https://i.imgur.com/du3fch7.png)

## 5 HandlerExecution

`HandlerExecution` 은 실행할 메서드가 존재하는 클래스의 인스턴스 정보와 실행할 메서드 정보를 가진다. HandlerAdapter 인터페이스를 상속받아서 HandlerAdapterStorage 객체에 추가해 주었다.

![스크린샷 2022-10-23 오전 12.38.08.png](https://i.imgur.com/TeewPYu.png)

![스크린샷 2022-10-23 오전 12.38.08.png](https://i.imgur.com/reGh57l.png)

![스크린샷 2022-10-23 오전 12.38.08.png](https://i.imgur.com/knnOPef.png)

## 6. 기존 MVC와 어노테이션 MVC 통합

![스크린샷 2022-10-20 오후 4.42.21.png](https://i.imgur.com/JeyjJN8.png)

init() 메서드를 통한 DispatcherServlet의 초기화 과정에서 `RequestMapping` 과 `AnnotationHandlerMapping` 모두 초기화 되며 두 HandlerMapping을 List로 관리한다.

![스크린샷 2022-10-20 오후 4.42.21.png](https://i.imgur.com/bFW3VJI.png)

init() 메서드가 정상적으로 수행 되었다면 그다음으로 service(request, response) 메서드가 수행된다.

service() 메서드는 앞에서 초기화한 2개의 HandlerMapping에서 요청 URL에 해당하는 Handler(Controller)를 찾아 메서드를 실행한다. (`getHandler(request)`)

![스크린샷 2022-10-20 오후 4.42.21.png](https://i.imgur.com/Uy3qXkC.png)

그다음 service() 메서드는 찾은 Handler의 인스턴스를 비교하여 해당 Handler를 처리할 수 있는 HandlerAdpater를 불러온다. AnnotationHandlerMapping의 Handler이면 HandlerExecution 타입이기 때문에 HandlerAdapter에는 HandlerExecution이 저장되게 된다.

handleAdapter()와 handle()메서드를 거쳐서 HandlerExecution의 handle 메서드가 수행되고 요청 URL에 매핑되어 있는 메서드가 수행되게 된다.

![스크린샷 2022-10-20 오후 4.42.21.png](https://i.imgur.com/zFLge2W.png)

이후의 과정은 이전 포스트에서 알아본 기존 MVC 구조의 실행 과정과 동일하다.

## 7. AnnotationHandlerMappingTest

![스크린샷 2022-10-23 오전 1.20.58.png](https://i.imgur.com/hjWiZmm.png)

![스크린샷 2022-10-23 오전 1.20.58.png](https://i.imgur.com/FjXMvWc.png)

리팩토링이 성공적으로 이루어졌는지 확인하기 위해서 다음과 같은 테스트코드를 작성하였다.

테스트가 성공적으로 통과되었으며 점진적으로 리팩토링을 해내는데 성공하였다.

---

### ****참고자료 📚****

[reflections-library, baeldung](https://www.baeldung.com/reflections-library)

[[Spring MVC] HandlerMapping](https://bellog.tistory.com/m/219)

[spring-mvc-handler-adapters, baeldung](https://www.baeldung.com/spring-mvc-handler-adapters)


---

## MVC 프레임워크 만들기 - MVC 1편

- **URL**: https://headf1rst.github.io/log/ko/post12
- **Date**: 2022-10-08 10:00
- **Tags**: `Spring` `MVC`

### Content


7월에 [넥스트 스텝](https://edu.nextstep.camp/)에서 진행하는 [만들면서 배우는 스프링 3기](https://edu.nextstep.camp/s/I7LCaCf3)에 참여하였습니다.

이 포스트는 해당 과정에서 스스로 고민하며 MVC 프레임워크를 차근 차근 구현해보았던 과정입니다.

## 1. MVC 패턴의 탄생

### 1.1 Servlet, JSP, Model

MVC 구현 과정을 살펴보기 이전에 MVC 패턴이 무엇이고 어떤 과정을 통해서 발전하였는지 개념을 짚고 넘어가도록 하자.

초기 자바 진영에서는 동적인 웹 페이지를 구현하기 위한 표준으로 [서블릿](https://mangkyu.tistory.com/m/14)이 등장하였다. 서블릿은 WAS의 tcp/ip 연결, 멀티 쓰레드 관리등을 담당하였으며 사용자 요청에 대한 처리와 처리 결과에 따른 응답을 생성해서 HTML 파일을 클라이언트에게 반환하는 역할을 하였다.

하지만 서블릿은 자바 코드로 작성되기 때문에 HTML 파일을 생성하기 어렵다는 단점이 존재했고, 화면 출력 로직을 담당하는 `템플릿 엔진` (JSP, Thymeleaf …)이 등장했다.

JSP의 등장으로 HTML 파일 생성은 쉬워졌지만 JSP가 비즈니스 로직까지 너무 많은 역할을 담당한다는 단점이 여전히 존재했다. (하나의 JSP파일에 코드가 수천줄이 넘어가고 유지보수가 어려웠다.)
UI와 로직의 역할 분리가 제대로 이뤄지지 않았기 때문에 간단한 UI를 변경하더라도 로직까지 함께 수정해야 하는 등 변경 라이프 사이클이 맞지 않았기 때문에 유지보수에 좋은 구조라고 볼 수도 없었다.

유지보수에 유연한 구조를 생성하고자 어플리케이션 구성 요소의 관심사를 분리한 MVC 패턴을 도입하였으며 서블릿, JSP 조합 MVC 패턴을 통해서 로직과 뷰 부분을 나누어서 개발하기 시작했다. 이후 MVC 패턴을 기반으로한 여러 MVC 프레임워크가 등장하기 시작했다 (스트럿츠, 스프링 MVC 등)

![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/ModelViewControllerDiagram2.svg/1200px-ModelViewControllerDiagram2.svg.png)
출처 - 위키피디아

- **MVC 패턴의 구성 요소**
    - `Controller`
        - HTTP 요청을 받아서 파라미터를 검증하고 비즈니스 로직을 실행하는 역할.
        - View에 전달할 결과 데이터를 조회해서 Model에 담는 책임을 수행한다.
    - `Model`
        - View에 출력할 데이터를 저장한다.
        - (View는 로직이 처리된 결과 데이터가 Model에 담겨서 전달되기 때문에 비즈니스 로직이나 데이터 접근을 몰라도된다. 오직 화면 렌더링 역할에만 최적화 될 수 있다.)
    - `View`
        - `Model` 에 담긴 데이터를 사용하여 화면을 렌더링하는 책임을 수행한다.
        

### 1.2 Front Controller 패턴

초기 MVC 패턴에서 클라이언트들은  서로 다른 Controller를 호출하였으며 공통 코드들이 각 Controller에 포함되어있었다.

코드의 중복을 없에기 위해서 모든 Controller로 가기 위한 입구 역할을 하는 Front Controller 패턴을 MVC 프레임워크에 도입하였다.

![스크린샷 2022-10-09 오후 6.28.21.png](https://i.imgur.com/e0ZXbT7.png)

바로 이 Front Controller가 Spring MVC 프레임워크의 `DispatcherServlet` 이며 다음과 같은 역할을 수행한다.

- 클라이언트의 요청을 받아서 요청 (uri)에 맞는 컨트롤러를 찾아 호출.
- 공통 기능을 처리.
    - 덕분에 Front Controller를 제외한 나머지 Controller는 Servlet을 사용하지 않아도 된다.

더 자세한 내용은 해당 [링크](https://www.baeldung.com/java-front-controller-pattern)를 통해서 알아보기로 하고 이제부터는 코드를 살펴보도록 하자.

## 2. DispatcherServlet

Front Controller의 역할을 하는 DispatcherServlet 클래스 코드를 만들어보자.

### 2.1 기능 구현

가장 먼저 [HttpServlet](https://docs.oracle.com/javaee/7/api/javax/servlet/http/HttpServlet.html) 인터페이스를 상속받아서 [init() 과 service()](https://docs.oracle.com/javaee/6/api/javax/servlet/Servlet.html#:~:text=init,-void%20init(ServletConfig&text=config)%20throws%20ServletException-,Called%20by%20the%20servlet%20container%20to%20indicate%20to%20a%20servlet,servlet%20can%20receive%20any%20requests.) 메서드를 다음과 같이 구현하였다.

![스크린샷 2022-10-09 오후 10.53.15.png](https://i.imgur.com/ptoieWN.png)

`init()` 메서드는 서블릿 컨테이너에 의해 한번 호출되면서 필드값에 값을 주입하게 된다.

필드값은 `RequestMapping` 과 `HandlerAdapterStorage` 객체를 선언하였는데, 각각 initMapping(), init() 메서드가 호출된다.

- `HandlerAdapterStorage` - init()
    - 어댑터를 등록한다.
    - HandlerExecution 객체와 SimpleControllerHandlerAdapter 객체의 인스턴스를 생성하여 handlerAdapters 리스트에 **순서 대로** 추가한다.

여기서 순서대로 어댑터들이 저장된것을 주의해야 하는데, 동일한 URI를 처리하는 어댑터가 여러개 존재하더라도 우선순위를 부여하기 위함이다. 이후에 `getHandlerAdapter()` 메서드가 호출되면 `HandlerExecution` → `SimpleControllerHandlerAdapter` 순으로 리스트에서 객체를 가져와서 인자로 주어진 handler를 지원하는지 여부를 검사한다.

> **Handler** 라는 용어가 어색할 수 있는데, Handler는 더 넓은 의미에서의 Controller를 뜻하며 현재 포스트에서는 Handler와 Controller를 같은 의미로 받아들여도 좋다.

  ![스크린샷 2022-10-09 오후 10.50.52.png](https://i.imgur.com/CtG6FlS.png)

- `ReqeustMapping` - initMapping()
    - 특정 requestURI를 처리할 수 있는 컨트롤러 객체를 생성한 다음 URI와 매핑하여 HashMap에 저장한다.

    ![스크린샷 2022-08-23 오전 3.01.41.png](https://i.imgur.com/oNV68VT.png)

## 2.1.1 service 메서드

`service(request, response)` 메서드는 DispatcherServlet의 init() 메서드가 성공적으로 수행되었을때 서블릿 컨테이너에 의해서 호출된다.

![스크린샷 2022-10-09 오후 11.00.31.png](https://i.imgur.com/KKZ6gds.png)

service 메서드의 핵심 로직을 살펴보도록 하자.

- `getHandler(request)` - requestURI와 매핑되는 Controller를 찾아온다.
- `requestMapping.findController(requestURI)` 메서드는 RequestMapping 객체내 URI를 키값으로 하는 값(Controller)을 반환한다. 
    ![스크린샷 2022-10-09 오후 11.04.15.png](https://i.imgur.com/pF0xb8p.png)
    

- 찾은 Controller를 지원하는 HandlerAdapter를 찾는다.
- HandlerAdapter는 지원하는 Controller에 구현된 handle 메서드를 수행하고 ModelAndView 객체를 반환한다.
    
    ![스크린샷 2022-10-09 오후 11.02.43.png](https://i.imgur.com/6r7svKL.png)
    

### 2.2 DispatcherServlet 전체 코드

```java
package core.mvc.asis;

import core.mvc.ModelAndView;
import core.mvc.tobe.*;
import core.mvc.view.View;
import exception.NotFoundException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.http.HttpStatus;

import javax.servlet.ServletException;
import javax.servlet.annotation.WebServlet;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

@WebServlet(name = "dispatcher", urlPatterns = "/", loadOnStartup = 1)
public class DispatcherServlet extends HttpServlet {
    private static final long serialVersionUID = 1L;
    private static final Logger logger = LoggerFactory.getLogger(DispatcherServlet.class);

    private HandlerAdapterStorage handlerAdapterStorage;
    private RequestMapping requestMapping;

    @Override
    public void init() {
        handlerAdapterStorage = new HandlerAdapterStorage();
        handlerAdapterStorage.init();

        requestMapping = new RequestMapping();
        requestMapping.initMapping();
    }

    @Override
    protected void service(HttpServletRequest request, HttpServletResponse response) throws ServletException {
        Object handler = getHandler(request);
        if (handler == null) {
            throw new NotFoundException(HttpStatus.NOT_FOUND);
        }
        HandlerAdapter adapter = handlerAdapterStorage.getHandlerAdapter(handler);
        handleAdapter(request, response, handler, adapter);
    }

    private void handleAdapter(HttpServletRequest request, HttpServletResponse response,
                               Object handler, HandlerAdapter adapter) throws ServletException {
        try {
            handle(request, response, handler, adapter);
        } catch (Exception e) {
            logger.error("Exception: {}", e.getMessage());
            throw new ServletException(e.getMessage());
        }
    }

    private void handle(HttpServletRequest request, HttpServletResponse response,
                        Object handler, HandlerAdapter adapter) throws Exception {
        ModelAndView modelAndView = adapter.handle(request, response, handler);
        View view = modelAndView.getView();
        view.render(modelAndView.getModel(), request, response);
    }

    private Object getHandler(HttpServletRequest request) {
        String requestURI = request.getRequestURI();
        logger.debug("Method: {}, RequestURI: {}", request.getMethod(), requestURI);
        return requestMapping.findController(requestURI);
    }
}
```

## 3. HandlerAdapter

- HandlerAdapter란?
    - `HandlerAdapter`
        - DispatcherServlet과 Controller 사이에 HandlerAdpater를 위치시켜서 DispatcherServlet이 다양한 형태의 Handler(Controller)를 호출할 수 있게 한다.
    - `어뎁터 패턴`
        - 기존 코드를 클라이언트가 사용하는 인터페이스의 구현체로 바꿔주는 패턴
        - 어뎁터 패턴을 사용하여 DispatcherServlet이 다양한 반환 타입의 컨트롤러를 처리할 수 있게 한다.
        - 장점
            - 기존 코드를 변경하지 않고 원하는 인터페이스 구현체를 만들어 재사용 가능
            - 기존 코드가 하던 일과 특정 인터페이스 구현체로 변환하는 작업을 각기 다른 클래스로 분리하여 관리가 가능
        - 단점
            - 복잡도 증가. 때에 따라서 기존 코드가 해당 인터페이스를 구현하도록 수정하는 것이 좋은 선택이 될 수 있다.

다양한 어댑터를 유연하게 사용하기 위한 `HandlerAdapter` 인터페이스를 선언해 주었다.

![스크린샷 2022-10-09 오후 11.11.31.png](https://i.imgur.com/CF6LYyF.png)

- `boolean supports(Object handler)`
    - 어댑터가 핸들러(컨트롤러)를 처리할 수 있는지 여부를 판단한다.
- `ModelAndView handle(HttpServletRequest, HttpServletResponse, Object)`
    - 어댑터가 실제 컨트롤러를 호출하고 결과로 ModelAndView를 반환 한다.
    - 컨트롤러가 ModelAndView를 반환하지 못하면 어댑터가 ModelAndView를 생성해서 반환 한다.

### 3.1 SimpleControllerHandlerAdapter

HandlerAdapter 인터페이스의 구현체인 `SimpleControllerHandlerAdapter` 객체를 살펴보면 `support()` 의 인자로 주어진 handler가 Controller 인터페이스를 구현한 객체일 경우 true를, 그렇지 않을 경우 false를 반환한다.

![스크린샷 2022-10-09 오후 11.14.21.png](https://i.imgur.com/9UjRZsq.png)

`handle()` 메서드는 아래와 같이 DispatcherServlet의 `handlerAdater()` → `handle()` → `adapter.handle()` 메서드에 의해서 호출된다.

![스크린샷 2022-10-09 오후 11.02.43.png](https://i.imgur.com/BA0tMbg.png)

만약 요청 uri를 처리할 수 있는 Controller를 알고있는 어댑터가 `SimpleControllerHandlerAdapter`일 경우, SimpleControllerHandlerAdapter의 handle 메서드가 호출된다.
handle() 메서드의 인자로 받아온 handler 객체를 Controller로 다운캐스팅 한 다음 `execute()` 메서드를 호출하여 Controller가 로직을 수행하도록 요청하게 된다.

## 4. Controller

Controller 인터페이스를 다음과 같이 정의하였고 그 구현체로 다양한 URI를 처리할 수 있는 각각의 Controller 구현체를 구현해 주었다. (HomeController, ForwardController, ListUserController …)

![스크린샷 2022-10-09 오후 11.16.06.png](https://i.imgur.com/NwEgKVD.png)

`exeute()` 메서드가 호출되었을때 어떤 로직이 수행되는지 Controller 인터페이스의 구현체인 `ListUserController` 를 예로 알아보도록 하자 .

### 4.1 execute 메서드

![스크린샷 2022-10-09 오후 11.20.09.png](https://i.imgur.com/L8po0KF.png)

ListUserController는 로그인되어있는 상태라면 모든 회원정보를 출력하는 `list.jsp` 파일을 렌더링하고, 그렇지 않은 상태라면 사용자가 로그인을 먼저 하고 다시 요청할 수 있게끔 로그인 폼으로 리다이렉트하는 로직을 수행한다.

이때 사용자의 로그인 상태 여부를 확인하기 위해서 `HttpServletRequest` 의 `getSession()` 메서드를 통해 HttpSession 정보를 isLogined 메서드의 인자로 넘겨주게 되는데 HttpServeltRequest가 Controller에게 데이터를 전달하는 MVC 패턴의 **Model** 역할을 하게 되는 것이다.

좀 더 로직을 자세히 알아보기 위해서 UserSessionUtils 객체를 어떻게 구현하였는지 알아보도록 하자.

### 4.1.1 UserSessionUtils

static 멤버만을 저장하는 유틸성 클래스이기 때문에 객체 생성을 막고자 추상 클래스로 선언해 주었다.

![스크린샷 2022-10-18 오후 8.37.24.png](https://i.imgur.com/cymChPc.png)

앞서 `ListUserController` 의 `execute()` 메서드에서 호출된 `UserSessionUtils.isLogined()` 메서드를 살펴보도록 하겠다.

조건문에서 `getUserFromSession()` 메서드를 호출하고 호출된 메서드는 HttpSession으로 부터 “user”를 키값으로 갖는 속성값을 가져온다.
만약 그러한 속성값이 존재하지 않을 경우 null을 반환하고, 존재할 경우 User 클래스로 다운 캐스팅하여 반환한다.

만약 getUserFromSession()의 결과값이 null일 경우, 해당 회원의 로그인 정보가 세션에 저장되어 있지 않다는 뜻이기 때문에 false를 반환하며 그렇지 않을 경우 로그인 되어있는 회원임을 나타내는 true를 반환한다.

다시 `ListUserController` 클래스의 execute()의 로직을 살펴보면 UserSessionUtils.isLogined() 메서드의 결과값에 따라서 
`RedirectView` 혹은 `ForwardView` 를 생성자의 인자로 넘겨받아 ModelAndView 객체를 생성하여 반환한다.

- UserSessionUtils 전체 코드

```java
package next.controller;

import next.model.User;

import javax.servlet.http.HttpSession;

public abstract class UserSessionUtils {
    public static final String USER_SESSION_KEY = "user";

    public static User getUserFromSession(HttpSession session) {
        Object user = session.getAttribute(USER_SESSION_KEY);
        if (user == null) {
            return null;
        }
        return (User) user;
    }

    public static boolean isLogined(HttpSession session) {
        if (getUserFromSession(session) == null) {
            return false;
        }
        return true;
    }

    public static boolean isSameUser(HttpSession session, User user) {
        if (!isLogined(session)) {
            return false;
        }

        if (user == null) {
            return false;
        }

        return user.isSameUser(getUserFromSession(session));
    }
}
```

## 5. View

`ModelAndView` 객체는 이름에서 알 수 있듯이 이동하고자 하는 View와 model이라는 이름의 HashMap 구조를 멤버변수로 포함하고 있다.

ModelAndView는 Controller의 로직 처리 결과 후 응답할 View와 View에 전달할 값을 저장한다.

![스크린샷 2022-08-23 오전 3.05.00.png](https://i.imgur.com/6qb8wGR.png)

앞서 살펴보았던 DispatcherServlet의 handle() 메서드를 마저 알아보도록 하자. 

![스크린샷 2022-10-09 오후 11.02.43.png](https://i.imgur.com/iqDt1QP.png)

인자로 전달받은 HandlerAdapter의 handle()을 호출하여 이동할 View
(ListUserController의 경우 `ForwardView` or `RedirectView`)와 View에 전달할 모델(데이터)을 알고있는 `ModelAndView` 객체가 반환된다.

반환된 ModelAndView 객체의 `getView()` 메서드를 통해서 이동할 View 구현체를 가져온다.
그다음 View 인터페이스를 상속한 구현체가 정의한 `render()` 함수를 호출하여 model 데이터를 인자로 넘겨주었다.

![스크린샷 2022-08-23 오전 3.05.28.png](https://i.imgur.com/sHLtiFN.png)

![스크린샷 2022-08-23 오전 3.06.57.png](https://i.imgur.com/1uv52fY.png)

View 인터페이스의 구현체인 ForwardView는 JSP 파일이 저장된 경로인 `viewPath` 를 멤버변수로 포함하고 있으며 render() 메서드가 호출되면 
`modelToRequestAttribute()` 메서드를 호출하여 Model에 담긴 데이터를 전부 꺼내서 `request.setAttibute` 에 다 넣어 주었다. 
(Model내 데이터를 전부 HttpServletRequest에 저장하는 이유는 아래에서 설명하도록 하겠다.)

HttpServletRequest의 `getRequestDispatcher(String)` 팩토리 메서드를 통해서 `RequestDispatcher` 객체를 생성해주었고 
메서드의 인자로 viewPath를 주어서 제어권이 이동할 페이지의 경로를 지정하였다.

RequestDispatcher의 `forward(request, response)` 메서드는 getRequestDispatcher(String)의 인자로 주어진 경로의 자원으로 제어를 넘기는 역할을 한다. 
앞서 ListUserController 객체에서 ForwardView를 생성하면서 인자인 viewPath로 `user/list.jsp` 를 명시해 주었기 때문에 
해당 jsp 파일로 제어가 넘어가게 되며 최종적으로 user/list.jsp의 처리 결과가 브라우저에 출력되게 된다.

이때 forward(request, response) 메서드가 제어권을 다른 새로운 자원으로 넘겨주면서 인자로 HttpServletRequest 객체를 포함하기 때문에
jsp 파일 내에서 필요한 데이터를 HttpServletRequest의 속성값에 저장해 줘야만 한다.

때문에 앞에서 modelToRequestAttribute() 메서드를 호출하여 Model의 데이터를 request.setAttribute로 넘겨준 것이다.
ModelAndView 객체내에 model이라는 이름의 필드값이 존재하지만 실질적인 Model의 역할은 HttpServletRequest가 담당하게 된다.

`RedirectView`는 생성자로 redirectPath를 넘겨받으며 `HttpServletResponse` 의 `sendRedirect(String)` 메서드를 호출한다.

![스크린샷 2022-08-23 오전 3.07.16.png](https://i.imgur.com/m5fA7u6.png)

sendRedirect(String) 메서드 또한 forward(request, response)와 마찬가지로 인자로 넘어온 경로로 제어를 이동시킨다. 

하지만 두 메서드는 다음과 같은 차이점이 존재한다.

**Redirect 와 forward의 차이점**

- `redirect`
    - 실제 클라이언트에 응답이 갔다가, 클라이언트가 redirect경로로 다시 요청 → url경로 변경 O
    - 요청이 처리되기 위해 다른 자원이나 다른 서버로 전달된다.
    - HttpServletRequest, Response에 저장되어있던 속성값들이 전부 초기화된다.
        - 브라우저는 redirect 요청을 아예 새로운 요청으로 간주한다.
        - 만약 새로운 자원에서 기존 속성값들을 사용하고 싶다면 세션에 저장하거나 URL과 함께 전달해야한다.
- `forward`
    - 서버 내부에서 일어나는 호출 → url 경로 변경 X
        - 웹 컨테이너가 모든 과정을 처리하고 클라이언트나 브라우저는 포함되지 않는다.
    - 요청이 처리되기 위해 같은 서버의 다른 자원에 전달된다.
    - forward 메서드는 인자로 HttpRequest, Response 객체를 넘겨주기 때문에 새로운 자원으로 제어권이 넘어가서 과정이 처리되더라도 request 객체 내의 이전 데이터를 사용할 수 있다.
        - 때문에 model 내 모든 데이터를 request.setAttribute()로 옮기는 작업을 수행하였다.

최종적으로 jsp파일로 화면 구성에 필요한 데이터가 전달되며 사용자의 요청에 해당하는 결과값이 출력되게 된다.

## 마치며

지금까지 간단한 MVC 프레임워크를 만들어 보았다.

하지만 아직 부족한 부분이 많이 보인다. 스프링 MVC 프레임워크가 다른 MVC 프레임워크와 비교해서 우위를 가져가기 시작한 시점은 바로 어노테이션 기반의 MVC 프레임워크를 도입하면서 부터였다. 

지금의 MVC 프레임워크는 어노테이션 기반이 아니며 동일한 URI여도 다른 HttpMethod라면 다르게 인식해야 하는데 그렇지 못하고 있다.

다음 포스팅에서는 이러한 문제점을 개선하며 점진적 리팩토링 과정을 거쳐서 어노테이션 기반의 MVC 프레임워크로 개선하는 과정을 다뤄보도록 하겠다.

---

### 참고자료 📚

[HttpServlet (Java(TM) EE 7 Specification APIs)](https://docs.oracle.com/javaee/7/api/javax/servlet/http/HttpServlet.html)

[RequestDispatcher.forward() vs HttpServletResponse.sendRedirect()](https://stackoverflow.com/questions/2047122/requestdispatcher-forward-vs-httpservletresponse-sendredirect)

[[개발자 면접준비]#1. MVC패턴이란](https://m.blog.naver.com/jhc9639/220967034588)

[페이지출력, 페이지전환 및 특정 url로 재 요청 을 해주는 RequestDispatcher 의 request.getRequestDispatcher()/forward() / HttpServletResponse의 response.sendRedirect()](https://u-it.tistory.com/m/entry/%ED%8E%98%EC%9D%B4%EC%A7%80%EC%B6%9C%EB%A0%A5-%ED%8E%98%EC%9D%B4%EC%A7%80%EC%A0%84%ED%99%98-%EB%B0%8F-%ED%8A%B9%EC%A0%95-url%EB%A1%9C-%EC%9E%AC-%EC%9A%94%EC%B2%AD-%EC%9D%84-%ED%95%B4%EC%A3%BC%EB%8A%94-RequestDispatcher-%EC%9D%98-requestgetRequestDispatcherforward-HttpServletResponse%EC%9D%98-responsesendRedirect)

[[JSP] 서블릿(Servlet)이란?](https://mangkyu.tistory.com/m/14)


---

## [오브젝트] 7장 - 객체 분해

- **URL**: https://headf1rst.github.io/log/ko/post11
- **Date**: 2022-10-02 10:00
- **Tags**: `객체지향`

### Content


모든 프로그래밍 패러다임은 추상화와 분해의 관점에서 설명 가능

## 추상화 메커니즘
시스템을 분해하는 방법을 프로시저와 데이터 추상화중 하나를 중심으로 하여 결정해야한다.

- 1. 프로시저 추상화
    - 소프트웨어가 무엇을 **해야**하는지 추상화
    - 기능 분해
        - 기능 구현을 위해 필요한 기능들을 순차적으로 찾아가는 탐색의 과정
- 2. 데이터 추상화
    - 소프트웨어가 무엇을 **알아야** 하는지 추상화
    - 타입을 추상화(추상 데이터 타입) vs 프로시저를 추상화(객체지향)
        - 타입을 추상화 하면 `추상 데이터 타입`
        - 프로시저를 추상화 하면 `객체지향`

## 1. 프로시저 추상화와 기능 분해

- 프로시저
    - 반복적으로 수행되는 작업을 한 장소에 모아서 로직을 재사용하고 중복을 방지하는 추상화 방법
    - 내부의 상세 구현을 모르더라도 인터페이스만 알면 프로시저 사용 가능.
    - 기능 분해 방식 - **하향식 접근법**

### 하향식 접근법

최상위 기능을 정의하고(메인 함수) 덜 추상적인 하위 기능으로 분해해 나가는 방법

#### 하향식 접근법의 문제점

- 현대 시스템은 동등한 수준의 다양한 기능으로 구성된다.
- 대부분의 시스템은 하나의 메임 함수로 구성돼 있지 않다.
    - "실제 시스템에 정상이란 존재하지 않는다"
- 요구사항 변경시 메인 함수를 빈번하게 변경해야 한다.
- 로직이 사용자 인터페이스와 강하게 결합되어있다.
- 데이터 형식이 변경될 경우 관련된 모든 함수를 수정해야한다.
- 설계 시작부터 시스템이 **어떻게** 동작해야 하는지에 집중하게 한다.
- 이른 시기에 함수 실행 순서를 고정하여 유연성과 재사용성이 떨어진다.

이와 같은 기능 분해의 문제를 해결하기 위해 정보 은닉과 모듈 개념이 등장.

## 모듈

- 변경의 방향에 맞춰 시스템을 분해하라
- 함께 변경되는 부분을 하나의 구현 단위(모듈)로 묶고 퍼블릭 인터페이스를 통해서만 접근 가능하도록 만드는 것. (높은 응집도, 낮은 결합도)
- 각 모듈은 외부에 감춰야 하는 비밀과 관련성 높은 데이터, 함수의 집합.

### 정보 은닉

시스템을 모듈 단위로 분해하기 위한 기본 원리.

시스템에서 **자주 변경되는 부분을** 상대적으로 덜 변경되는 **인터페이스 뒤로 감춰라.**

- 모듈 분해
    - 감춰야 하는 정보를 선택하고 정보를 안정적인 보호막(퍼블릭 인터페이스)을 통해 보존하는 과정
    - 모듈 분해 후, 기능 분해를 통해서 모듈에 필요한 퍼블릭 인터페이스를 구현

### 모듈의 장점과 한계

#### 장점

- 모듈 데이터 변경으로 인한 파급효과를 제어할 수 있기 때문에 코드 수정 및 디버깅이 용이하다.
- 비즈니스 로직과 사용자 인터페이스에 대한 관심사가 분리된다.
- 전역 변수와 전역 함수를 제거하여 네임스페이스 오염을 방지한다.

#### 단점

- 인스턴스의 개념을 제공하지 않는다.

이러한 단점을 개선하기 위해 `추상 데이터 타입`이라는 개념이 등장.

## 2. 데이터 추상화

### 추상 데이터 타입

- 프로그래밍 언어가 제공하는 타입처럼 동작하는 **사용자 정의 타입**
- 타입을 개발자가 정의할 수 있다.
- 하나의 물리적 타입 안에 개념적 타입에 대한 구현을 감춘다.

### 클래스는 추상 데이터 타입인가?

#### 클래스와 추상 데이터 타입의 차이

- 클래스
    - 상속과 다형성을 지원 - 객체지향 프로그래밍
    - 절차를 추상화 한것
    - 타입을 기준으로 오퍼레이션을 묶는다.
- 추상 데이터 타입
    - 상속과 다형성 지원 X - 객체기반 프로그래밍
    - 타입을 추상화 한것
    - 오퍼레이션을 기준으로 타입을 묶는다

- 클래스가 추상 데이터 타입의 개념을 따르는지 확인하는 방법
    - 클래스 내부에 인스턴스의 타입을 표현하는 변수의 유무
        - 인스턴스 변수에 저장된 값을 기반으로 메서드 내에서 타입을 명시적으로 구분하는건 객체지향을 위반
    - 타입을 기준으로 절차를 추상화하지 않았다면 객체지향 분해가 아니다.

## 변경을 기준으로 선택하라

설계의 유용성은 변경의 방향성과 발생 빈도에 따라 결정된다.

- 클래스 구조를 선택하는 경우
    - 타입 추가라는 변경의 압력이 더 강한 경우
        - 새로운 타입을 빈번하게 추가해야 하는 경우
- 추상 데이터 타입을 선택하는 경우
    - 변경의 주된 압력이 오퍼레이션 추가인 경우
        - 새로운 오퍼레이션을 빈번하게 추가해야 하는 경우

변경의 축을 찾아야 한다.
객체지향적인 접근법이 은총알은 아니다.

## 느낀점

추상 데이터 타입이 사용된 예가 뭐가있을지 고민해 보았는데 스택과 같은 자료구조가 떠올랐다.

자료구조 수업에서 C++로 스택과 연결 리스트 등을 구현해 보았었는데, 이번 기회에 자바로도 한번 구현해 보도록 해야겠다.


---

## 다중 요청 처리를 위한 ThreadPool 적용하기

- **URL**: https://headf1rst.github.io/log/ko/post9
- **Date**: 2022-09-20 10:00
- **Tags**: `Spring` `ThreadPool`

### Content


프레임워크는 개발자가 쉽고 편하게 개발을 할 수 있도록 많은 기술을 추상화해서 제공한다.

스프링 또한 많은 부분이 추상화 되었으며 개발자 스스로가 의문을 갖지 않는다면, 모른채 넘어갈 기술들이 여럿 존재한다.

오늘은 그러한 기술들 중, 개발자들을 대신해서 사용자의 다중 요청을 처리해주는 WAS의 `ThreadPool`에 대해서 알아보도록 하겠다.

# WAS의 Thread 생성 과정

스프링 부트는 2.5.4 버전 이후로 Tomcat (WAS)을 내장하고 있다.

WAS는 사용자의 요청마다 Thread를 할당해 주는데, 코드를 통해 WAS의 Thread 생성 과정을 살펴보도록 하자.

```java
import java.net.ServerSocket;
import java.net.Socket;

public class WebApplicationServer {
    private static final int DEFAULT_PORT = 8080;

    public static void main(String[] args) throws Exception {
        int port = 0;

        if (args == null || args.length == 0) {
            port = DEFAULT_PORT;
        } else {
            port = Integer.parseInt(args[0]);
        }

        // 서버소켓을 생성한다. 웹 서버는 8080 포트를 사용한다.
        try (ServerSocket listenSocket = new ServerSocket(port)) {

            Socket connection;
            while ((connection = listenSocket.accept()) != null) {
                Thread thread = new Thread(new RequestHandler(connection));
                thread.start();
            }
        }
    }
}
```
위 코드는 다음 과정을 순서대로 수행하게 된다.

- 어플리케이션을 실행한다. 
- `try (ServerSocket listenSocket = new ServerSocket(port)) {`
   - 8080 포트를 디폴트로 사용하는 `ServerSocket` 객체를 생성한다. 
   - `ServerSocket` 객체는 주어진 포트에서 들어오는 요청을 청취한다.
- `while ((connection = listenSocket.accept()) != null) {`
   - `listenSocket.accept()`는 연결 요청이 들어올 때까지 블로킹된다.
   - 연결 요청이 들어오면, 연결을 수락하고 `Socket` 객체를 반환한다.
   - 연결 요청이 끊길때 까지 루프를 돌며 요청을 처리한다.
- 클라이언트의 요청이 들어오면, 요청(task)를 처리하기 위한 스레드를 생성하여 할당한다.
- 할당된 스레드가 `RequestHandler` 객체를 생성한다.
   - [thread.start()](https://kim-jong-hyun.tistory.com/101) 에 의해서 Thread가 task를 수행한다
   - `RequestHandler` 에 오바라이드된 `run()` 메서드를 수행한다.
    

하지만 이처럼 사용자 요청이 있을 때 마다 스레드를 생성해서 사용자 요청을 처리하게 되면 다음과 같은 문제가 발생한다.

- 모든 요청마다 스레드를 생성하기 때문에 스레드 생성 비용 발생 → 성능 저하.
- 동시 접속자가 많아질 경우, Context Switching 비용 증가, WAS의 메모리 자원이 부족하여 서버가 다운될 가능성이 존재.

이처럼 여러 유저의 요청을 효율적으로 처리하고 동시 접속자가 많더라도 안정적으로 서비스를 제공하기 위해서 WAS는 **Thread Pool**을 제공한다.

# Thread Pool 이란?

[Thread Pool](https://www.baeldung.com/thread-pool-java-and-guava)은 어플리케이션 실행에 필요한 [Thread](https://en.wikipedia.org/wiki/Thread_(computing))들을 미리 생성한 다음, Pool에 있는 Thread를 돌려가며 사용하여 사용자의 요청을 처리한다.
(Task를 처리한 Thread는 다시 Thread Pool에 반납되어 재사용 된다.)

미리 만들어 두는 방식과 Thread가 task를 처리하는 방식에 따라서 여러 Pool 구현체들이 존재한다.
대표적인 Thread Pool에는 `newFixedThreadPool`이 있다.

## Thread Pool의 Thread 할당 과정

![스크린샷 2022-09-08 오후 3.21.21.png](https://i.imgur.com/kZs00M1.png)

Thread Pool의 Thread 할당 과정을 살펴보면 다음과 같다.

- 설정된 `core size` 만큼 Thread Pool에 Thread를 생성한다.
- 사용자로 부터 Task(요청)가 들어올 때마다 큐에 Task를 저장한다.
- Thread Pool에 idle 상태의 Thread가 있다면 큐에서 Task를 꺼내 해당 Thead에 할당한다.
    - idle 상태의 Thread가 Pool에 존재하지 않으면 Task는 큐에 대기한다.
    - ❗️ 대기중인 Task로 인해 **큐가 꽉 차면, Thread를 새로 생성한다**. (설정된 `Maximum Thread Size` 까지)
    - Maximum Thread Size 까지 Thread의 수가 도달하고 큐도 꽉 차게 되면 추가 Task에 대해선 `Connection-refused` 오류를 반환한다.
- Task를 처리한 Thread는 다시 idle 상태로 Thread Pool에 반납된다.
    - 큐가 비어있고 `core size` 이상의 Thread가 생성되어있다면 **Thread를 삭제한다.**

## Thread Pool을 사용하여 다중 요청을 처리하는 WAS

이제 자바에서 기본으로 제공하는 [ThreadPoolExecutor](https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html) 를 활용해 아래의 설정값을 바탕으로하는 Thread Pool 기능을 추가해보자.

- 최대 Thread Pool의 크기 = `250` (Pool Size)
- 모든 Thread가 사용중인 (Busy) 상태이면 `100` 명까지 대기 상태 유지 (Queue Size)

```java
import java.net.ServerSocket;
import java.net.Socket;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class WebApplicationServer {

    private static final Logger logger = LoggerFactory.getLogger(WebApplicationServer.class);

    private static final int DEFAULT_PORT = 8080;
    private static final int PORT_ARGS_INDEX = 0;
    private static final int THREAD_POOL_SIZE_ARGS_INDEX = 1;
    private static final int THREAD_QUEUE_SIZE_ARGS_INDEX = 2;
    private static final int DEFAULT_THREAD_POOL_SIZE = 250;
    private static final int DEFAULT_THREAD_QUEUE_SIZE = 100;

    public static void main(String args[]) throws Exception {

        int port = port(args);
        ThreadPoolExecutor threadPoolExecutor = threadPoolExecutor(poolSize(args), queueSize(args));

        // 서버소켓을 생성한다. 웹서버는 기본적으로 8080번 포트를 사용한다.
        try (ServerSocket listenSocket = new ServerSocket(port)) {
            logger.info("Web Application Server started {} port.", port);

            // 클라이언트가 연결될때까지 대기한다.
            Socket connection;
            while ((connection = listenSocket.accept()) != null) {
                threadPoolExecutor.execute(new RequestHandler(connection));
            }
        }
    }
    ...

    private static int port(String[] args) {
        return extractValueByIndexOrDefault(args, PORT_ARGS_INDEX, DEFAULT_PORT);
    }

    private static int extractValueByIndexOrDefault(String[] args, int index, int defaultValue) {
        if (args == null || args.length < index + 1) {
            return defaultValue;
        }
        return Integer.parseInt(args[index]);
    }
}
```

코드에서 볼 수 있듯이, Thread Pool 적용의 핵심 로직은 `threadPoolExecutor` 메서드에 존재한다.

`threadPoolExecutor` 메서드를 살펴보기에 앞서, 인자로 전달되는 값을 구하는 두개의 메서드를 살펴보도록 하자.

`threadPoolExecutor` 메서드는 두개의 인자값을 전달받는다.

- Thread Pool의 `core size`
- Task를 저장할 `queue size`

이들은 각각 `poolSize(args)`, `queueSize(args)` 메서드에 의해서 값이 구해진다.

```java
public class WebApplicationServer {
    ...
    
    private static int poolSize(String[] args) {
        return extractValueByIndexOrDefault(args, THREAD_POOL_SIZE_ARGS_INDEX, DEFAULT_THREAD_POOL_SIZE);
    }

    private static int queueSize(String[] args) {
        return extractValueByIndexOrDefault(args, THREAD_QUEUE_SIZE_ARGS_INDEX, DEFAULT_THREAD_QUEUE_SIZE);
    }
```

`port(args)`, `poolSize(args)` , `queueSize(args)` 모두 `extractValueByIndexOrDefault()` 메서드에 의해서 값이 파싱된다.

`extractValueByIndexOrDefault()` 메서드는 사용자의 요청시 전달되는 `args[]` 에 필요한 옵션의 값이 전달되지 않은 경우에 기본값을 반환하고 옵션 값이 전달된 경우에는 전달된 값을 반환한다.

### ThreadPoolExecutor

`poolSize` 와 `queueSize` 메서드로 부터

전달받은 두 인자를 사용하여 `ThreadPoolExecutor` 객체를 생성하여 Thread Pool을 적용한다.

```java
public class WebApplicationServer {
    ...
    
    private static ThreadPoolExecutor threadPoolExecutor(int corePoolSize, int queueSize) {
        return new ThreadPoolExecutor(corePoolSize, corePoolSize, 0L, TimeUnit.MILLISECONDS,
                new LinkedBlockingQueue<>(queueSize));
    }
```

> ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue)

- corePoolSize
    - 어플리케이션 시작시 Pool에 할당되는 쓰레드 수
- maximumPoolSize
    - Pool에 유지될 수 있는 **최대** 쓰레드 수
- keepAliveTime
    - corePoolSize 보다 쓰레드 개수가 많아 진 상태에서, 새로운 테스크를 기다리는 시간.
    - keepAliveTime 이상 시간이 경과하면 쓰레드를 없애서 corePoolSize만큼 쓰레드 수를 유지한다.
- unit
    - keepAliveTime의 시간 단위
- workQueue
    - 실행 되기전에 홀드시켜 두는 테스크를 유지하는 큐.
    - idle 상태의 쓰레드가 없는 경우, 테스크를 workQueue에 저장한다.

- corePoolSize = 1, maximumPoolSize = 1 이면 `newSingleThreadExecutor` 가 된다.
- corePoolSize = 0, maximumPoolSize = MAX_VALUE 이면 `newCachedThreadPool`

현재 우리는 최소, 최대가 250인 고정된 Thread Pool을 생성한 것이다.

`threadPoolExecutor.execute(new RequestHandler(connection))`

Thread Pool을 만들고 나서는 클라이언트의 요청이 들어오기까지 대기하다가 요청이 들어오는 순간 Thread Pool의 idle한 Thread를 하나 할당하여 요청 (Task)를 처리한다.

- `execute(테스크)`
    - Thead Pool에서 하나의 Thread를 할당.

## Thread Pool 수보다 많은 요청을 동시에 보내보기

앞서 적용한 Thread Pool과 동일한 [newFixedThreadPool](https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/Executors.html) 를 사용하여 Thread Pool이 정상 동작하는지 알아보기 위한 테스트를 해보자.

먼저 Thread Pool의 총 스래드 개수보다 적은 동시 요청을 보냈을때 WAS가 정상 동작하는지 검증하는 테스트 코드를 작성하였다.

![스크린샷 2022-09-23 오후 6.24.57.png](https://i.imgur.com/06RShUw.png)

- `executorService.execute()`
    
    Thread Pool에서 스레드를 하나 할당해서 테스크를 수행한다.
    
- `restTemplate.getForEntity()`
    
    동기방식으로, 인자로 주어진 url 주소에 HTTP GET 요청을 보내고 결과는 ResponseEntity를 받는다.
    

Thread Pool에서 스레드를 하나씩 할당해 주어 index.html을 불러오는 get 메서드를 실행하도록 하였다.

그리고 하나의 스레드가 정상적으로 수행을 마치면 latch의 숫자가 하나씩 감소한다.

200개의 스레드를 가지고 있는 Thread Pool이 100개의 요청을 다 처리하였다면 latch의 숫자는 0이어야하며 정상적으로 모든 요청을 처리하였기 때문에 await의 값도 true 여야만 한다.

Thread Pool 내부의 스레드가 요청 테스크 보다 많기 때문에 WAS가 정상 동작할 것이라고 생각하였고

테스트 실행결과, WAS가 3초만에 요청들을 처리하는 것을 확인할 수 있었다.

![스크린샷 2022-09-23 오후 6.23.28.png](https://i.imgur.com/XdtUK2j.png)

이번에는 Thread Pool에 200개의 스레드를 생성하고 이보다 많은 400개의 동시 요청을 처리할 수 있는지를 테스트 해보았다.

![스크린샷 2022-09-23 오후 7.35.43.png](https://i.imgur.com/is78b8W.png)

200개의 고정된 크기의 스레드풀을 만들고 400개의 테스크를 넣어주었기 때문에 200개의 워커(스레드)는 200개의 테스크를 먼저 처리하고, 나머지 200개는 Queue에서 할당되기만을 기다리고 있을 것이다.

이후, 먼저 끝난 워커가 나머지 스레드들을 처리하게 된다.

(더 자세한 과정이 궁금하다면 다음 [링크](https://hamait.tistory.com/937?category=79137)를 참고하기를 바란다.)

![스크린샷 2022-09-23 오후 7.35.24.png](https://i.imgur.com/0PMF4bv.png)

그렇다면 더 많은 700개의 테스크를 200개의 고정 크기 스레드풀이 처리할 수 있을까?

현재 테스트 코드의 스레드 풀은 고정 스레드 풀이기 때문에 초기 스레드 개수에서 스레드가 추가로 생성되지는 않는다. 

때문에 newFixedThreadPool의 workQueue 사이즈 만큼의 테스크를 처리할 수 있을것이다.

java/util/concurrent/Executors.java 파일의 Executors 클래스는 정적 메서드로 newFixedPool을 제공한다.

![스크린샷 2022-09-23 오후 8.14.32.png](https://i.imgur.com/WfjqPOD.png)

고정 스레드 풀이기 때문에 corePoolSize와 maximumPoolSize의 값이 같고 corePoolSize보다 스레드 개수가 많아질 일이 없기 때문에 keepAliveTime은 0이다.

눈여겨봐야할 부분은 바로 workQueue인데, [LinkedBlockingQueue](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingQueue.html) 를 사용한다.

LinkedBlockingQueue는 생성자의 인자로 queue의 사이즈를 지정해주지 않으면 최대 큐 사이즈를 **Integer.MAX_VALUE (2*31 - 1)**로 설정하고 값이 삽입될 때 마다 동적으로 node를 생성하여 값을 저장한다.

덕분에 우리는 Thread Pool에 존재하는 스레드의 개수보다 더 많은 동시요청이 들어오더라도 정수형의 최대 값 만큼의 테스크를 처리할 수 있다.

테스트 실행 결과, 700개의 테스크를 200개의 스레드가 성공적으로 처리한 것을 확인할 수 있다.

![스크린샷 2022-09-23 오후 7.34.31.png](https://i.imgur.com/582ZHWQ.png)

하지만 스프링부트를 사용하면서 Thread Pool을 직접 적용한적도, 신경써본적도 없었을 수 있다.

실제로 스프링부트는 Thread Pool을 직접 구현할 필요 없이 설정 파일을 통해서 Thread Pool 관련 설정을 할 수 있는 기능을 제공한다.

## 스프링 부트의 Embedded Tomcat

스프링과 스프링부트의 차이점 중 하나는 바로 내장 WAS (Tomcat)지원 여부이다.

지금까지 Thread Pool을 적용하기 위한 코드를 작성하고 이를 알아보았지만, 감사하게도 스프링부트는 Tomcat을 내장하고 있다.

 내장 Tomcat 덕분에 위와 같이 Thread Pool을 구현할 필요 없이 `application.yml` 혹은 [application.properties](http://application.properties) 에서 Tomcat의 Connector설정을 변경 할 수 있다.

```yml
# application.yml (적어놓은 값은 default)
server:
  tomcat:
    threads:
      max: 200 # 생성할 수 있는 thread의 총 개수
      min-spare: 10 # 항상 활성화 되어있는(idle) thread의 개수
    max-connections: 8192 # 수립가능한 connection의 총 개수
    accept-count: 100 # 작업큐의 사이즈 (운영체제에서 관리)
    connection-timeout: 20000 # timeout 판단 기준 시간, 20초
  port: 8080 # 서버를 띄울 포트번호
```

## 마치며

지금까지 Thead Pool을 WAS에 적용해 보면서 WAS가 다중 요청을 어떻게 처리하는지에 대해 알아보았다.

사실 다중 요청 처리에는 Thread Pool 뿐만 아니라 WAS의 Connector도 밀접하게 연관되어 있다.

현재 구현된 WAS는 Socket Connection을 처리할 때 자바의 기본적인 I/O를 사용한다.

Thead Pool에 의해 관리되는 스레드는 소켓 연결을 받아 요청을 처리하고 요청에 대해 응답한 다음, 소켓 연결이 종료되면 Thead Pool에 반납된다.

즉, connection이 닫힐 때까지 하나의 스레드는 특정 connection에 계속 할당되어 있게 된다.

이러한 방식으로 스레드를 할당하여 사용하면, 동시에 사용되는 스레드 수가 동시 접속할 수 있는 사용자 수가 되어 버리는데 스레드들이 충분히 사용되지 않고 idle 상태로 낭비되는 시간이 많이 발생하게 된다.

이러한 문제점을 해결하고 리소스를 효율적으로 사용하기 위한 방법으로 `NIO Connector` 가 등장하였는데 다음 포스트에서는 지금까지 구현한 BIO 방식과 NIO 방식의 차이점에 대해서 알아보도록 하겠다.

---

### 참고자료 📚

- [Task queuing in Executors.newFixedThreadPool()](https://medium.com/@amardeepbhowmick92/task-queuing-in-executors-newfixedthreadpool-31bc8c24b4d2)
- [Introduction to Thread Pools in Java](https://www.baeldung.com/thread-pool-java-and-guava)
- [JAVA 쓰레드풀 분석 - newFixedThreadPool 는 어떻게 동작하는가?](https://hamait.tistory.com/937)
- [스프링부트는 어떻게 다중 유저 요청을 처리할까? (Tomcat9.0 Thread Pool)](https://velog.io/@sihyung92/how-does-springboot-handle-multiple-requests)
- [병행성(Concurrency)을 위한 CountDownLatch](https://imasoftwareengineer.tistory.com/100)


---

## [오브젝트] 5장 - 책임 할당하기

- **URL**: https://headf1rst.github.io/log/ko/post8
- **Date**: 2022-09-19 10:00
- **Tags**: `객체지향`

### Content


## 책임 중심 설계

- 어떤 객체에게 어던 책임을 할당할지 결정해야한다
- 문제 해결을 위한 다양한 책임 할당 방법이 존재하며 일종의 트레이드오프 활동이다.
- 상황과 문맥에 따라 최적의 책임 할당 방법을 선택해야한다.

- 책임 중심 설계를 위한 원칙
    - 데이터보다 행동을 먼저 결정
        - 중요한 것은 데이터가 아닌 외부에 제공하는 행동 (객체의 책임)
        - 객체가 수행해야 하는 책임이 무엇인지 결정 후, 책임을 수행하는 데 필요한 데이터를 결정
    - 협력이라는 문맥 안에서 책임을 결정
        - 객체 책임의 품질은 협력에 적합한 정도로 결정된다.
        - 책임은 객체 입장이 아니라 객체가 참여하는 협력에 적합해야 한다.
        - 메시지를 전송하는 클라이언트의 의도에 적합한 책임을 할당하라.
        - 메시지가 객체를 선택하게 하라.

- 메시지를 전송해야 하는데 누구에게 전송해야 하지? 라고 질문
    - -> 메세지 기반 설계
- 객체를 결정하기 전에 객체가 수신할 메시지를 먼저 결정
    - -> 송신자 관점에서 수신자가 캡슐화됨

## 책임 할당 GRASP 패턴

- 설계 시작시 도메인을 책임 할당의 대상으로 사용하라. (도메인 모델 그리기)
    - 개념들의 의미와 관계가 완벽할 필요 없다.
    - 도메인 개념 정리에 시간투자 X. 설계를 시작하기 위한 개념들의 모음이면 충분
- 어플리케이션이 제공해야하는 기능 -> 어플리케이션의 책임
- 책임을 어플리케이션에 대해 전송된 메시지로 간주
    - 메시지를 책임질 첫 번째 객체를 선택하는것으로 책임 주도 설계 시작

### Information Expert 패턴

- 책임을 수행할 정보를 알고 있는 객체에게 책임을 할당하라.
    - 객체의 책임과 책임을 수행하는데 필요한 상태는 동일한 객체 안에 존재해야 한다.
    - 정보와 행동이 가까이 있으면 캡슐화 유지됨 (응집도 높이고 결합도 낮추고)

### Low Coupling & High Cohesion 패턴

- 책임을 할당할 수 있는 다양한 대안이 존재한다면 응집도와 결합도 측면에서 더 나은 대안을 선택.

### Creator 패턴

- 객체를 생성할 책임을 어떤 객체에게 할당할지에 대한 지침을 제공
- 객체 A를 생성해야 할 때 아래 조건을 최대한 만족하는 B에게 생성 책임을 할당
    - B가 A 객체를 포함하거나 참조
    - B가 A 객체를 기록
    - B가 A 객체를 긴밀히 사용
    - B가 A에 대한 정보 전문가다

- 클래스 응집도 판단하는 방법 (변경의 이유가 하나 이상인 클래스를 찾는 방법)
    - 변경의 이유를 기준으로 클래스를 분리하라 (SRP)
    - 클래스의 인스턴스를 초기화하는 시점에 경우에 따라 서로 다른 속성들을 초기화하고 있다면 응집도가 낮다는 증거
    - 메서드 그룹이 속성 그룹을 사용하는지 여부로 나뉜다면 응집도가 낮다는 증거

### Polymorphism 패턴

- 타입을 명시적으로 정의하고 각 타입에 다형적으로 행동하는 책임을 할당하라.
    - 조건문을 사용해 설계하면 변경에 취약한 설계가 된다.
    - 다형성을 이용해 변화를 다루기 쉽게 확장하라

### Protected Varations 패턴

- 변경될 가능성이 높다면 캡슐화하라.
    - 클래스를 변경에 따라 분리하고 인터페이스를 이용해 변경을 캡슐화하라
    - 결합도와 응집도 향상

- 처음부터 책임 주도 설계를 따르는 것보다 동작하는 코드를 먼저 작성하고 리팩터링하는 것이 더 훌륭한 결과물을 낳을 수 있다.
    - 객체로 책임을 분배할 때 메서드를 응집도 있는 수준으로 분해하라
    - 자신이 소유하고 있는 데이터를 스스로 처리하도록 메서드를 알맞는 클래스에 배치하라
        - 메서드를 다른 클래스로 이동시킬 때는 인자에 정의된 클래스 중 하나로 이동하는게 일반적

## 느낀점

책임주도 설계를 잘하기란 정말 어렵다. 때문에 책에서도 언급되었듯이 동작하는 코드를 먼저 작성하고 나서 책임주도 설계로 리팩터링 해나가고는 한다.

하지만 코드의 스멜을 찾기는 생각보다 어려울 수 있고 리팩터링으로 인한 트레이드 오프를 개발자가 인지하지 못 할 수도 있다.

이러한 이유에서 코드리뷰 문화는 코드의 품질을 높이는데 중요한 요소라고 생각하며 선한 피드백을 주고 받을 수 있는 동료가 될 수 있도록 노력하고 그러한 환경을 만드는데 기여해야겠다.


---

## [오브젝트] 4장 - 설계 품질과 트레이드오프

- **URL**: https://headf1rst.github.io/log/ko/post7
- **Date**: 2022-09-12 10:00
- **Tags**: `객체지향`

### Content


좋은 객체지향 설계란 올바른 객체에게 올바른 책임을 할당하면서 캡슐화를 통해 낮은 결합도와 높은 응집도를 가진 구조를 창조하는 것

- 구현
    - 변경될 가능성이 높은 부분
- 인터페이스
    - 상대적으로 안정적인 부분

- 변경의 정도에 따라 `구현`과 `인터페이스`를 분리
    - 외부에서는 인터페이스에만 의존하도록 관계 조절
    - 구현 세부사항을 안정적인 인터페이스 뒤로 캡슐화 해야 한다. - OOP의 핵심

- `캡슐화`
    - 변경될 수 있는 어떤 것이라도 감추는 것
    - 내부 구현 변경으로 인해 외부의 객체가 영향을 받으면 캡슐화 위반
    - 설계시에 변하는게 무엇인지 고려하고 변하는 개념을 캡슐화하라
- `응집도`
    - 모듈 내의 요소들이 하나의 목적을 위해 긴밀하게 협력할 수록 높은 응집도
    - 응집도가 낮으면 변경시에 여러 모듈을 수정해야함.
- `결합도`
    - 다른 모듈과의 의존성 정도
    - 다른 모듈에 대해 꼭 필요한 지식만을 갖게 해서 결합도를 낮추는게 이상적.
    - 결합도가 높으면 변경시에 여러 모듈을 수정해야함.

응집도와 결합도 모두 한 모듈이 변경되기 위해서 다른 모듈의 변경을 요구하는 정도로 측정 가능.

### getter, setter를 지양하자 (추측에 의한 설계 전략 X)

- getter, setter는 캡슐화를 위반.
    - getFee, setFee 등 객체 내부에 fee라는 인스턴스 변수가 존재한다는걸 인터페이스에 나타냄
    - 객체가 수행할 책임이 아니라 내부에 저장할 데이터에 초점을 맞췄기 때문에 문제 발생
    - 객체 내부 구현이 객체의 인터페이스에 드러난다.
        - 객체 내부 구현을 변경하면 해당 인터페이스에 의존하는 모든 클라이언트들도 함께 변경해야 한다. (High coupling)

> 단일 책임 원칙 : 클래스는 단 한가지의 변경 이유만 가져야한다. - 응집도를 높이는 설계 원칙

### 캡슐화 -> 높은 응집도, 낮은 결합도

- 객체는 내부 데이터를 감추고 외부에서는 인터페이스에 정의된 메서드를 통해서만 상태에 접근.
- 의미있는 객체의 메서드
    - = 객체가 책임져야 하는 무언가를 수행하는 메서드

- 객체 스스로 자신의 상태를 처리해야한다.

### 객체의 인터페이스에 구현이 노출되어 캡슐화가 깨진 경우

`isDiscountTable(DayOfWeek dayOfWeek, LocalTime time)`

- 해당 메서드는 객체 내부에 DayOfWeek과 LocalTime 타입의 정보가 인스턴스 변수로 포함되어 있다는 사실을 인터페이스를 통해 외부에 노출한다.
- 객체 내부 속성을 변경하면 위 메서드의 파라미터를 수정하고 해당 메서드를 사용하는 모든 클라이언트도 함께 수정되야 한다.

### 데이터 중심 설계의 문제점

- 너무 이른 시기에 데이터에 관해 결정하도록 강요
- 협력이라는 문맥을 고려하지 않음
    - 객체를 고립시킨 채  오퍼레이션 결정

### 느낀점

많은 프로젝트에서 대부분의 로직은 서비스 레이어에 존재합니다.

로직이 서비스레이어에 존재하게 되면 객체는 단순히 데이터를 제공하는 역할만을 하고 있을 가능성이 높습니다.

가능한 객체 스스로 로직을 처리할 수 있는 구조로 변경해야 하며 서비스 레이어는 레포지토리 레이어와 소통해서 객체에 메시지를 전달하여 도메인이 로직을 처리할 수 있도록 하는 역할에 집중되어야 한다고 생각합니다.

이러한 방식은 객체지향적인 구조를 제공할 뿐만 아니라 테스트를 더 용이하게 해줍니다.

개인적으로 Mock을 사용하면 테스트 코드를 읽기 어려워지고 구현 비용도 크기 때문에 선호하지 않는데, 서비스 레이어의 로직을 객체로 옮기게 되면 Mock을 사용할 필요가 없어지기 때문에 테스트 작성 비용을 줄일 수 있지 않을까 생각해보았습니다.

마지막으로 "좋은 설계란 오늘의 기능을 수행하면서 내일의 변경을 수용할 수 있는 설계다." 라는 말이 인상깊었던 구절이었습니다.

이번장을 읽으면서 떠올렸던 객체지향 생활체조 원칙과 포스트 몇개를 찾아보았습니다.
- [getter 메소드를 사용하지 않도록 리팩토링한다.](https://www.slipp.net/questions/565)
- ["getter를 사용하는 대신 객체에 메시지를 보내자"](https://tecoble.techcourse.co.kr/post/2020-04-28-ask-instead-of-getter/)
- ["객체지향 생활체조 원칙 9 - 게터/세터/프로퍼티를 쓰지 않는다"](https://limdingdong.tistory.com/15)


---

## [오브젝트] 3장 - 역할, 책임, 협력

- **URL**: https://headf1rst.github.io/log/ko/post6
- **Date**: 2022-09-05 10:00
- **Tags**: `객체지향`

### Content


- 객체지향의 본질 : 협력하는 객체들의 공동체를 창조하는 것
    - 기능 구현을 위해 어떤 협력이 필요하고 협력을 위해 어떤 역할, 책임이 필요한지 파악

- 객체들은 메시지를 주고 받으며 협력한다

- `협력`
    - 어플리케이션 기능 구현을 위한 객체들의 상호작용
    - 다른 객체에 무엇인가를 **요청**하는 것
- `책임`
    - 협력에 참여하기 위해 객체가 수행하는 로직
- `역할`
    - 협력안에서 수행하는 책임이 모여 객체가 수행하는 역할을 구성

## 협력 - 객체 설계를 위한 문맥 제공

객체지향 시스템 - `자율적`인 객체들의 `공동체`
객체는 고립된 존재가 아닌 협력하는 사회적 존재

- 한 객체는 어떤 것이 필요할 때 다른 객체에게 전적으로 `위임` 하거나 서로 `협력`한다.
    - 객체는 메시지를 통해서 협력한다
    - 메시지를 수신한 객체는 `메서드`를 실행해 요청에 응답
        - 객체는 수신받은 메시지를 처리할 방법을 **스스로** 선택한다
            - 객체는 자율적인 존재
            - 객체 내부 구현 (상태, 행동)을 캡슐화 하여 자율성을 확보

### 객체 설계시 상태와 행동을 결정하는 기준

객체가 참여하고 있는 협력에 따라 객체의 `상태` 와 `행동`이 정해진다.

메세지 처리 가능 여부로 협력이 결정 -> 협력이 객체의 행동을 결정 -> 행동이 객체의 상태를 결정

객체의 상태는 그 객체의 행동에 필요한 정보가 무엇인지로 결정된다.

## 책임

- `책임`
    - 협력에 참여하기 위해 객체가 수행하는 행동
    - 하는 것과 아는 것, 두 가지 범주
    - `하는 것`
        - 객체 생성 및 계산 수행 등 스스로 하는것
        - 다른 객체의 행동을 시작하는 것
        - 다른 객체의 활동을 제어, 조절하는 것
    - `아는 것`
        - 사적인 정보를 아는것
        - 관련된 객체를 아는것
        - 자신이 유도하거나 계산할 수 있는 것에 관해 아는 것

책임을 할당하기 위해서는 협력을 정의해야한다.
협력을 설계하기 위해서는 시스템이 사용자에게 제공하는 기능을 시스템이 담당할 하나의 책임으로 바라보는것에서 시작.

- 시스템은 사용자에게 제공해야 하는 `기능`인 시스템 책임을 파악
- 시스템 책임을 더 작은 책임으로 분할
- 분할된 책임을 수행할 수 있는 적절한 객체 (역할)을 찾아 책임 할당
- 해당 객체에 책임 할당하여 두 객체가 협력하도록 한다.

객체가 책임을 수행하게 하는 방법 : 메시지를 전송하는 것
책임을 할당하는 것 = 메시지의 이름을 결정하는 것

### 메시지가 객체를 결정

- 객체에게 책임을 할당하는 데 필요한 메시지를 먼저 식별
- 메시지를 처리할 객체를 선택

1. 객체가 최소한의 인터페이스를 가질 수 있게 된다
2. 객체는 충분히 추상적인 인터페이스를 가질 수 있게 된다
    - 인터페이스는 무엇을 하는지는 표현하지만 어떻게 수행하는지는 노출하면 안된다.

### 행동이 상태를 결정

협력에 필요한 객체의 행동을 결정하고 나서 상태를 결정
협력 관계 속에서 다른 객체에게 무엇을 제공하고 무엇을 얻어야 할지를 고민하라

## 역할을 통해 유연하고 재사용 가능한 협력을 얻는다

- `역할`
    - 다른 것으로 교체할 수 있는 책임의 집합
    - 다양한 종류의 객체를 수용할 수 있는 일종의 슬롯
    - 구체적인 객체들의 타입을 캡슐화하는 추상화
    - 역할을 구현하는 방법
        - 추상 클래스
        - 인터페이스

- 협력에 적합한 책임을 수행하는 대상이
    - 한 종류라면 - 객체
    - 여러 종류의 객체들이 참여할 수 있다면 - 역할

역할 덕분에 설계의 구성 요소를 추상화 할 수 있다

### 느낀점

이번장을 읽으면서, 머리로는 이해한 객체 설계 기법을 어떻게 하면 프로젝트에 적용해 볼 수 있을까 하는 고민을 해보았습니다.

개인적으로 TDD가 모듈간의 의존도를 낮추고 객체지향적으로 프로그램을 설계하는데 도움을 줄 수 있지 않을까 생각해보았습니다.

TDD를 기반으로 한다면 의존관계가 높은 테스트 코드 자체를 만들기가 어렵기 때문에 하나의 큰 기능을 작은 단위의 기능으로 쪼게게 됩니다.

또한 자연스럽게 테스트를 통과하기 위한 메서드를 만든 다음, 메서드를 위한 필드값을 정의하는 순서로 구현이 이루어지기 때문에 행동이 상태를 결정하는 구조로 설계가 이루어 집니다.

이러한 이유에서 TDD는 테스트 기법이 아닌 설계 기법이라 하지 않을까 생각해보게 되었습니다.


---

## [오브젝트] 2장 - 객체지향 프로그래밍

- **URL**: https://headf1rst.github.io/log/ko/post5
- **Date**: 2022-08-29 10:00
- **Tags**: `객체지향`

### Content


클래스를 먼저 결정하고, 어떤 `속성`과 `메서드`가 필요한지 고민하는것리 아니라 `객체`에 초점을 맞춰야한다

1. 어떤 클래스가 필요한지 이전에 어떤 객체가 필요한지 고민하라
   클래스는 공통적인 상태, 행동을 공유하는 객체를 추상화한것

2. 객체는 독립적인 존재가아닌 협력하는 공동체의 일원이다

- 객체의 윤곽을 잡고 공통된 특성과 상태를 가진 객체들을 **타입** 으로 분류하라
- 타입을 기반으로 클래스를 구현하라

클래스의 내부와 외부를 구분. 경계를 명확하게하여 객체의 자율성을 보장한다 -> 개발자의 구현의 자유 제공

객체는 상태와 행동을 갖고 스스로 판단하고 행동하는 자율적인 존재

데이터와 기능을 객체 내부로 함께 묶는것 -> `캡슐화`

- 객체를 이용해서 타입을 정의하도록 하자
    - 의미를 명시적이고 분명하게 표현 할 수 있으며 이는 설계의 명확성과 유연성을 높인다

협력 관점에서 어떤 객체가 필요할지 결정하고 객체들의 공통 상태와 행위를 구현하기 위해 클래스를 작성

### 의존성의 양면성

코드의 의존성과 실행시점의 의존성은 다를수 있다.
두 의존성이 다르면 다를수록 코드는 더 유연해지고 확장성이 좋아진다.

반면 코드를 이해하기 위해서는 객체를 생성하고 연결하는 부분을 찾아야하기 때문에 코드를 이해하기는 힘들어진다

설계는 곧 `트레이드오프의 산물`.

### 다형성


인터페이스 -> 객체가 이해할 수 있는 메시지의 목록을 정의

메세지와 메서드는 다른개념이다.

예를들어 영화 객체가 할인 정책 객체의 인스턴스에 calculateDiscountPolicy 메시지를 전송한다.

실행되는 메서드는 영화와 협력하는 객체, 즉 메시지를 수신하는 객체의 클래스가 무엇이냐에 따라 달라진다. - `다형성`

다형성은 객체지향 프로그램의 컴파일 시간 의존성과 실핼 시간 의존성이 다를 수 있다는 사실을 기반으로 한다

다형적 협력에 참여하는 객체들은 모두 같은 메시지를 이해할 수 있어야 한다 -> 인터페이스가 동일해야 한다

인터페이스를 통일하기 위해 사용한 방법 -> 상속

**동적 바인딩:** 메시지와 메서드를 실행 시점에 바인딩

**정적 바인딩**: 컴파일 시점에 실행될 함수나 프로시저를 결정하는것

우리는 동적 바인딩 메커니즘을 사용하여 컴파일 시점의 의존성과 실행 시점의 의존성을 분리하고 하나의 메시지를 선택적으로 다른 메서드에 연결할 수 있다

상속을 사용하여 동일한 인터페이스를 공유하는 클래스들을 하나의 **타입 계층**으로 묶을수 있다

### 구현 상속과 인터페이스 상속

**구현 상속 (서브클래싱)** : 코드 재사용 목적으로 상속을 사용한 경우

**인터페이스 상속 (서브타이핑)** : 다형적인 협력을 위해 부모 클래스와 자식 클래스가 인터페이스를 공유 할 수 있도록 상속을 사용한 경우

인터페이스를 재사용할 목적이 아니라 구현을 재사용할 목적으로 상속을 사용하면 변경에 취약한 코드를 생산할 수 있다

### 추상화의 장점
- 추상화의 계층만 따로 보면 요구사항의 정책을 높은 수준에서 서술할 수 있다
- 상위 정책을 기술하여 어플리케이션의 협력 흐름을 술
- 설계가 유연해진다

### 유연한 설계
예외 케이스를 최소화하고 일관성을 유지할 수 있는 방법을 선택하라

유연성이 필요한 곳에 추상화를 사용하라

### 코드 재사용
코드를 재사용 하기 위해서는 상속보다는 **합성**을 사용하라

### 코드 재사용으로 상속을 지양하는 이유

코드 재사용을 목적으로 상속을 사용하면 설계에 안좋은 영형을 끼친다.

- 캡슐화를 위반
  상속을 이용하기 위해서 개발자는 부모 클래스릐 내부 구조를 알아야한다

결과적으로 부모 클래스가 자식 클랴스에 노출되어 캡슐화가 약화된다

캡슐화가 약화되어 자식이 부모 클래스와 강하게 결합되기 때문에 부모 클래스가 변경되면 자식 클래스도 함께 변경되야 한다

- 유연하지 못한 설계
  부모 클래스와 자식 클래스 사이의 관계를 컴파일 시점에 결정하기 때문에 실행 시점에 객체 종류 변경이 불가능

### 합성을 사용해서 코드를 재사용하자

`합성`
- 인터페이스에 정의된 메시지를 통해서만 코드를 재사용하는 방법.

``` java
public class Movie {
    ...
    private DiscountPolicy discountPolicy;
    ...

    public Money calculateMovieFee(Screening screening) {
        return fee.minus(discountPolicy.calculateDiscountAmount(screening));
    }
}
```

`Movie`는 요금 계산을 위해 `DiscountPolicy`의 `calculateDiscountAmount()` 코드를 재사용한다.

`Movie`는 `DiscountPolicy`인터페이스를 통해 약하게 결합되어있다 (의존하고 있다).

실제로 `Movie`는 `DiscountPolicy`가 외부에 `calculateDiscountAmount()` 메서드를 제공한다는 것만 알고 내부 구현에 대해서는 전혀 모른다.


합성의 장점은 다음과 같다

- 구현을 효과적으로 캡슐화
  인터페이스에 정의된 메시지를 통해서만 재사용 가능하기 때문에.

- 유연한 설계
  의존하는 인스턴스를 교체하는것이 쉽다.

상속은 클래스를 통해 강하게 결합된다.
합성은 메시지를 통해 느슨하게 결합된다.

### 상속을 무조건 쓰지 말라는건 아니다

코드를 재사용하는 경우에는 상속보다는 합성을 선호하는 것이 옳다.

하지만 다형성을 위해 인터페이스를 재사용하는 경우에는 상속과 합성을 함께 조합해서 사용한다.

---

`객체지향 설계의 핵심`:
- 적절한 협력을 식별
- 협력에 필요한 역할을 정의
- 역할을 수행할 수 있는 적절한 객체에 적절한 책임을 할당

### 느낀점

이번 장을 읽으면서, 과거에 한 친구가 단순히 중복을 제거하는 목적으로 상속을 사용한 설계를 저에게 보여줬던 것이 생각났습니다.

당시에도 그러한 설계가 좋지 않은 설계라는걸 알고는 있었지만 모호하게 알고있었기 때문에, 그 친구에게 제 의견을 제대로 전달하지 못했던 경험을 하였습니다.

나 혼자서 아무리 유연한 코드를 짜고 객체지향에 대해 잘 안다고 해도 결국에 하나의 프로젝트는 조직원들이 다 같이 완성해 나가는 것이기 때문에, 팀원들을 설득하지 못한다면 결과물은 객체지향적이지 않은 코드가 나오고 말것입니다.

따라서 내가 배운것을 스스로 적용해 볼 수 있는 수준에서 그치지 않고 누군가에게 설명하고 설득할 수 있는 수준까지 공부하는것이 중요하다는 생각을 해보았습니다.


---

## [오브젝트] 1장 - 객체, 설계

- **URL**: https://headf1rst.github.io/log/ko/post4
- **Date**: 2022-08-22 10:00
- **Tags**: `객체지향`

### Content


`패러다임` - 한 시대의 사회 전체가 공유하는 이론 혹은 방법.
절차형 → 객체지향으로 패러다임 전환을 맞았다.

프로그래밍 패러다임은 과거의 패러다임을 폐기시키는 혁명적 패러다임이 아니라 과거의 패러다임을 개선하는 `발전적 패러다임`이다.

객체지향이 적합하지 않은 상황에서는 언제라도 다른 패러다임을 적용할 수 있는 시야와 지식을 길러야한다.

## 티켓 판매 애플리케이션

작은 소극장을 운영하고 있으며 무료로 관람할 수 있는 초대장을 발송하는 이벤트를 열었다.

- 이벤트 당첨 관람객과 일반 관람객을 구분해야한다.

- **일반 관람객**
    - 티켓을 매표소에서 구매하여 입장
- **당첨 관람객**
    - 매표소에서 초대장과 티켓을 교환하여 입장.

``` java
// 초대장 객체
public class Invitation{
    private LocalDateTime when;
}

// 티켓 객체
public class Ticket{
    private Long fee;
    
    public Long getFee(){
        return fee;
    }
}
```

관람객은 초대장, 현금, 티켓 3가지 소지품을 가방에 갖고 있다.

``` java
public class Bag{
    private Long amount;
    private Invitation invitation;
    private Ticket ticket;
    
    public boolean hasInvitation() {
        return invitaion != null;
    }
    
    public boolean hasTicket(){
        return ticket != null;
    }

    public void setTicket(Ticket ticket) {
        this.ticket = ticket;
    }

    public void minusAmount(Long amount) {
        this.amoun -= amount;
    }

    public void plusAmount(Long amount) {
        this.account += account;
    }
}
```

가방에 초대장이 있는 경우와 없는 두가지 경우가 있을 수 있기 때문에 Bag의 인스턴스 생성 시점에 해당 제약을 강제할 수 있도록 생성자를 추가한다.

``` java
public class Bag{
    public Bag(long amount){
        this(null, amount);
    }

    public Bag(Invitation invitation, long amount) {
        this.invitation = invitation;
        this.amount = amount;
    }
}
```

관람객이 가방을 갖는걸 표현하면 다음과 같다.

``` java
// 관람객 객체
public class Audience{
    private Bag bag;

    public Audience(Bag bag) {
        this.bag = bag;
    }
    
    public Bag getBag() {
        return bag;
    }
}
```

매표소에는 관람객에게 판매할 티켓과 티켓 판매 금액을 갖고 있어야 한다.

``` java
// 매표소 객체
public class TicketOffice{
    private Long amount;
    private List<Ticket> tickets = new ArrayList<>();

    public TicketOffice(Long amount, Ticket... tickets) {
        this.amount = amount;
        this.tickets.addAll(Arrays.asList(tickets));
    }
    
    public Ticket getTicket() {
        return tickets.remove(0);
    }

    public void minusAmount(Long amount) {
        this.amount -= amount;
    }

    public void plusAmount(Long amount) {
        this.amount += amount;
    }
}
```

- 판매원은 매표소에서 초대장을 티켓으로 교환해 주거나 티켓을 판매하는 역할을 수행.

- 판매원은 자신이 일하는 매표소를 알고 있어야 한다.

``` java
public class TicketSeller{
    private TicketOffice ticketOffice;

    public TicketSeller(TicketOffice ticketOffice) {
        this.ticketOffice = ticketOffice;
    }
    
    public TicketOffice getTicketOffice() {
        return ticketOffice;
    }
}
```

소극장은 관람객을 맞이하는 역할을 수행할 수 있어야 한다.

``` java
public class Theater{
    private TicketSeller ticketSeller;

    public Theater(TicketSeller ticketSeller) {
        this.ticketSeller = ticketSeller;
    }

    public void enter(Audience audience) {
        if (audience.getBag().hasInvitation()) {
            Ticket ticket = ticketSeller.getTicketOffice().getTicket();
            audience.getBag().setTicket(ticket);
        } else {
            Ticket ticket = ticketSeller.getTicketOffice().getTicket();
            audience.getBag().minusAmount(ticket.getFee());
            ticketSeller.getTicketOffice().plusAmount(ticket.getFee());
            audience.getBag().setTicket(ticket);
        }
    }
}
```

## 2. 무엇이 문제인가

### 모듈이 가져야 하는 3가지 기능
- 제대로 실행되어야 한다.
- 변경이 용이해야 한다.
- 이해하기 쉬워야 한다.

티켓 판패 애플리케이션은 제대로 실행되어야 한다는 모듈의 기능을 만족하지만 나머지 두 기능은 만족하지 못한다.

`Theater` 클래스의 `enter()` 메서드를 글로 설명하자면 다음과 같다.

- `소극장` 은 `관람객` 의 가방 을 열어서 초대장이 있는지 확인
    - 가방에 초대장이 있으면 `판매원`은 매표소의 티켓을 관람객의 가방으로 옮김.
    - 가방안에 초대장이 없으면 관람객의 가방에서 티켓 금액만큼의 현금을 꺼내 매표소에 적립
        - 매표소에 보관돼 있는 티켓을 관람객 가방으로 옮김

### 이해하기 쉬운가?

일반적으로 소극장은 관객과 소통을한다. 때문에 소극장이 관객에게 가방에 티켓이 있는지 확인하도록 메세지를 던지면, 관객은 자신이 들고 있는 가방 객체로 부터 티켓 포함 여부를 확인하는 메세지를 던져야한다.

하지만 지금 소극장 객체가 행하는 행위인 `enter()` 는 소극장이 관람객의 가방을 가져가서 안에 티켓이 있는지 확인하는 행위를 하는것과 같다.

판매원의 경우에도 마찬가지이다.
소극장이 판매자의 허락도 없이 매표소의 티켓과 현금에 마음대로 접근한다.

``` java
audience.getBag().hasInvitation();
ticketSeller.getTicketOffice().getTicket();
```

사람의 상식과 전혀 다르게 동작하는 코드는 코드를 읽는 사람에게 이해에 어려움을 준다.

뿐만 아니라 `enter()`메서드를 이해하기 위해서는 관람객이 가방을 가지고 있고 가방에는 현금과 티켓이 있어야하는 등의 정보를 코드를읽는 개발자가 모두 기억하고 있어야만 한다.

때문에 해당 코드는 코드를 작성하고 읽는 사람 모두에게 큰 부담을 주게 된다.


### 변경에 용의한가?

지금의 구조는 결합도가 너무 높다.

> 결합도 (Coupling) - > 객체 사이의 의존성을 의미.
> 객체 사이의 의존성이 너무 높을 경우 결합도가 높다고 한다.

관람객이 가방을 들고 다니지 않는 구조로 변경 되었을 경우.
`Audience` 클래스에서 `Bag`을 제거하는건 당연하고 Audience의 Bag에 직접 접근하는 `Theater`의 `enter()` 메서드도 같이 수정되어야 한다.

이는 Theater가 `관람객이 가방을 들고 있다는 사실` , `판매원이 메표소에서만 티켓을 판매한다는 사실`에 지나치게 의존해서 동작하기 때문이다.

애플리케이션 구동에 필요한 최소한의 의존성만 유지하고 불필요한 의존성을 제거하여 객체간의 결합도를 낮추는 설계를 목표로 해야한다.

## 3. 설계 개선하기

기존의 코드는 이해하기 어렵고 변경에 용이하지 않다.

이는 `Theater`가 관람객의 가방에, 판매원의 매표소에 직접 접근하기 때문이다.

이는 객체의 자유도를 해치며 행위를 규제한다.

객체가 자신의 행위를 스스로 결정할 수 있도록 자율성을 높이는 구조로 개선해야 한다.

- `Audience`가 직접 Bag를 처리.
- `TicketSeller`가 TicketOffice를 직접 처리.

- 캡슐화 (encapsulation)
  객체 내부의 세부적인 사항을 감추는 것.
    - 캡슐화의 목적은 변경하기 쉬운 객체를 만드는것.
    - 캡슐화로 객체 내부 접근을 제한하면 결합도를 낮출 수 있다.

객체들간에는 오직 **메시지**를 통해서 상호작용해야한다.
다른 객체가 어떤 객체의 내부에 대해서 알아서는 안된다.
(Theater는 Audience의 내부에 Bag 객체가 있다는 것을 알고있다. ⛔️)

객체는 상대 객체가 메시지를 이해하고 응답할 수 있는지 여부만을 알 수 있다.

- 응집도 (Cohesioin)
  자신과 연관된 작업만을 수행하고 연관없는 작업은 다른 객체에 위임하는 행위
    - 응집도를 높이기 위해 객체 스스로 자신의 데이터를 책임해야한다.
    - 객체는 전달받은 메시지를 스스로 처리하는 자율적인 존재여야 한다.

외부의 간섭을 최대한 배제하고 메시지를 통해서만 협력하도록 한다.

설계에 있어서, 객체가 어떤 데이터를 가지느냐보다는 객체에 어떤 책임을 할당할 것이냐에 초점을 맞추자.

가능한 객체의 내부에 접근하는 로직은 캡슐화를 시키자.
(public메서드를 private으로 변경)

설계는 트레이드오프의 산물이며 모든 사람들을 만족시킬 수 있는 설계를 만들 수는 없다.

OOP의 핵심은 무생물을 의인화하여 스스로 생각하고 행동하는, 능동적이고 자유로운 생명체로 인식하는 것이다. (엘리스가 경험했던 이상한 나라의 생명체들 처럼)

## 4. 객체지향 설계

설계란 코드를 배제하는것

오늘 요구하는 기능을 온전히 수행하면서 내일의 변경은 유연하게 수용해야한다.
모든 요구사항을 한번에 수집하는것은 불가능하며 개발 진행에서 요구사항 변경은 불가피하다.

따라서 변경에 유연한 코드작성은 필수적.

### 변경에 유연한 코드란?

- 이해하기 쉬운 코드
- 협력하는 객체 사이의 의존성을 적절하게 관리한 설계
- 객체간 결합도는 낮추고 응집도는 높인 코드

### 느낀점

"설계는 트레이드 오프의 산물"이라는 말이 인상깊었다.

1절에서도 "객체지향이 적합하지 않은 상황에서는 언제라도 다른 패러다임을 적용할 수 있는 시야와 지식을 길러야한다." 라는 구절이 있었는데 기술이나 패러다임은 주어진 문제를 해결해 주는 수단일 뿐이지 정답이 될 수는 없다는 생각이 들었다.

기술이나 패러다임에 매몰되는 개발자가 되지 않기 위해서 서로 피드백을 꺼리낌 없이 나눌 수 있는 동료가 될 수 있도록 소프트스킬에도 신경을 써야겠다.

### 생각해본 주제
- 디미터의 법칙
- 객체지향 생활 체조 원칙 (한 줄에 점 하나만 찍는다.)


---

## Static 변수 저장위치와 JVM 구조의 변화

- **URL**: https://headf1rst.github.io/log/ko/post2
- **Date**: 2022-07-11 10:00
- **Tags**: `Java` `JVM`
- **Summary**: Static 변수 저장위치와 JVM 구조의 변화

### Content


Static 키워드를 사용하여 정적 변수와 정적 메서드를 만들수 있는데, 이들을 정적 멤버 (혹은 클래스 멤버) 라고 합니다.

```java
class Lesson {
		static int score = 0;
		static String grade = 'F';

		static void getScore() {
			 ...
		}
}
```

Lesson 클래스(Class)는 Method Area에 생성되고, new 연산을 통해 생성한 Lesson 클래스의 객체(Object)는 Heap **영역**에 생성됩니다.

![Untitled](https://i.imgur.com/Stip2zD.png)

그렇다면 정적 멤버들은 메모리상의 어느 위치에 저장이 될까요?

정적 멤버(클래스 멤버)는 말 그대로 객체(인스턴스)에 소속된 멤버가 아니라 클래스에 고정된 멤버입니다. 

그렇기 때문에, 정적 멤버는 Class와 함께 클래스 로더에 의해서 **Method Area**에 저장되지 않을까요?

저의 이 생각은 Java 7까지는 맞지만 Java 8부터는 반만 맞는 대답이 되고 말았습니다.

왜 Java 8부터는 반만 맞는 대답인걸까요?

그 이유는 Java 7과 8의 JVM 구조에 변화가 있었기 때문입니다.

## Java 7의 HotSpot JVM 힙 그리고 static 변수

Java 7 버전까지만 해도 정적 멤버들은 Method Area를 포함하고 있는  `PermGen` 영역에 저장되었었습니다.

![스크린샷 2022-07-16 오전 11.14.55.png](https://i.imgur.com/TK8mAwL.png)

PermGen은 정적 멤버 외에도, **클래스 메타 데이터, interned String이 저장되었습니다.**

- 클래스 메타데이터 : 클래스의 이름, 생성정보, 필드정보, 메서드 정보 등

하지만 문제는 PermGen 영역이 매우 제한적인 크기를 갖고 있다는 것이었습니다.

PermGen 영역의 default 크기는, 32-bit JVM에선 64M, 64-bit에선 84M에 불과합니다.

때문에, 클래스 로딩이 많아지게 되면 사용 가능한 메모리가 부족해서 다음과 같은 에러가 발생하고는 했습니다.

```java
java.lang.OutOfMemoryError: PermGen space
```

뿐만 아니라, JVM은 PermGen 영역의 사이즈를 유지시키기 위해서 주기적으로 Garbage Collection연산을 수행하게 되는데 이는 성능 이슈를 야기했습니다.

물론 `-XX:PermSize` , `-XX:MaxPermSize` 와 같은 명령어를 통해서 사용자가 직접 PermGen영역을 설정해 주는것이 가능했습니다.

하지만 자동으로 영역의 사이즈가 늘어나지 않았고, 매모리 가용 용량을 미리 예측해서 설정하는것은 쉽지 않은 일이었습니다.

## Java 8의 Hotspot JVM 그리고 static 변수

이러한 문제를 해결하기 위해서 Java 8에선 PermGen 영역이 없어지고 `MetaSpace` 영역이 새로 생겼습니다.

Metaspace 영역은 힙이 아닌 Native 메모리 영역으로 취급됩니다.

![스크린샷 2022-07-16 오전 11.14.20.png](https://i.imgur.com/4VucRG4.png)

[https://openjdk.org/jeps/122](https://openjdk.org/jeps/122)

힙 영역은 JVM에 의해 관리되는 영역이지만 Native 메모리 영역은 OS 레벨에서 관리하는 영역입니다.

즉, Metaspace가 Native 메모리를 이용함으로써 개발자는 영역 확보의 상한을 크게 의식할 필요가 없어지게 되었습니다. (metaspace의 사이즈는 auto increase 됩니다)

`metaspace` 에서는 **클래스 메타 데이터**만을 저장하고 정적 멤버와 interned string은 힙에서 관리되게 되었습니다.

Metaspace는 클래스 메타 데이터를 native메모리에 저장하고 부족할 경우 자동으로 늘려줍니다.

덕분에 더이상 PermSize 설정을 고려할 필요가 없어졌고 MetaspaceSize, MaxMetaspaceSize가 새롭게 사용되게 되었습니다. 

만약 별도의 MetaspaceSize 설정을 하지 않으면 Native memory 자원을 최대한 사용하게 됩니다.

(Default jvm MaxMetaspaceSize = None)

## 결론

요약하자면, Java 7에서는 정적 멤버 변수가 PermGen(Method Area)에 저장되었습니다.

하지만 메모리 관리상의 문제로 인해 PermGen 영역이 사라지고 Metaspace 영역이 등장하면서, 정적 멤버 변수는 힙에 저장이 되도록 변경 되었습니다.

**참고 자료**

[JEP 122: Remove the Permanent Generation](https://openjdk.org/jeps/122)

[Where are static methods and static variables stored in Java?](https://stackoverflow.com/questions/8387989/where-are-static-methods-and-static-variables-stored-in-java)

[Java PermGen의 역사](https://blog.voidmainvoid.net/315)

[Hotspot JVM의 힙 구조](https://77loopin.github.io/java/Java-1/)

[https://www.geeksforgeeks.org/metaspace-in-java-8-with-examples/#:~:text=Method Area is a part,which leads to an OutOfMemoryError](https://www.geeksforgeeks.org/metaspace-in-java-8-with-examples/#:~:text=Method%20Area%20is%20a%20part,which%20leads%20to%20an%20OutOfMemoryError).


---

## CORS, 알고보니 우리편?

- **URL**: https://headf1rst.github.io/log/ko/post1
- **Date**: 2022-05-26 10:00
- **Tags**: `프로젝트`

### Content


Server Side Template 방식이 아닌 프론트와 백으로 나눠서 API 통신을 하는
프로젝트의 경우, 열에 아홉은 만나게 되는게 바로 `CORS` 입니다.

아니나 다를까 현재 진행중인 프로젝트에서도 CORS 관련 이슈가 올라왔습니다.

![스크린샷 2022-05-25 오전 10.08.30.png](https://i.imgur.com/KQEVRRE.png)

오늘은 CORS 정책을 해결하는 과정과 CORS란 무엇인지 알아보도록 하겠습니다.

## Cross Origin Resource Sharing Policy

개발을 막 시작하시는 많은 분들이 CORS를 그저 번거로운 오류라고 생각하시지만 사실
CORS는 말 그대로 에러나 오류가 아닌 **하나의 보안 정책**입니다.

CORS 정책을 이해하기 위해서는 아래 3가지를 이해해야만 합니다.

- Origin
- SOP
- Access-Control-Allow-Origin

### 1. Origin

`Origin` : 요청이 시작된 서버의 위치

- `Client` : http://localhost:3000
- `Server` : http://localhost:8080

로컬에서 리엑트와 스프링 서버를 실행하였을때 둘은 각각 3000과 8080포트를 사용하게 됩니다. 포트번호가 다르기
때문에 이 둘의 **Cross Origin**이며 이로 인해서 CORS 에러가 발생하게 됩니다.

두 서버 IP의 Origin이 같다면 **Same Origin,** 다르면 **Cross
Origin** 이라고 하며, 아래의 3가지가 같아야 같은 Origin입니다.

- Schema
- HOST
- Port

![스크린샷 2022-05-25 오전 11.06.46.png](https://i.imgur.com/vc7T5r5.png)

**Cross Origin 예제**

```text
A. https://localhost:80
B. http://localhost:80
```

위의 A, B는 Host, Port는 동일하나 Schema 부분이 다릅니다. 즉, Cross Origin입니다.

```text
A. https://localhost
B. https://127.0.0.1
```

언뜻보면 localhost의 IP주소가 127.0.0.1이기 때문에 Same Origin 처럼 보입니다. 하지만 위 예제는 Cross Origin입니다.
브라우저 입장에서는 String-value를 서로 비교하기 때문에 다른 출처로 판단합니다.

**Same Origin 예제**

```text
A. http://localshot:80
B. http://localhost
```

위 예제는 Same Origin 입니다.
B의 경우 포트가 생략되어있지만 http의 기본 포트가 80이기 때문에 A, B는 동일 출처가
됩니다.

### 2. SOP

SOP (Same Origin Policy)은 동일한 출처의 Origin만 리소스를 공유할 수 있도록
하는 보안 정책입니다.

바로 이 SOP 때문에 CORS 문제를 마주하게 되지만 SOP 표준 덕분에 XSS, CSRF와 같은
보안상의 이슈를 막을 수 있습니다.

### 3. Access Control Allow Origin

앞서 SOP 정책에 의해서 Same Origin의 자원만 공유가 가능하는 것을 알아보았습니다. 하지만 웹에서 다른 리소스의 공유를 할 수 없다는건 말이 안되는 일입니다.
때문에 Access Control Allow Origin으로 Cross Origin에서도 자원 공유를
가능하게 할 수 있습니다.

## 그래서 Cross Origin에서 자원 요청은 어떻게 해야할까?

1\_ HTTP 통신 헤더인 Origin 헤더에 요청을 보내는 곳의 정보를 담고 서버로 요청을
보낸다.

2\_ 이후 서버는 Access Control Allow Origin 헤더에 허용된 Origin이라는
정보를 담아서 보낸다.

3\_ 클라이언트는 헤더의 값과 비교해 정상 응답임을 확인하고 지정된 요청을 보낸다.

4\_ 서버는 요청을 수행하고 200 상태를 응답한다.

## Spring Boot 프로젝트에서 CORS 해결하기

Preflight 요청에서 적절한 Access Control을 위한 해결 방법에는 3가지가 있습니다.

- `CorsFilter` 생성하여 직접 response에 header를 넣어주기
- Controller단에서 `@CrossOrigin` 어노테이션 추가하기
- `WebMvcConfigurer` 를 상속받는 설정 파일 생성하여 설정 추가하기

CorsFilter를 생성하는 방법의 경우, 필터를 생성하고 커스텀 하는게 번거로웠고
CrossOrigin 어노테이션을 사용하는 케이스는 설정해야하는 어노테이션이 많아진다는
단점이 있었습니다.

때문에 저는 WebMvcConfigurer를 상속받는 설정 파일을 생성하여 CORS를 해결하는
방법으로 테스크를 진행하였습니다.

### Configuration으로 해결하기

![스크린샷 2022-05-25 오후 12.55.02.png](https://i.imgur.com/hlj97Tq.png)

`WebMvcConfigurer` 인터페이스를 상속받는 `WebConfig` 클래스를 생성해주었습니다.
그 다음, WebMvcConfigurer 인터페이스의 `addCorsMappings` 메서드를 구현해
주었습니다.

**registry.addMapping** 메서드

registry.addMapping 메서드를 이용해서 CORS를 적용할 URL 패턴을 정의해
주었습니다. Ant-style도 지원하지만 저는 `"/**"` 와일드 카드를 사용하였습니다.

**allowedOrigins 메서드**

allowedOrigins 메서드를 사용해서 자원 공유를 허락할 Origin을 지정하였습니다. “\*”로 모든 Origin을 허락해 주었습니다.
그 밖에도 다음과 같이 한번에 여러 Origin을 설정할 수도 있습니다.

```java
.allowedOrigins("http://localhost:8080", "http://localhost:8081");
```

**allowedMethods**

allowedMethods를 이용하면 허용할 http method를 지정할 수 있습니다. “\*”를 이용하여 모든 http method를 허용할 수 있습니다.

**maxAge**

maxAge 메서드를 이용하여 원하는 시간만큼 preFlight request를 캐싱 해 둘 수
있습니다.

**Default 값**

- allow all origins
- allow GET, HEAD, POST
- allow all headers
- Set max age to 1800 seconds (30분)

## Nginx에서의 CORS 설정

위의 CORS 관련 설정을 했음에도 또 다시 CORS 에러가 발생하여
어려움을 겪었습니다.

기존과 달라진 점이라면 CORS 이슈가 발생하는 경우도 있고 그렇지
않을 때도 있었다는 점이었습니다.

이는 서버 자체에서 받아들일 파일의 사이즈를 설정하는 nginx의
`client_max_body` 값이 너무 낮게 설정되어있었기
때문이었습니다.

다음과 같이 nginx.conf 파일 혹은 default 파일 내부에
`client_max_body` 를 추가해준 다음 nginx를 재실행하여
문제를 해결하였습니다.

![스크린샷 2022-05-25 오후 12.55.02.png](https://i.imgur.com/9lbV5wg.png)


---

## 내가 블로그를 새로 시작하는 이유

- **URL**: https://headf1rst.github.io/log/ko/post3
- **Date**: 2022-04-02 10:00
- **Tags**: `회고`

### Content


## 앵무새식 블로그 글

처음 블로그를 시작하는 경우에 `TIL (Today I Learned)` 을 목적으로 하루에 하나의 포스팅을 하는 경우를 주위에서 많이 봤고 나 또한 그랬다.

그 당시에는 하나의 포스팅을 마치고 나면 마치 해당 주제에 대해 잘 알고있는것만 같은 성취감을 느낄수 있었고 매일 새로운것을 알아가는듯이 뿌듯함을 느꼈다.

하지만, 그날 배운 내용을 바로 포스팅하기 때문에 글의 퀄리티가 좋을 수 없을 뿐더러, 나의 생각이 녹아 있는 글이 아닌, 단순히 인터넷 자료나 서적의 내용을 복붙 한 “앵무새식”으로 정리한 글들이 대부분이었다.

* 김영한님의 스프링 MVC 강의
* 자바의 정석
* 토비의 스프링
* …

당시에는 “기억보단 기록을" 이라는 말의 뜻을 단순히 “배운 내용을 모두 기록한다”라고 받아 들였고 꾸준하게 어떤 자료를 복붙해서 올리는 만행을 저지르고 말았다.

<br>

![](https://i.imgur.com/2BFLxo2.png)

물론 머리로 이해한 내용을 글로 정리함으로써 공부한 내용이 오래 기억에 남는다는건 사실이다.

하지만 이러한 글들이 장기적으로 봤을때, 개발자 본인과 개발자 생태계에 도움이 되지 않는다고 생각하며 앵무새식의 포스팅은 자칫 개발 생태계에 악영향을 끼칠수도 있다고 생각한다.

실제로 JVM의 구조에 대해 공부하면서 인터넷의 여러 포스팅을 참고한적이 있는데, 어느 포스팅에서는 JVM 7 버전에 대해서 다루고 있었고, 다른곳에선 JVM 8 버전에 대해서 다루고 있었지만 어느 한곳에서도 해당 포스트가 어떤 버전의 JVM을 다루고 있는지에 대한 내용이 전혀 없는것을 보았다.

즉, 오래된 포스팅을 보고 그대로 블로그에 옮겨 적고, 그걸 누군가 또 참고해서 블로그에 옮겨 적는 악순환이 반복되면서 양질의 자료를 찾는데 더 많은 시간을 소비하게 만들고 있다.

그리고 기존의 [내 블로그](https://headf1rst.github.io/) 글 또한 이들과 다를게 없으며 양질의 자료 수집에 어려움을 주고 있는 글일 뿐이라는 생각이 들었다.

이 밖에도 내가 스스로 느낀 복붙 형식의 포스팅의 단점은 다음과 같다.

* “학습”이 목적이 아닌 “1일 1 포스팅”에 매몰
  * 글의 퀄리티를 신경쓰지 않고 포스팅에 집착.
  * 주제를 제대로 이해하지 못하고 정리하는 등 주객전도.
  * 블로그에 업로드된 글의 수가 내 지식이라고 생각하는 오만함.
  
* 저작물에 대한 인식 저하
  * 인터넷 자료 혹은 서적을 단순 요약하는걸 당연시 여김.
  * 이미 해당 주제에 대한 포스트가 인터넷에 즐비함.

## 앞으로의 블로그 방향성

앞으로 이 블로그에는 앵무새처럼 복붙하는 글이 아닌, 내 생각이 녹아있는 글을 작성하는 것을 목표로 세웠다.

공부한 내용을 정리하는 글이더라도 단순 스펙을 나열하는 것이 아니라 “왜"를 항상 생각하며, 해당 기술이 왜 등장했고, 스펙은 어떻게 되고, 어떤 장단점이 있는지에 대한 나의 생각을 정리하고자 한다.

혼자 참고할 용도의 요약 글들은 앞으로 개인 노션이나 [깃허브](https://github.com/headF1rst/study-log)에 정리하려고 한다.  저작권에 대한 우려도 있지만, 이러한 글은 이미 검색으로 쉽게 찾을수 있기 때문이다.

비루한 나의 글이 누군가에게는 도움이나 동기부여를 줄 수 있기를 바라면서, 개발 생태계에 기여하는 기술 블로그를 운영할 수 있도록 책임감을 가지고 포스팅 해야겠다.


---


# English Posts

## Solving the Billion-Dollar Mistake: Modern Java Null Safety with JSpecify and NullAway

- **URL**: https://headf1rst.github.io/log/en/post9
- **Date**: 2025-09-24
- **Tags**: `Null Safety`
- **Summary**: Learn how to eliminate NullPointerExceptions using JSpecify annotations and NullAway static analysis in modern Java applications

### Content


Whether you're a developer just starting with Java or a senior engineer with two decades of experience, the most frequently encountered error is undoubtedly the **NullPointerException**.

![Top Crash Reasons](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v82uppsp39zvfkuuwr1m.png)

In fact, statistics show that NullPointerException is the second most common software defect, highlighting just how many developers struggle with NPEs.

[Tony Hoare](https://news.ycombinator.com/item?id=12427069) famously called it his "billion-dollar mistake."

> "I call it my billion-dollar mistake. It was the invention of the null reference in 1965. … This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years."

To improve null safety, various attempts have been made in the Java ecosystem. Following `Optional` and the `@NonNull` annotation from JSR 305, **JSpecify** has emerged as the modern standard.

This article will demonstrate how to use JSpecify to enhance your project's stability and why it's a low-overhead choice for new and existing systems alike.

## NullPointerException: Why Is It Still a Problem?

Consider this code, which calls an API to extract a token and uses the result without any null checks.

```java
// TokenExtractor.java
public interface TokenExtractor {
    String extractToken(String authorization);
}

// Main.java
TokenExtractor tokenExtractor = new DefaultTokenExtractor();
String token = tokenExtractor.extractToken("some-auth-header");
System.out.println("Token length: " + token.length()); // <-- NullPointerException!
```

This code will throw a `NullPointerException` if the `extractToken` method returns `null`. When it does, the token variable becomes null, and any attempt to access its members, like `token.length()`, triggers the exception.

The fundamental problem in Java is that nullability is **implicit**. Unless explicitly stated in API documentation, developers can't be sure whether a return value can be null, leading to misunderstandings and bugs.

## JSpecify: A Standard for Explicit Null Safety

To solve this problem, teams from Google, JetBrains, Spring, and others collaborated to create the JSpecify standard.

JSpecify is more than just a set of annotations; it provides a clear specification for null safety, ensuring consistent behavior across tools like IDEs and static analyzers.

JSpecify defines nullability in three states:
1. **Unspecified**: The default state in Java, where a value may or may not be null.
2. **Nullable (@Nullable)**: Explicitly indicates that a value can be null.
3. **Non-null (@NonNull)**: Guarantees that a value will never be null.

![JSpecify Null](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qp2g74jw6udz7f0x9nbf.png)

### Adding JSpecify Dependency

```gradle
implementation 'org.jspecify:jspecify:1.0.0'
```

Now, we can annotate the `TokenExtractor` interface to make its nullability explicit.

```java
import org.jspecify.annotations.Nullable;

public interface TokenExtractor {
  
    @Nullable // Specifies that the return value can be null
    String extractToken(String authorization);
}
```

With this change, an IDE like IntelliJ IDEA will warn you of a potential `NullPointerException` at the `token.length()` call, helping you fix the issue before it becomes a runtime error.

![IntelliJ IDE Null](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/i34w3oga4na12thlupwp.png)

## Improving Readability: Setting Defaults with `@NullMarked`

In most APIs (around 90% of the time), values are expected to be non-null. Adding `@NonNull` to every parameter and return type is tedious, clutters code, and harms readability.

To address this, JSpecify provides the `@NullMarked` annotation.

By applying `@NullMarked` at the package level (in a `package-info.java` file), all types within that package are considered non-null by default.

```java
// src/main/java/com/example/package-info.java
@NullMarked
package com.example;

import org.jspecify.annotations.NullMarked;
```

Now, everything is treated as non-null unless explicitly marked with `@Nullable`, resulting in much cleaner and more manageable code.

## Build-Time Verification: Strengthening Null Safety with NullAway

IDE warnings are helpful, but they can't prevent developers from committing code that ignores them.

To enforce null safety, you can use a static analysis tool like [NullAway](https://github.com/uber/NullAway). NullAway is an Error Prone plugin that analyzes JSpecify annotations during the build process and will cause the build to fail if it finds any null-safety violations.

You can configure it in Gradle as follows:

```gradle
plugins {
  id 'net.ltgt.errorprone' version '4.1.0'
}

dependencies {
    implementation 'org.jspecify:jspecify:1.0.0'
    
    errorprone "com.google.errorprone:error_prone_core:2.37.0"
    errorprone "com.uber.nullaway:nullaway:0.12.6"
}

tasks.withType(JavaCompile).configureEach {
    options.errorprone {
        disableAllChecks = true
        option("NullAway:OnlyNullMarked", "true")
        error("NullAway")
    }
}
```

If you ignore the IDE warning and run the build, it will fail with a compile-time error, preventing code that violates null safety from being deployed.

![build error](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/k82l826lqbmlzs5rclpu.png)

## JSpecify in the Spring Ecosystem

Starting with Spring Framework 7 (included in Spring Boot 4), the entire codebase has been migrated to use JSpecify annotations. This means Spring developers can leverage null-safety information from Spring APIs directly in their IDEs and build tools without any additional setup.

For example, the `RestClient`'s `.body()` method returns a `@Nullable String`, reminding developers to handle the possibility of a `null` response appropriately.

```java
// Spring's RestClient API
@Nullable
String body = restClient.get().uri("/user").retrieve().body(String.class);

// The IDE warns that 'body' can be null, prompting a null check.
System.out.println(body.length());
```

## The Future of Null Safety in Java

In the long term, null-safety features are planned for the Java language itself as part of Project Valhalla.

New syntax like `String?` (nullable) and `String!` (non-null) has been proposed. However, due to Java's commitment to backward compatibility, the default for unannotated types will likely remain "unspecified". [(https://openjdk.org/jeps/8303099)](https://openjdk.org/jeps/8303099)

Since this feature is still in the proposal stage and will likely take several years to materialize, JSpecify and NullAway currently represent the most practical and powerful way to improve the stability of Java applications.

## Conclusion

JSpecify and NullAway are a powerful combination for addressing Java's "billion-dollar mistake." By using explicit annotations, you clarify your code's intent and can eliminate potential NullPointerExceptions at compile time through IDE and build tool integration.

Based on my experience introducing JSpecify to my team, I've seen firsthand how a simple configuration can significantly improve application stability and code quality. A major advantage is the ability to adopt it gradually on a package-by-package basis, which makes applying it to existing projects far less daunting.

I encourage you to try JSpecify in your projects and start writing safer, more robust Java code today.


---

## PostgreSQL MVCC Internals: From xmin/xmax to Isolation Levels

- **URL**: https://headf1rst.github.io/log/en/post8
- **Date**: 2025-07-06
- **Tags**: `PostgreSQL`
- **Summary**: Deep dive into PostgreSQL's MVCC implementation, understanding xmin/xmax and how different isolation levels work internally

### Content


When multiple users access a database concurrently, how can we guarantee Isolation, one of the core `ACID` properties? While we could apply locks when reading or writing data, this approach is inefficient because other users must wait for lock to be released.

To address this challenge, modern RDBMSs like PostgreSQL and MySQL (InnoDB) employ a technique called **MVCC (Multi-Version Concurrency Control)**.

The core idea of MVCC is to create a new version of a row each time it's modified, instead of overwriting existing data. This method controls concurrency by storing version information for each row, tracking which transaction created or deleted it.

While various database vendors share this core concept of MVCC, their implementations differ. Let's explore these differences by examining the MVCC methods of two of the most popular databases: PostgreSQL and MySQL.

## How PostgreSQL Implements MVCC

In PostgreSQL, each row in a table (internally called a 'tuple') contains several system columns that are not directly visible to the user. The key columns for transaction control are xmin and xmax.

- `xmin`: The transaction ID that created this row version.
- `xmax`: The transaction ID that deleted this row version.

Using these two columns, PostgreSQL operates as follows:

- **INSERT**: A new row is created, and the current transaction's ID is recorded in the xmin column. The xmax column remains null.

- **DELETE**: Instead of being physically removed immediately, the row's xmax column is updated with the current transaction's ID. This row is now considered a 'dead tuple'.

- **UPDATE**: This operates as a combination of a DELETE and an INSERT. First, the xmax of the current row is marked with the transaction ID. Then, a new row with updated values is inserted, and its xmin is set to the current transaction ID.

For a specific row to be visible to a transaction, it must satisfy both of the following visibility rules:

- **Creation Rule (xmin)**: The transaction that created the row version (xmin) must have been committed before the current transaction's snapshot was taken.

- **Deletion Rule (xmax)**: If the row version was deleted, the deleting transaction (xmax) must not have been committed before the current transaction's snapshot was taken.

## How MySQL (InnoDB) Implements MVCC

InnoDB, MySQL's default storage engine, also employs MVCC, but it manages old versions in a distinct way.

- **Core Metadata**: Each row contains important hidden columns, including `DB_TRX_ID` (the transaction ID that created the version, similar to xmin) and `DB_ROLL_PTR` (**the rollback pointer**).

- **Previous Version Storage**: Unlike PostgreSQL, which keeps old versions within the table itself, InnoDB stores before-images of data in a separate area called the **Undo Log**. The `DB_ROLL_PTR` is an address that points to the previous version of the data in this Undo Log.

- **Operation**: When a transaction encounters a data version that is newer than its snapshot, it follows the `DB_ROLL_PTR` to traverse the Undo Log, **reconstructing** a version of the data that is visible to it.

## Visibility in PostgreSQL by Isolation Level

Ultimately, visibility in PostgreSQL is determined by `xmin`, `xmax`, and a **snapshot**. The differences between isolation levels arise from when this snapshot is created.

Let's explain the visibility rules from the perspective of a transaction, `Tx A`, using a `users` table that contains two rows.

![mvcc1](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/38e7s46prs1j3dwdavyd.png)

### READ COMMITTED (Default)

In `READ COMMITTED` level, a **new snapshot is created for every SQL statement**. This means changes committed by other transactions become visible immediately. (`Non-Repeatable Read`)

#### 1. `Tx A` starts (TXID: 100) and executes its first `SELECT`.

A new snapshot, Snapshot_1, is created for this statement, capturing the current state of the database.

> Snapshot_1 Contents: {'Committed TXs': {90, 91}, 'In-Progress TXs': {100}}

Visibility Check (based on `Snapshot_1`):

- 'Alice' row (`xmin=90`): xmin is in the committed list. -> **Visible**.

- 'Bob' row (`xmin=91`): xmin is in the committed list. -> **Visible**.

Result for Tx A: It sees two records: 'Alice' and 'Bob'.

#### 2. Tx B modifies data and commits.

The original 'Alice' row is updated with `xmax=101`, and a new 'Alicia' row is created with `xmin=101`.

The database's global list of committed transactions is updated to `{90, 91, 101}`.

![mvcc2](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dv4t554i4ghzazoz7wdp.png)

#### 3. Tx A executes its second SELECT within the same transaction.

According to the `READ COMMITTED` rule, a **completely new** Snapshot_2 is created for this `SELECT` statement.

> Snapshot_2 Contents: {'Committed TXs': {90, 91, 101}, 'In-Progress TXs': {100}}

Visibility Check (based on `Snapshot_2`):

- Original 'Alice' row (`xmax=101`): `xmax` is in the committed list. -> **Not visible**.

- New 'Alicia' row (`xmin=101`): `xmin` is in the committed list. -> **Visible**.

- 'Bob' row (`xmin=91`): `xmin` is in the committed list. -> **Visible**.

Final Result for `Tx A`: It now sees two records: 'Alicia' and 'Bob'.

![mvcc3](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/slm5bd3c50nl5b05o9rg.png)

`Tx A` executed the same query twice within the same transaction but received different results. This phenomenon, known as a **Non-Repeatable Read**, is the expected behavior for the `READ COMMITTED` isolation level.

### REPEATABLE READ / SERIALIZABLE

In `REPEATABLE READ`, a **snapshot is created only once**, when the first SQL statement in the transaction is executed. This same snapshot is then reused until the transaction ends.

Because of this, even if `Tx B` changes data and commits, `Tx A` continues to judge visibility based on its old, fixed snapshot and therefore does not see the changes. This provides a consistent view of the data throughout the transaction, preventing Non-Repeatable Reads. The `SERIALIZABLE` level uses the same snapshot policy but adds more sophisticated dependency tracking to prevent all concurrency anomalies.

## Why Different Methods? (Design Philosophy Trade-offs)

The different MVCC implementations in PostgreSQL and MySQL reflect trade-offs in their design philosophies.

- **PostgreSQL**: This approach is optimized for **read performance**. Because previous versions are stored in the table itself, there is no overhead from reconstructing rows, which makes reads very fast. However, since 'dead tuples' accumulate within the table, a periodic cleanup process called `VACUUM` is essential. Without it, the table can suffer from 'bloat', becoming unnecessarily large.

- **MySQL (InnoDB)**: This approach prioritizes **table space efficiency**. Change history is managed in a separate Undo Log, so the table itself remains clean and contains only the latest data. However, reading past data may incur the additional cost of traversing the Undo Log to reconstruct a row version, and the Undo Log space itself requires management.

Both approaches have their pros and cons. Neither is absolutely superior; rather, they represent different choices made to achieve the distinct goals of each system.


---

## Prompt Engineering Tips from Anthropic Engineers

- **URL**: https://headf1rst.github.io/log/en/post7
- **Date**: 2025-06-24
- **Tags**: `Prompt Engineering` `AI` `Anthropic` `LLM`
- **Summary**: Valuable prompt engineering tips and insights from Anthropic engineers

### Content


Here are a few impressive points from [youtube (AI prompt engineering: A deep dive)](https://www.youtube.com/watch?v=T9aRN5JkmL8&t=2463s) where Anthropic engineers shared their prompt writing tips and experiences.

## Impressive Prompt Writing Tips

* When the model makes a mistake, it can be helpful to ask why it was wrong and directly ask the model how to instruct it next time to avoid the mistake.
* When the model faces unexpected input or ambiguous situations, you should provide clear instructions on what to do (e.g., output an "UNCERTAIN" tag) to prevent it from giving a wrong answer.
* Inducing the model to explain its reasoning process before providing an answer (Chain of Thought) improves the model's accuracy.
* You can ask the model to "interview" you. When it's difficult to clearly grasp what you want, you can get help constructing a prompt by asking the model to ask you questions and elicit the necessary information.

## Tips for Improving Prompt Engineering Skills

* **Read many other people's prompts and model outputs:** You can learn a lot by looking at great prompts, analyzing their structure and intent, and by carefully observing the model's outputs.
* **Attempt many conversations with the model:** You need to practice improving prompts by directly and repeatedly communicating with the model.
* **Show your prompts to others:** Showing a prompt you've written to someone without prior knowledge of the task can help you discover parts that are not clear.
* **Test the model's limits:** Trying to do the most difficult tasks you think the model cannot do and exploring its limits leads to the greatest learning.


---

## From Theory to Practice: A JMH Showdown Between Sequential and Parallel Streams

- **URL**: https://headf1rst.github.io/log/en/post6
- **Date**: 2025-06-15
- **Tags**: `Java` `JMH`
- **Summary**: Practical performance comparison between sequential and parallel streams using JMH benchmarking

### Content


In our [previous post](https://dev.to/headf1rst/discover-how-forkjoinpool-powers-javas-high-performance-parallel-processing-3pn7), we delved into the mechanics of ForkJoinPool, the powerful engine behind Java's parallel processing capabilities. Theory is essential, but seeing the real-world impact is what truly matters. Now, it's time to put that theory to the test.

This post presents a practical performance comparison between traditional sequential processing and parallel processing using `parallelStream`. We'll use the **Java Microbenchmark Harness (JMH)** to precisely measure the performance gains achieved when validating a large set of data (transportation plans in our example).

## The Benchmark: Setting the Stage

This section covers all the details required to understand how the test was designed, configured, and implemented.

### Test Environment and JMH Configuration

To ensure a fair and reliable comparison, we established a controlled test environment using JMH. The objective is to measure the average execution time of the `TransportPlanExcelUploadValidator`'s validation logic.

- **Test Data**: A list containing 1,000 to 10,000 transport plan entries.
- **Benchmark Tool**: JMH (Java Microbenchmark Harness).
- **Warmup Iterations**: 5 rounds to allow the JVM to perform initial optimizations.
- **Measurement Iterations**: 10 rounds to capture a stable average execution time.
- **Forks**: 1 fork to run the test in a separate process, ensuring isolation.

It's important to briefly discuss why the **warmup iterations** are critical for accurate results on the JVM. When Java code is first run, the JVM may interpret it or use a baseline compiler. Only after code has been executed multiple times (making it "hot"), does the Just-In-Time (JIT) compiler step in to perform significant optimizations, translating bytecode into highly efficient native code. The warmup phase ensures that this JIT compilation and other optimizations have completed, so that our measurements reflect the true performance of the optimized code, not the initial, slower startup phase.

### The Benchmark Code

The core of our benchmark lies in the `TransportPlanExcelUploadValidatorBenchmark` class. We've defined two separate methods, each annotated with `@Benchmark`, to measure the performance of the sequential and parallel approaches.

```java
@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.MILLISECONDS)
@State(Scope.Benchmark)
public class TransportPlanExcelUploadValidatorBenchmark {

    private ParallelValidator parallelValidator;
    private SequentialValidator sequentialValidator;

    private LocalDate transportDate;
    private List<TransportPlanExcelUploadInfo> uploadPlans;
    private Set<TransportPlan> registeredPlans;
    private Map<String, Route> routes;

    @Setup
    public void setup() {
        // Prepare test data
        ManagerService managerService = mock(ManagerService.class);
        parallelValidator = new ParallelValidator(managerService);
        sequentialValidator = new SequentialValidator(managerService);

        // Setup Fixtures...

        // Mock manager service methods
        given(managerService.findAllByFullCarNumberAndUsableIsTrue(anySet()))
                .willReturn(Collections.emptyList());

        // Generate test data
        uploadPlans = IntStream.range(0, 1_000)
                .mapToObj(i -> {
                    DawnMiddleMileTransportPlanInfo info = new DawnMiddleMileTransportPlanInfo();
                    info.setTransportDate(transportDate);
                    info.setDeparture("Test Departure");
                    info.setDestination("Test Destination");
                    info.setExpectedEntryTime(String.format("%02d:00", (i % 24)));
                    info.setCarNumber("12가" + (3000 + i));
                    info.setDeliveryRound(1);
                    info.setRowNumber(i + 1);
                    return info;
                })
                .collect(Collectors.toList());
    }

    @Benchmark
    public void sequentialValidation(Blackhole blackhole) {
        var result = sequentialValidator.validateExcelUpload(transportDate, uploadPlans, registeredPlans, routes);
        blackhole.consume(result);
    }

    @Benchmark
    public void parallelValidation(Blackhole blackhole) {
        var result = parallelValidator.validateExcelUpload(transportDate, uploadPlans, registeredPlans, routes);
        blackhole.consume(result);
    }
}
```

### Implementation: Sequential vs. Parallel Validators

Here is a brief look at the two different validator implementations being tested. The core difference lies in how they iterate over the data and collect results.

The `SequentialValidator` uses a standard `for` loop. It iterates through the list of plans one by one, validates each item in a single thread, and adds the results directly to standard `ArrayLists`. This represents a traditional, single-threaded approach.

The `ParallelValidator`, in contrast, leverages `parallelStream().forEach()` to process the plans concurrently. To safely collect results from multiple threads without causing race conditions, it uses thread-safe `ConcurrentLinkedQueue` collections. Once the parallel processing is complete, these queues are drained into `ArrayLists` to form the final result.

#### ParallelValidator.java

```java
@Service
@RequiredArgsConstructor
public class ParallelValidator {

    private final ManagerService managerService;

    public TransportPlanUploadValidationResult validateExcelUpload(...) {
        // ...
        ConcurrentLinkedQueue<TransportPlanExcelUploadInfo> validInfosQueue = new ConcurrentLinkedQueue<>();
        ConcurrentLinkedQueue<ValidationFailure> failuresQueue = new ConcurrentLinkedQueue<>();

        sortedUploadPlans.parallelStream().forEach(uploadPlan -> {
            List<String> failedReasons = validateExcelUploadRow(uploadPlan, validationContext);

            if (failedReasons.isEmpty()) {
                validInfosQueue.add(uploadPlan);
            } else {
                failuresQueue.add(ValidationFailure.of(uploadPlan.getRowNumber(), failedReasons));
            }
        });

        List<TransportPlanExcelUploadInfo> validInfos = new ArrayList<>(validInfosQueue);
        List<ValidationFailure> failures = new ArrayList<>(failuresQueue);

        return TransportPlanUploadValidationResult.of(validInfos, failures);
    }
    // ...
}
```

#### SequentialValidator.java

```java
@Service
@RequiredArgsConstructor
public class SequentialValidator {

    private final ManagerService managerService;

    public TransportPlanUploadValidationResult validateExcelUpload(...) {
        List<TransportPlanExcelUploadInfo> validInfos = new ArrayList<>();
        List<ValidationFailure> failures = new ArrayList<>();
        // ...
        for (TransportPlanExcelUploadInfo uploadPlan : sortedUploadPlans) {
            List<String> failedReasons = validateExcelUploadRow(uploadPlan, validationContext);

            if (!failedReasons.isEmpty()) {
                failures.add(ValidationFailure.of(uploadPlan.getRowNumber(), failedReasons));
            } else {
                validInfos.add(uploadPlan);
            }
        }

        return TransportPlanUploadValidationResult.of(validInfos, failures);
    }
    // ...
}
```

## Running the Benchmark

The benchmark was executed using the following main class, which configures and runs the JMH Runner.

```java
public class RunBenchmark {

    public static void main(String[] args) throws RunnerException {

        Options accurateOptions = new OptionsBuilder()
                .include(TransportPlanExcelUploadValidatorBenchmark.class.getSimpleName())
                .forks(1)  // Use 1 JVM forks
                .warmupIterations(5)  // 5 warmup iterations
                .warmupTime(TimeValue.seconds(1))  // 1 seconds per warmup iteration
                .measurementIterations(10)
                .measurementTime(TimeValue.seconds(2))  // 2 seconds per measurement iteration
                .resultFormat(ResultFormatType.JSON)
                .result("jmh-results.json")
                .build();

        int availableProcessors = Runtime.getRuntime().availableProcessors();
        System.out.println("Benchmark started: Sequential vs Parallel processing performance comparison");
        System.out.println("Data size: 1,000 transport plans");
        System.out.println("Available processors (cores): " + availableProcessors);
        System.out.println("---------------------------------------------");

        new Runner(accurateOptions).run();
    }
}
```

## The Results: Quantifying the Performance Gain

Now, let's examine the findings from our benchmark tests.

### Case 1: A 1,000-Item Dataset

![JMH Benchmark Result](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1knf86i6r6wpjdcwetnd.png)

First, let's look at the results for a dataset of 1,000 items. The `parallelValidation` benchmark scored **6.392 ms/op**, while `sequentialValidation` came in at **20.359 ms/op**.

![JMH Benchmark Result Comp](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iplv0k3c8bbolw75lyow.png)

This translates to a **68.60%** performance improvement, a significant gain even for a moderately sized dataset.

### Case 2: A 10,000-Item Dataset

![JMH Benchmark Result](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vklyrxhsw9uleaulo444.png)

When we increased the load to 10,000 items, the advantage of parallel processing became even more stark. The parallel version clocked in at **321.574 ms/op** compared to the sequential version's **1917.098 ms/op**. The performance improvement jumped to an impressive **83.23%**. This demonstrates that the benefits of `parallelStream` become more pronounced as the data volume grows and the initial thread management overhead becomes less significant relative to the total work.

![JMH Benchmark Result Comp](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iy8u1ndqxv49iommydw9.png)

## Visualizing the Results

To make the results easier to interpret, the output JSON from JMH can be uploaded to the [JMH Visualizer](https://jmh.morethan.io/). This free tool generates clear charts that compare the performance scores, making it easy to see the difference at a glance.

![JMH Visualizer](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n0ku0mzifrr3g4o74mnf.png)

## Conclusion

Before applying parallelStream across your application, always consider the following:

- **Parallelism is Not a Silver Bullet**. Unconditional parallel processing is not always better. For small datasets or very simple tasks, the overhead costs can lead to performance degradation. You must consider if the cost of task splitting, merging, and Fork/Join scheduling is more expensive than the actual operation (like a simple string comparison).

- **Benchmarking is Essential**. The success in this scenario does not guarantee success in others. The only way to know for sure if a change yields a performance benefit is to test it in a controlled environment.

This leads to the most critical takeaway: **Measure, Don't Assume**. While theory provides a strong foundation, only empirical data from a benchmark can confirm whether a change is a true optimization for your specific use case.


---

## Discover how ForkJoinPool powers Java's high-performance parallel processing

- **URL**: https://headf1rst.github.io/log/en/post5
- **Date**: 2025-06-03
- **Tags**: `ForkJoinPool`
- **Summary**: Understanding ForkJoinPool and how it powers Java's parallel stream processing

### Content


Recently, I optimized our transport operation plan Excel upload feature, boosting performance for logistics system administrators. This tool allows them to upload weekly transportation schedules that our system processes and registers. The upload includes critical validation checks: verifying vehicle numbers, preventing duplicate plans, and identifying scheduling conflicts. By implementing parallel processing for these validations, processing time significantly reduced, enabling logistics managers to finalize transportation plans more efficiently.

To achieve this, I leveraged **parallel stream**, a powerful feature introduced in Java 8. While the specific performance metrics and a deep dive into parallel stream with JMH will be covered in a future post, this article focuses on **ForkJoinPool**, the engine that parallel stream uses internally to work its parallel processing.

---

## What is ForkJoinPool?

At its core, ForkJoinPool is a specialized ThreadPoolExecutor designed to efficiently run a large number of tasks using a pool of worker threads. It's built around the **divide-and-conquer** principle. Large tasks are recursively broken down into smaller, more manageable subtasks (the "fork" step). These subtasks are then processed independently by different threads. Once these smaller tasks complete, their results are progressively combined (the "join" step) to produce the final result of the original large task.

![Divide and conquer](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/o3sowil84p2wqkkzf61i.png)

This approach is particularly well-suited for CPU-intensive operations where work can be easily parallelized.

---

## The Work-Stealing Algorithm: Keeping Threads Busy

One of the standout features of ForkJoinPool is its work-stealing algorithm, which significantly boosts efficiency. Here's how it works:

- Each worker thread in the ForkJoinPool maintains its own deque (double-ended queue) of tasks assigned to it.
- A thread primarily processes tasks from the head of its own deque.
- If a thread finishes all its tasks and becomes idle, it doesn't just sit there. Instead, it looks at the deques of other busy threads and "steals" a task from the tail of their deque.

![Deque per thread](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/w3au9s0uv901zein29zq.png)

This work-stealing mechanism ensures that threads remain busy as long as there's work to be done anywhere in the pool. It provides excellent load balancing, maximizes CPU utilization, and helps improve overall throughput, especially for tasks with unpredictable execution times.

![Work-Stealing Algorithm](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qoyjn9y9gxn8u2k3cuuf.png)

---

## ParallelStream and CommonPool

When you use `parallelStream()` in Java 8 and later, you're implicitly using a ForkJoinPool. Specifically, parallelStream operations are executed on the static `ForkJoinPool.commonPool()`.

The commonPool() is a JVM-managed, globally available instance of ForkJoinPool. It's convenient because you don't need to manually create, configure, or shut down the pool; the JVM handles its lifecycle. This makes it very easy to introduce parallelism into your applications.

By default, the number of threads in the commonPool() (its parallelism level) is typically set to the number of available processor cores - 1.
(`Runtime.getRuntime().availableProcessors() - 1`). 

The "minus one" is to leave a core for the main application thread or other non-ForkJoinPool tasks. However, if the system has only a single processor, the commonPool()'s parallelism will be 1.

---

## CommonPool is a Shared Resource

It's crucial to note that the commonPool() is a shared resource throughout your application. Not only parallelStream but also `CompletableFuture`'s asynchronous methods (like `supplyAsync()` and `runAsync(Runnable)`) use the commonPool() by default if no specific Executor is provided.

Because it's shared, you need to be careful about the types of tasks you submit to it. ForkJoinPool is optimized for CPU-bound tasks – computations that keep the CPU busy. If you use the commonPool() for long-running I/O-bound tasks (e.g., network requests, database queries, file operations) where threads might block and wait for extended periods, you risk starving the pool. 

If all threads in the commonPool() become blocked on I/O operations, other parts of your application relying on it (including other parallelStream operations or CompletableFutures) will be unable to get processing time, leading to severe performance degradation or even deadlocks.

For I/O-bound tasks, it's generally better to use a separate, dedicated ExecutorService (like a cached thread pool or a fixed-size thread pool configured for the expected number of blocking tasks) to avoid monopolizing the commonPool().

---

## Conclusion

In essence, ForkJoinPool provides a powerful and efficient framework for parallel task execution, especially for computational workloads. Understanding its mechanics, particularly the work-stealing algorithm and its use in the commonPool(), can help you write more performant and scalable Java applications.


---

## Why @Transactional Sometimes Fails: A Deep Dive into Spring AOP Proxies

- **URL**: https://headf1rst.github.io/log/en/post4
- **Date**: 2025-06-01
- **Tags**: `AOP` `Transactional`
- **Summary**: Understanding Spring AOP proxies and why @Transactional sometimes fails with internal method calls

### Content


## How @Transactional Works

### Transaction Management via AOP and the Proxy Pattern

AOP (Aspect-Oriented Programming) is a programming paradigm that complements Object-Oriented Programming (OOP) by addressing its limitations in enterprise application development. It allows developers to separate and modularize cross-cutting concerns like transactions, caching, and logging, enabling them to focus on business logic.

Spring supports AOP using the Proxy Pattern. A proxy object intercepts method calls to the real object, applying common functionalities before and after the business logic executes.

Similarly, for transaction management, when a method annotated with `@Transactional` is called, a proxy object handles the transaction's start and end (commit/rollback) before and after the method execution. So, how is this proxy object created?

---

## Two Proxy Creation Methods in Spring AOP

Spring offers two ways to create proxy objects: **JDK Dynamic Proxy** and **CGLIB Proxy**.

**1. JDK Dynamic Proxy**

This method uses JDK's `java.lang.reflect.Proxy` class to dynamically create proxy objects at runtime that implement interfaces. The target object must implement at least one interface, and the proxy object is created based on the abstract methods of that interface.

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kp5rfzww1urar2lbcfyv.png)

Interface-based proxy objects intercept method calls via an `InvocationHandler` to process additional functionalities.
For example, if logging is added in addition to transaction handling, another necessary proxy object is created, and methods are called in the following order:

> ProxyLogging -> ProxyTransaction -> Target

**2. CGLIB Proxy (Default Method)**

CGLIB (Code Generation Library) is a library that can dynamically generate classes at runtime by manipulating bytecode. Unlike JDK Dynamic Proxy, it creates proxy objects by subclassing the target object. This means a proxy can be created even if the target object doesn't implement an interface. The proxy object overrides the target object's methods to intercept calls and process additional functionalities before and after method execution.

Since Spring Boot 2.x, **CGLIB proxy is the default method** for creating AOP proxies, including for `@Transactional` behaviour, regardless of whether the target object implements an interface.

While this proxy-based AOP mechanism is very powerful, it also creates certain scenarios that require caution. One common point of confusion for developers is method calls within the same class, i.e., self-invocation scenarios. With this understanding of how @Transactional operates through proxies, let's now delve into why transactions might not be applied in such self-invocation cases and the reasons behind it.

---

## Internal Method Calls and AOP Proxies

Now that we understand the principle behind `@Transactional`, let's examine why transactions are not applied when an internal method (a method within the same class) is called, and the reasons for this behavior.

```java
@Service
@RequiredArgsConstructor
public class TransportService {

    private final TransportRepository transportRepository;

    public void sendTransportEvents(List<Long> transportIds) {
        transportRepository.changeStatuses(transportIds);
        // Sending...
    }
}

@Repository
public interface TransportRepository {
    void save(Transport transport);

    default void changeStatuses(List<Long> transportIds) {
        for (Long transportId : transportIds) {
            changeStatus(transportId); // Internal call within the proxy if TransportRepositoryImpl is proxied
        }
    }

    void changeStatus(Long transportId);

    Optional<Transport> findById(Long id);
}

@Repository
@RequiredArgsConstructor
public class TransportRepositoryImpl implements TransportRepository {

    private final TransportJpaRepository transportJpaRepository;

    @Override
    public void save(Transport transport) {
        transportJpaRepository.save(transport);
    }

    @Transactional(propagation = Propagation.REQUIRES_NEW)
    @Override
    public void changeStatus(Long transportId) {
        Transport transport = transportJpaRepository.findById(transportId).orElseThrow();
        transport.updateStatus(TransportStatus.SENT);
    }

    @Override
    public Optional<Transport> findById(Long id) {
        return transportJpaRepository.findById(id);
    }
}

@SpringBootTest
class TransportServiceTest {

    @Autowired
    private TransportService sut;
    @Autowired
    private TransportRepository transportRepository; // This will be a proxy

    @Test
    void whenTransportEventsAreSent_statusChangesToSENT() {
        // given
        var transportA = new Transport(1L, TransportStatus.PENDING);
        var transportB = new Transport(2L, TransportStatus.PENDING);
        List.of(transportA, transportB).forEach(transportRepository::save);

        // when
        sut.sendTransportEvents(List.of(1L, 2L));

        // then
        var result1 = transportRepository.findById(1L).get();
        var result2 = transportRepository.findById(2L).get();

        assertThat(result1.getStatus()).isEqualTo(TransportStatus.SENT);
        assertThat(result2.getStatus()).isEqualTo(TransportStatus.SENT);
    }
}
```

In `TransportRepositoryImpl.changeStatus(transportId)`, we attempt to change the `transport's` status from PENDING to `SENT` using JPA's dirty checking. If dirty checking worked as intended, the test should pass. However, the test fails, and the status remains `PENDING`.

### Why Does This Happen?

```java
@Repository
@RequiredArgsConstructor
@Slf4j
public class TransportRepositoryImpl implements TransportRepository {

    private final TransportJpaRepository transportJpaRepository;
    private final EntityManager em;
    // ...

    @Transactional(propagation = Propagation.REQUIRES_NEW)
    @Override
    public void changeStatus(Long transportId) {
        Transport transport = transportJpaRepository.findById(transportId).orElseThrow();
        transport.updateStatus(TransportStatus.SENT);

        if (em.contains(transport)) {
            log.info("Entity is managed");
        } else {
            log.info("Entity is detached"); // This will be logged
        }
        log.info("Transaction active: {}", TransactionSynchronizationManager.isActualTransactionActive()); // false
    }
}
```

For dirty checking to work correctly, the entity must be managed by the persistence context. 

However, the logs would show that the `Transport` entity is not managed by the persistence context (Entity is detached) and that no transaction is active (Transaction active: false). This is because the transaction was not applied to this method call.

![persistence](https://i.imgur.com/BANQI7X.png)

The lifecycle of the persistence context is tied to the transaction. If a transaction is not active, a persistence context is not created (or an existing one isn't joined), and thus, the entity is not managed within it. Consequently, in the `changeStatus()` method where no transaction is applied during this specific invocation path, a persistence context isn't effectively used for dirty checking.

Spring creates a proxy object for `TransportRepositoryImpl` (likely using CGLIB by default). This proxy object overrides the `changeStatus()` method (which is annotated with @Transactional) to inject transaction-related logic before and after the actual method call.

The problem arises because the `TransportRepository's` default method `changeStatuses()` calls `changeStatus()` through the this reference of the proxy. However, when `changeStatuses()` (which itself is not @Transactional on the interface proxy) iterates and calls `changeStatus(transportId)`, this call is an internal invocation within the target `TransportRepositoryImpl` object if the proxy delegates to it without re-intercepting.

More precisely:
When `transportRepository.changeStatuses()` is called on the proxy:

1. The proxy might invoke the default method `changeStatuses()` on itself.
2. Inside `changeStatuses()`, the call `changeStatus(transportId)` is effectively `this.changeStatus(transportId)`. If `this` refers to the raw `TransportRepositoryImpl` instance or if the proxy doesn't re-intercept calls from within itself to itself, the proxy's transactional advice for `changeStatus()` is bypassed.

Spring proxies can only apply transactional (and other AOP) advice to calls that go through the proxy from an external caller. Methods called from within the same class instance (self-invocation) typically bypass the proxy mechanism, so transactions are not applied to the inner call.

---

## Why Do Internal Calls Bypass the Proxy?

In Spring AOP, proxies operate when called by an external client (another bean or component). When an external client calls a method on a bean that is proxied, it's actually calling a method on the proxy object. The proxy then intercepts this call, applies any configured AOP advice (like starting a transaction), and then delegates to the actual method on the target object.

However, when a method is called from within the same class instance (e.g., `this.someOtherMethod()`), it's not an external call being made through the proxy. Instead, the method is invoked directly on the current instance's `this` reference, which refers to the raw target object, not the proxy. Therefore, the proxy's AOP advice (including transactional behavior) is bypassed.

To resolve this:

1. It's recommended to apply @Transactional to methods that are called externally (e.g., directly from the `TransportService` to `transportRepository.changeStatus(id)` for each ID).

2. Alternatively, you can separate the internal method that needs its own transaction into a different Spring bean so it can be called externally (i.e., injected and then called, ensuring the call goes through its proxy).

In the given example, the most straightforward fix to ensure `changeStatus` is transactional would be to call it directly from `TransportService` in a loop, or refactor `TransportRepository` so changeStatuses itself is @Transactional (if that's the desired transactional boundary) and handles the logic appropriately, or to make `changeStatuses` a method on `TransportRepositoryImpl` directly and have `TransportService` call it. The current default method on the interface makes the proxy interaction complex.

---

## Conclusion

To summarize, Spring's @Transactional annotation relies on AOP proxies (typically CGLIB in modern Spring Boot) to manage transactions by intercepting external method calls. A critical takeaway is that @Transactional may not apply to self-invocations because these internal calls bypass the proxy mechanism.

It's important to understand that this self-invocation challenge and the underlying proxy behavior are not exclusive to @Transactional. This is a fundamental aspect of how Spring AOP functions. Consequently, other AOP-driven features, such as declarative caching (@Cacheable), method security (@Secured, @PreAuthorize), or any custom aspects you implement, can be similarly affected by internal calls bypassing the proxy.

Therefore, a solid understanding of AOP's core working principles—especially how proxies intercept external calls and why self-invocations are not subject to this interception—is paramount. This knowledge is key not just for mastering @Transactional, but for effectively utilizing the full spectrum of Spring's powerful AOP capabilities, ultimately leading to more robust and predictable applications.


---

## Java ClassLoaders: How the JVM Dynamically Loads & Executes Your Code

- **URL**: https://headf1rst.github.io/log/en/post3
- **Date**: 2025-05-27
- **Tags**: `Java`
- **Summary**: Deep dive into Java ClassLoaders and JVM's dynamic class loading mechanisms

### Content


Java's "Write Once, Run Anywhere" principle is foundational to its sustained popularity. This portability is powered by the Java Virtual Machine (JVM), specifically its sophisticated system of ClassLoaders and dynamic class-loading mechanisms. Understanding these components can greatly improve your insight into Java's runtime behaviour and performance characteristics.

Let's explore step-by-step how Java transforms your code into executable functionality.

## What are Java ClassLoaders?

A Java ClassLoader is responsible for dynamically loading Java classes into the JVM at runtime. When the JVM requires a class, it's the ClassLoader's task to locate the class definition (typically a `.class` file) and load it into memory.

Java uses a hierarchical ClassLoader structure composed of three built-in loaders:

- **Bootstrap ClassLoader:**
  - The root ClassLoader, implemented in native code.
  - Loads Java's core API classes from `JAVA_HOME/lib` (e.g., `rt.jar`, `tools.jar`).

- **Platform ClassLoader** (formerly Extension ClassLoader):
  - Loads extension classes from `JAVA_HOME/lib/ext` or directories specified by the `java.ext.dirs` property.
  - Typically handles classes prefixed with `javax.*`.

- **Application ClassLoader** (System ClassLoader):
  - Loads classes specified by the application's classpath (`CLASSPATH` or `-cp` option).
  - Handles application-level classes and JARs.

### The ClassLoader Hierarchy & Parent Delegation Model

Java ClassLoaders operate according to the **Parent Delegation Model**. Under this model, class loading requests are first delegated up to the parent ClassLoader. Only when all parent loaders fail to find the class does the current loader attempt to load it.

The hierarchy looks like this:

```
Bootstrap ClassLoader
      ↑ delegates to
Platform ClassLoader
      ↑ delegates to
Application ClassLoader
```

**Advantages of Parent Delegation:**

- **Prevents Redundant Loading**: Ensures classes are loaded just once.
- **Maintains Consistency**: Core classes like `java.lang.Object` are uniformly loaded by the Bootstrap loader.
- **Security**: Protects core Java classes from malicious overrides by lower-level loaders.

## The JVM Class Loading Process

Java's class loading consists of three main phases:

![JVM Class Loading Process](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bytytkwrigvqf70hj997.png)

### 1. Loading

* The ClassLoader reads the class bytecode into memory.
* Parses class metadata (name, superclass, interfaces, methods, fields).
* Stores metadata in the JVM Method Area.
* Creates a corresponding `java.lang.Class` instance in heap memory.
* Classes are loaded dynamically, triggered upon their first usage (object instantiation, static method invocation, or static field access).

### 2. Linking

This prepares a loaded class for execution, divided into three sub-steps:

* **Verification**:
    * Ensures bytecode integrity, adherence to JVM specifications.
    * Checks class structure, inheritance rules, interface implementation, bytecode validity, and symbolic reference correctness.
* **Preparation**:
    * Allocates memory for static fields and assigns default values (e.g., numeric fields to `0`, object references to `null`).
* **Resolution**:
    * Converts symbolic references into direct memory addresses or offsets.
    * JVM implementations can perform resolution eagerly (at link-time) or lazily (upon first reference).

### 3. Initialization

This is the final phase of class loading. During initialization:
* Static variables are assigned their actual values as defined in the code (e.g., `static int count = 100;` would set `count` to `100`, overriding the default `0` from the preparation phase).
* Static initialization blocks (if any) are executed.

This process is triggered only when the class is actively used for the first time (e.g., an instance is created, a static method is called, or a static field is accessed that is not a compile-time constant). The JVM ensures that initialization is done in a thread-safe manner.

## Dynamic Class Loading and Binding in Java

Java's dynamic loading and binding capabilities provide substantial flexibility:

### Dynamic Loading

Java loads classes at runtime, only when needed.
You can explicitly load classes via:

```java
Class<?> clazz = Class.forName("com.example.MyClass");
```

```java
if (someCondition) {
    try {
        Class<?> clazz = Class.forName("com.example.MyClass");
        Object instance = clazz.getDeclaredConstructor().newInstance();
        // use instance
    } catch (Exception e) {
        e.printStackTrace();
    }
}
```

### Dynamic Binding (Late Binding)
JVM determines the exact method to invoke at runtime.

Essential for polymorphism:

```java
class Animal {
    void sound() { System.out.println("Animal makes a sound"); }
}

class Dog extends Animal {
    @Override
    void sound() { System.out.println("Dog barks"); }
}

public class DynamicBindingExample {
    public static void main(String[] args) {
        Animal animal = new Dog();
        animal.sound();  // Outputs: "Dog barks"
    }
}
```

**Pros and Cons of Java's Dynamic Features**

- **Performance Cost**: Runtime loading, verification, and binding introduce slight overhead.
- **Enhanced Flexibility**: Enables runtime decisions, dynamic plugins, and extensible designs without recompilation.

**Interface-Driven Runtime Decisions**

Using interfaces allows runtime determination of implementations:

```java
public interface PaymentService { void pay(); }

public class CreditCardPayment implements PaymentService {
    public void pay() { System.out.println("Paying with Credit Card"); }
}

public class PayPalPayment implements PaymentService {
    public void pay() { System.out.println("Paying with PayPal"); }
}

public class PaymentProcessor {
    public static void main(String[] args) throws Exception {
        String paymentType = "CreditCardPayment"; // This could come from config or user input
        PaymentService paymentService = (PaymentService)
            Class.forName("com.example." + paymentType)
                 .getDeclaredConstructor().newInstance();

        paymentService.pay();  // "Paying with Credit Card"
    }
}
```

## Java Object Layout and JOL

Java Object Layout (JOL) helps developers understand object memory structures:

- **Object Header**: Metadata (hash code, garbage collection info, lock states).
- **Instance Fields**: Object's actual data.
- **Padding**: Ensures memory alignment (typically multiples of 8 bytes).

### Using JOL Library

You can use the JOL library to inspect these details. Add the dependency:

```gradle
dependencies {
    implementation 'org.openjdk.jol:jol-core:0.16'
}
```

Then, observe the layout:

```java
import org.openjdk.jol.info.ClassLayout;

class SimpleObject {
    int intField;
    long longField;
    byte byteField;
    Object refField;
}

public class JolTest {
    public static void main(String[] args) {
        SimpleObject obj = new SimpleObject();

        System.out.println("Before hashCode():");
        System.out.println(ClassLayout.parseInstance(obj).toPrintable());

        // Calling hashCode() can trigger its computation and storage in the Mark Word
        obj.hashCode();

        System.out.println("After hashCode():");
        System.out.println(ClassLayout.parseInstance(obj).toPrintable());
    }
}
```

Running this code will show the internal layout of `SimpleObject`. The output before and after calling `obj.hashCode()` might reveal changes in the object's Mark Word, as the hash code, if not already computed, gets stored there.

![JOL](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/td1kd28a2ldsuwfyq512.png)

### Object Header

- **Mark Word**: Stores hash codes, GC status (age bits), synchronization states (lock information). Its structure can change depending on the object's state.
- **Class Pointer (Klass Pointer)**: References class metadata in the Method Area, linking the object instance to its class definition.

## Conclusion

Java's ClassLoader and dynamic class loading system enable JVM's platform-independent and extensible runtime environment. By loading, verifying, initializing, and binding classes on-demand, Java strikes a balance between performance and flexibility, making it ideal for complex, secure applications. A deep understanding of these internals transforms your role from a Java developer into a confident operator of the JVM.


---

## Understanding and Resolving the N+1 Query Problem in JPA

- **URL**: https://headf1rst.github.io/log/en/post2
- **Date**: 2025-05-05
- **Tags**: `JPA` `Hibernate` `N+1 Problem`
- **Summary**: Learn what causes the N+1 query problem in JPA, how to identify it, and practical solutions using fetch joins, EntityGraph, and QueryDSL

### Content


The **N+1 query problem** arises when one initial query (1) is followed by N additional queries—one for each result row from the first query. While this might not be an issue for small datasets, it can lead to serious performance degradation at scale.

## Why It Happens in JPA

JPA's N+1 problem is closely related to how entity relationships are fetched using JPQL, which is an object-oriented abstraction over SQL. JPQL queries typically only target the root entity and **ignore related entities unless explicitly fetched**.

Even Spring Data JPA's query methods and QueryDSL ultimately execute JPQL under the hood, so understanding this behavior is essential.

When a related entity is needed during entity access (based on the `FetchType`), Hibernate performs **additional queries** to retrieve them. Let's see how this works in practice.

## Code Example: The N+1 in Action

### Entity Definitions

```java
@Entity
@Getter
@AllArgsConstructor
@NoArgsConstructor(access = AccessLevel.PROTECTED)
public class Transport {

    @Id @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = "driver_id")
    private Driver driver;

    public static Transport from(Driver driver) {
        return new Transport(null, driver);
    }
}
```

```java
@Entity
@Getter
@AllArgsConstructor
@NoArgsConstructor(access = AccessLevel.PROTECTED)
public class Driver {

    @Id @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String name;

    public static Driver from(String name) {
        return new Driver(null, name);
    }
}
```

### Test Code to Observe N+1 Behavior

```java
@SpringBootTest
@Transactional
class TransportRepositoryTest {

    @Autowired 
    private TransportRepository transportRepository;
    @Autowired 
    private DriverRepository driverRepository;
    @Autowired 
    private EntityManager entityManager;

    @Test
    void testNPlusOneQueryProblem() {
        int driverCount = 3;
        for (int i = 0; i < driverCount; i++) {
            Driver driver = Driver.from("Driver " + i);
            driverRepository.save(driver);
        }

        List<Driver> allDrivers = driverRepository.findAll();
        for (Driver driver : allDrivers) {
            transportRepository.save(Transport.from(driver));
        }

        entityManager.flush();
        entityManager.clear();

        System.out.println("=== Finding all transports ===");
        List<Transport> allTransports = transportRepository.findAll();

        System.out.println("=== Accessing driver properties (will trigger N+1 queries) ===");
        for (Transport transport : allTransports) {
            System.out.println("Transport ID: " + transport.getId() + ", Driver Name: " + transport.getDriver().getName());
        }
    }
}
```

### Why Do We Flush and Clear?

We explicitly call `flush()` and `clear()` on the `EntityManager` to prevent the persistence context (first-level cache) from returning entities already in memory. This ensures all queries are sent to the database, and we can accurately observe the N+1 behavior.

### Console Output (Simplified)

```
=== Finding all transports ===
select t1_0.id, t1_0.driver_id from transport t1_0

=== Accessing driver properties (will trigger N+1 queries) ===
select d1_0.id, d1_0.name from driver d1_0 where d1_0.id=?
select d1_0.id, d1_0.name from driver d1_0 where d1_0.id=?
select d1_0.id, d1_0.name from driver d1_0 where d1_0.id=?
```

## What Happens If We Change to `FetchType.EAGER`?

```java
@ManyToOne(fetch = FetchType.EAGER)
@JoinColumn(name = "driver_id")
private Driver driver;
```

With EAGER loading, one might expect JPA to load both Transport and Driver in a single query. But in reality, the result looks like this:

```
=== Finding all transports ===
select t1_0.id, t1_0.driver_id from transport t1_0
select d1_0.id, d1_0.name from driver d1_0 where d1_0.id=?
select d1_0.id, d1_0.name from driver d1_0 where d1_0.id=?
select d1_0.id, d1_0.name from driver d1_0 where d1_0.id=?
```

JPA still executes one query for each related `Driver`. Why?

> JPQL does **not** automatically join associated entities, even with EAGER fetch settings. Unless explicitly specified, related entities are still queried separately.

## It Happens with Both `LAZY` and `EAGER`

* **Lazy Loading**: With `FetchType.LAZY`, JPA proxies the `driver` reference. When accessed (e.g., `transport.getDriver().getName()`), Hibernate fetches it on demand.
* **Eager Loading**: With `FetchType.EAGER`, JPA loads the parent entity first, then executes separate queries for the related entities—especially when JPQL is used without joins.

Even default JPA repository methods like `findAll()` or `findById()` use Hibernate-generated SQL, and will still cause N+1 issues when navigating relationships.

## Solutions to the N+1 Problem

### Use JPQL `fetch join`

```java
@Query("SELECT t FROM Transport t JOIN FETCH t.driver")
List<Transport> findAllWithDriver();
```

This instructs JPA to fetch the associated `Driver` in a single query, avoiding lazy-loading altogether.

### Use QueryDSL `fetchJoin()`

```java
public List<Transport> findAllWithDriverUsingQuerydsl() {
    QTransport transport = QTransport.transport;
    QDriver driver = QDriver.driver;

    return queryFactory
        .selectFrom(transport)
        .leftJoin(transport.driver, driver).fetchJoin()
        .fetch();
}
```

This approach uses QueryDSL's fluent API to explicitly instruct Hibernate to perform a join fetch between `Transport` and its associated `Driver`.

### Use `@EntityGraph`

```java
@EntityGraph(attributePaths = "driver")
List<Transport> findAll();
```

This achieves the same result as a `fetch join` but integrates cleanly with Spring Data JPA method names.

## The Role of Hibernate SQL Generation

Hibernate may choose between different fetching strategies depending on the access path:

* `EntityManager.find()` may result in join queries.
* JPQL ignores fetch type unless a fetch join is explicitly declared.
* `FetchType.EAGER` is not always honored as a join.

> You control your query performance by **being explicit** about your fetching strategy.

## Summary

* N+1 = 1 initial query + N individual queries per entity
* Occurs regardless of `FetchType`
* JPQL ignores associations unless `JOIN FETCH` is used
* Solved using fetch joins, entity graphs, or QueryDSL fetch joins
* Always validate with real query logs

Understanding N+1 isn't just a theory—it's something you can test and observe. Armed with the right strategy, you can eliminate it and make your JPA application truly production-grade.


---

## Kafka Producer Stability Check: Ensuring Message Safety in Apache Kafka

- **URL**: https://headf1rst.github.io/log/en/post1
- **Date**: 2025-05-04
- **Tags**: `Kafka` `Producer` `Reliability` `Apache Kafka`
- **Summary**: Learn how to build fault-tolerant Kafka producers that survive rolling patches and broker failures

### Content


During a recent incident, our team observed message loss from a Kafka producer during an Amazon MSK rolling patch. What began as a routine upgrade quickly uncovered hidden weaknesses in our producer's configuration.

As we dug into the issue, I developed a clearer picture of how Kafka producers interact with broker leaders and what it truly takes to build a production-grade, fault-tolerant producer pipeline. This post captures those insights—covering critical configuration options that influence message delivery reliability and mechanisms behind them.

Let's begin by examining how message loss can occur during a rolling patch—and then broaden our lens to explore other scenarios where Kafka messages might be at risk.

## A Successful Scenario: How Rolling Patches Should Work

Amazon MSK performs "rolling patches" to apply updates while minimizing disruption by restarting brokers one at a time.

In a well-configured environment, the patching process follows a series of fault-tolerant steps that ensure message delivery remains uninterrupted:

**1. Initial State**:

   * All brokers (1, 2, and 3) are operational.
   * Partition 1 has its leader on Broker 1, and its ISR (In-Sync Replicas) includes {Broker 1, 2, 3}.
   * The producer is configured for high resilience, using settings such as a high `retries` count, `acks=all`, and `enable.idempotence=true`.

![kafka cluster setting](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8gb5ff7le7bqsma2ufk7.png)

**2. Patch Initiation (Target: Broker 1)**:

   * MSK initiates a controlled shutdown of Broker 1.
   * The Kafka controller detects the shutdown and reassigns leadership of Partition 1 to another ISR member, such as Broker 2.
   * This metadata change is propagated throughout the cluster.

![kafka cluster](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/634g8uf9aklhre2o55lq.png)

**3. Producer's Initial Reaction**:

   * The producer may still believe Broker 1 is the leader.
   * Send attempts to Broker 1 fail, triggering connection errors or `NotLeaderOrFollowerException`.

**4. Metadata Refresh and Retry Logic**:

   * The producer, equipped with a high retry count, continues retrying.
   * These failures trigger a metadata refresh (either reactively or via `metadata.max.age.ms`).
   * The producer receives updated metadata indicating Broker 2 as the new leader and updates its internal routing.

![metadata](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ckjr45rpuli7cf5lb2k8.png)

**5. Successful Message Delivery**:

   * The message is retried and sent to Broker 2.
   * Broker 2 persists the message locally and replicates it to Broker 3 (Broker 1 is offline).
   * With acknowledgments from all in-sync replicas (2 and 3), and `min.insync.replicas=2` satisfied, Broker 2 responds with a final ACK.

![ISR](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vx16uutcmelb9dcm1lny.png)

As a result, the message is successfully delivered, even though the original leader was taken offline. Kafka's failover mechanism, combined with a resilient producer configuration, ensures no data is lost.

---

However, in our case, the `retries` setting was limited to 5 (Kafka versions before 2.1 defaulted to 0), and `retry.backoff.ms` was set to its default value of 100ms. This left less than a second of total retry time.

Leader re-election and metadata propagation didn't complete within that narrow window. As a result, the producer exhausted all retry attempts before it became aware of the new leader.

Eventually, the producer gave up. If the application doesn't explicitly handle this failure—such as routing to a Dead Letter Queue—the message is lost.

Despite the presence of other brokers, the producer failed to reach the correct leader within its constrained retry window, resulting in irreversible message loss.

---

## Why Leader Re-Election Isn't Instant

When a partition leader becomes unavailable—such as during a rolling patch—Kafka initiates a leader re-election process to maintain availability and consistency. This process is coordinated by a special broker known as the **Controller**.

To understand this process, it's important to first review the roles brokers play in replication. Kafka topics are divided into partitions, and each partition is replicated across multiple brokers. Among these, one broker is elected as the **Leader**, responsible for all read and write operations for that partition. All producers and consumers interact solely with the leader. The other brokers serve as **Followers**, replicating data from the leader to remain synchronized.

### The Role of Controller Broker

The Controller Broker functions as the cluster's brain. It monitors broker health (via ZooKeeper or KRaft), detects failures, and orchestrates the leader re-election process. Importantly, the controller itself is designed to be highly available.

Here's how the re-election process typically unfolds:

1. **Failure Detection**: The controller notices the leader is unresponsive, usually via missed heartbeats or expired sessions.

2. **Partition Identification**: It identifies all partitions for which the failed broker was the leader.

3. **ISR Consultation**: For each affected partition, it consults the ISR (In-Sync Replica) list to determine which followers are fully up to date.

4. **Safe Leader Assignment**: A new leader is selected from the ISR (assuming `unclean.leader.election.enable=false`), ensuring no data loss.

5. **Metadata Update**: The controller records the leadership change in the cluster's metadata (ZooKeeper or KRaft).

6. **Cluster-Wide Propagation**: The new metadata is broadcast to all brokers.

7. **Client Refresh**: Kafka clients (like producers) either detect errors like `NotLeaderOrFollowerException` or refresh metadata after the `metadata.max.age.ms` interval. This enables them to learn the identity of the new leader and resume operations.

### The Timing Challenge

Each step introduces some delay. In practice, the full process—from detecting failure to clients updating their metadata—can take several seconds to tens of seconds, depending on cluster size, network conditions, and whether you're running ZooKeeper or KRaft.

This delay is precisely the danger window: if the producer exhausts its retries before learning about the new leader, the message will be lost.

Understanding this timing is critical to configuring your producer appropriately—and is exactly what our team learned the hard way.

---

## Building a Resilient Kafka Producer: Key Configurations

A resilient Kafka producer doesn't happen by accident—it's the result of carefully chosen configuration settings that account for real-world failure scenarios like broker downtime and leader re-elections.

Below are the key settings that significantly improve the producer's reliability:

### `acks=all`

This setting ensures that the leader broker waits for acknowledgment from all in-sync replicas (ISRs) before responding to the producer. It offers the highest level of durability.

* **Benefit**: Protects against data loss if the leader fails after writing but before replication.
* **Risk without it**: With `acks=1`, the leader acknowledges after writing locally. If it fails before replication, the message is lost.

### `retries=Integer.MAX_VALUE`

Allows the producer to retry failed sends indefinitely (bounded by `delivery.timeout.ms`). Starting with Kafka 2.1, this is the **default** value.

* **Benefit**: Handles transient failures like leader unavailability or network hiccups.

* **Risk without it**: Limited retries can exhaust before leader re-election or metadata refresh completes. (bounded by `delivery.timeout.ms`).

### `enable.idempotence=true`

Prevents duplicate message delivery when retries occur, while also preserving message order within a single partition.

* **How it works**: When idempotence is enabled, each Kafka producer is assigned a unique **Producer ID (PID)**. For every partition the producer writes to, it attaches a **monotonically increasing sequence number** to each message. Brokers track the last successfully written sequence number for each PID/partition pair.

  If the broker receives a message with a sequence number it has already seen—or one that is out of order—it treats it as a duplicate and silently discards it.

* **Guaranteeing Order**: Idempotent producers also preserve message ordering per partition. This is especially critical during retries. To safely maintain this ordering, Kafka enforces that `max.in.flight.requests.per.connection` must be set to **5 or fewer**. Higher values may cause out-of-order retries, which Kafka cannot deduplicate reliably.

* **Requirements**:

  * `acks=all` must be enabled to ensure replication safety.
  * `retries` must be greater than 0 to allow resending.
  * `max.in.flight.requests.per.connection` must be ≤ 5 to keep idempotence active.

By meeting these conditions, Kafka ensures **exactly-once semantics per partition** within a single producer session—without sacrificing performance or message integrity.

This mechanism is crucial for mission-critical systems, where even a single duplicate or out-of-order event could lead to inconsistent downstream state.

* **How it works**: Assigns sequence numbers to messages and uses producer IDs to detect and discard duplicates.
* **Requirement**: Must be used with `acks=all` and `retries>0`.

### Additional Settings to Consider

* `max.in.flight.requests.per.connection<=5`: Controls how many messages can be sent to a broker without receiving acknowledgments.

  When `enable.idempotence=true`, Kafka requires this value to be **≤ 5** to ensure safe deduplication. If it's higher, Kafka disables idempotence to avoid state management complexity.

* `request.timeout.ms`: Time the producer waits for a response from the broker. Should generally be less than or equal to `delivery.timeout.ms`.

By tuning these configurations appropriately, your producer becomes resilient to transient errors, rolling patches, and even brief leader outages—dramatically reducing the risk of message loss.

## When Retries Aren't Enough: Why You Still Need a DLQ

Even with idempotence enabled and retries set to the maximum, Kafka producers can still encounter unrecoverable failures. Scenarios like extended broker outages, prolonged network partitions, message serialization errors, or misconfigurations (e.g., messages exceeding size limits) can cause final send failures.

That's where a **Dead Letter Queue (DLQ)** comes in.

### What Is a DLQ?

A Dead Letter Queue is a secondary Kafka topic or external system where messages are routed after repeated delivery attempts have failed.

### Why You Still Need a DLQ

* **Transient vs. Terminal Failures**: Kafka's retry mechanisms handle transient failures. DLQs catch terminal ones.
* **Delivery Timeout**: Even with `retries=Integer.MAX_VALUE`, Kafka producers ultimately give up if `delivery.timeout.ms` is exceeded.
* **Non-Retriable Errors**: Errors like schema validation failure, record size violations, or authentication issues won't be fixed by retrying.
* **Observability**: DLQs give teams visibility into failed messages for postmortem analysis or manual replay.

### DLQ Design Best Practices

1. **Use a Separate Kafka Topic**: Isolate failed messages in a clearly named topic (e.g., `my-topic.DLQ`).
2. **Include Contextual Metadata**: Such as error reason, original topic and partition, timestamp, and message key.
3. **Avoid Blocking Main Flow**: Ensure DLQ writes are async or decoupled so they don't slow down the main processing path.
4. **Secure and Monitor**: Apply appropriate ACLs and set up alerting/monitoring on DLQ volume.

Implementing a DLQ is a pragmatic and necessary layer of protection. It ensures that when all else fails, your data doesn't disappear silently—and your team has the tooling needed to recover from unexpected edge cases.

---

## Testing Your Configuration: Simulating Failure Scenarios

Reading documentation and tuning configurations is only part of the equation—validating your Kafka producer setup through failure simulations is essential to ensure true resilience.

Here's a step-by-step guide to stress-testing your producer under real-world conditions:

### 1. **Spin Up a Test Cluster**

Set up a local Kafka cluster with at least three brokers using Docker Compose or test infrastructure.

* Ensure replication factor = 3 and `min.insync.replicas = 2` for target topics.

### 2. **Configure Your Producer**

Prepare two configurations:

* **Baseline**: Low retries, no idempotence (e.g., `retries=3`, `acks=1`).
* **Resilient**: Recommended settings (`acks=all`, `retries=Integer.MAX_VALUE`, `enable.idempotence=true`, appropriate backoff and timeouts).

### 3. **Simulate Rolling Broker Restart**

While actively producing messages:

* Restart one broker at a time to mimic a rolling patch.
* Introduce controlled shutdown and observe producer logs.

### 4. **Observe and Compare**

* Do messages get lost or duplicated?
* Do retries behave as expected?
* Are DLQ fallbacks triggered for unrecoverable failures?

By rigorously testing your Kafka setup under adverse conditions, you can verify that your configuration not only looks good on paper—but actually holds up under stress. This ensures peace of mind in production environments where message loss is not an option.

---

## Conclusion: Owning Message Reliability End-to-End

Kafka's durability guarantees are strong, but not infallible. Without a properly configured producer, even a routine rolling patch can lead to silent message loss—something no team wants to discover after the fact.

Through this real-world failure and recovery, we learned that ensuring message safety is a shared responsibility between Kafka and the application. It requires more than just enabling replication—it demands careful attention to producer configuration, retry behavior, idempotence, DLQ design, and validation through controlled failure testing.

By:

* Setting `acks=all`
* Enabling idempotence
* Maximizing retries with meaningful backoff
* Using DLQs for unrecoverable cases

—you can confidently build systems that survive the unexpected.

Message safety isn't a default. It's a design choice. And with the right choices, Kafka becomes not just fast and scalable, but truly reliable.


---


# About This File

This is `llms-full.txt`, part of the llms.txt specification for AI-friendly websites.

- **Blog**: https://headf1rst.github.io/log
- **Author**: Sanha Ko
- **llms.txt**: https://headf1rst.github.io/log/llms.txt
- **GitHub**: https://github.com/headF1rst

For more information about llms.txt, visit: https://llmstxt.org/
