<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="apple-touch-icon" sizes="180x180" href="https://i.imgur.com/2nHGFTv.png"/><link rel="icon" type="image/png" sizes="32x32" href="https://i.imgur.com/2nHGFTv.png"/><link rel="icon" type="image/png" sizes="16x16" href="https://i.imgur.com/2nHGFTv.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><title>Kafka Producer Stability Check: Ensuring Message Safety in Apache Kafka</title><meta name="title" content="Kafka Producer Stability Check: Ensuring Message Safety in Apache Kafka"/><meta name="description" content="Learn how to build fault-tolerant Kafka producers that survive rolling patches and broker failures"/><meta name="keywords" content="Kafka, producer, reliability, message safety, rolling patch, broker failure"/><meta name="robots" content="index, follow"/><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="language" content="English"/><meta name="author" content="Sanha Ko"/><meta property="og:title" content="Kafka Producer Stability Check: Ensuring Message Safety in Apache Kafka"/><meta property="og:url" content="https://headf1rst.github.io/log/en/post1"/><meta property="og:type" content="blog"/><meta property="og:image" content="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ojt2w7s7rr9kws35r2yg.png"/><meta property="og:description" content="Learn how to build fault-tolerant Kafka producers that survive rolling patches and broker failures"/><link rel="alternate" hrefLang="ko" href="https://headf1rst.github.io/log/ko/post1"/><link rel="alternate" hrefLang="en" href="https://headf1rst.github.io/log/en/post1"/><link rel="alternate" hrefLang="x-default" href="https://headf1rst.github.io/log/ko/post1"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kafka Producer Stability Check: Ensuring Message Safety in Apache Kafka","datePublished":"2025-05-04","dateModified":"2025-05-04","author":{"@type":"Person","name":"Sanha Ko"},"description":"Learn how to build fault-tolerant Kafka producers that survive rolling patches and broker failures","image":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ojt2w7s7rr9kws35r2yg.png","url":"https://headf1rst.github.io/TIL/en/post1","keywords":"Kafka,Producer,Reliability,Apache Kafka","inLanguage":"en-US","publisher":{"@type":"Organization","name":"headF1rst","logo":{"@type":"ImageObject","url":"https://headf1rst.github.io/TIL/favicon.ico"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://headf1rst.github.io/TIL/en/post1"}}</script><meta name="next-head-count" content="26"/><link rel="preload" href="/log/_next/static/css/437194234ad87d89.css" as="style"/><link rel="stylesheet" href="/log/_next/static/css/437194234ad87d89.css" data-n-g=""/><link rel="preload" href="/log/_next/static/css/38994dd0aed51f22.css" as="style"/><link rel="stylesheet" href="/log/_next/static/css/38994dd0aed51f22.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/log/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/log/_next/static/chunks/webpack-1b0decc0375da57b.js" defer=""></script><script src="/log/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/log/_next/static/chunks/main-6e6d6dc6617c8481.js" defer=""></script><script src="/log/_next/static/chunks/pages/_app-7797c1cc757d9cb4.js" defer=""></script><script src="/log/_next/static/chunks/pages/%5Blang%5D/%5Bid%5D-81798309aa500efe.js" defer=""></script><script src="/log/_next/static/cw6uK4UnfZBis5R_g8eEP/_buildManifest.js" defer=""></script><script src="/log/_next/static/cw6uK4UnfZBis5R_g8eEP/_ssgManifest.js" defer=""></script></head><body><div id="__next"><nav class="flex bg-white sticky top-0 left-0 z-50 justify-between items-center border-b-2 border-gray-100 py-3 md:justify-start md:space-x-10 px-10 sm:px-5 dark:bg-[#0d1117] dark:text-[#c9d1d9] dark:border-gray-600"><div class="flex justify-start"><a href="/log/en/"><div class="flex items-center gap-2 cursor-pointer"><h1 class="text-lg ">JustAnotherBlog</h1></div></a></div><div class="flex justify-between gap-10 sm:hidden items-center"><a href="/log/en/"><button class="font-light hover:text-indigo-300 text-base">Home</button></a><a href="/log/en/?section=tech"><button class="font-light hover:text-indigo-300 text-base">Tech</button></a><a href="/log/en/?section=domain"><button class="font-light hover:text-indigo-300 text-base">Domain</button></a><a href="https://plain-composer-c65.notion.site/29c7640fdf054059b6ea28ed61189bfb" target="_blank" rel="noreferrer" class="hover:text-indigo-300 text-base font-light flex gap-1">About<span class="flex justify-center" style="align-items:center"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="m13 3 3.293 3.293-7 7 1.414 1.414 7-7L21 11V3z"></path><path d="M19 19H5V5h7l-2-2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2h14c1.103 0 2-.897 2-2v-5l-2-2v7z"></path></svg></span></a><div class="relative"><button class="flex items-center gap-1 hover:text-indigo-300 text-base font-light">ğŸ‡ºğŸ‡¸<!-- --> <!-- -->English</button></div></div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="lg:hidden" height="25" width="25" xmlns="http://www.w3.org/2000/svg"><path d="M32 96v64h448V96H32zm0 128v64h448v-64H32zm0 128v64h448v-64H32z"></path></svg></nav><main class="dark:bg-[#0d1117] h-screen pb-10"><div class="flex flex-col w-3/5 sm:w-5/6 m-auto pt-20 pb-20 gap-10 dark:bg-[#0d1117] dark:text-[#c9d1d9]"><ul id="scroll-spy" class="sm:hidden p-10 fixed top-50 right-0 h-full w-1/5 text-gray-500 dark:text-[#c9d1d9]"></ul><div class="text-5xl font-bold">Kafka Producer Stability Check: Ensuring Message Safety in Apache Kafka</div><div class="flex flex-col gap-2"><div class="text-base text-gray-600 dark:text-gray-300">2025-05-04</div><div class="flex flex-wrap gap-2 dark:text-black sm:m-0"><span class="p-1 pl-3 pr-3 rounded-md bg-indigo-100 hover:bg-indigo-200 cursor-pointer transition ease-in-out duration-200 text-sm">Kafka</span><span class="p-1 pl-3 pr-3 rounded-md bg-indigo-100 hover:bg-indigo-200 cursor-pointer transition ease-in-out duration-200 text-sm">Producer</span><span class="p-1 pl-3 pr-3 rounded-md bg-indigo-100 hover:bg-indigo-200 cursor-pointer transition ease-in-out duration-200 text-sm">Reliability</span><span class="p-1 pl-3 pr-3 rounded-md bg-indigo-100 hover:bg-indigo-200 cursor-pointer transition ease-in-out duration-200 text-sm">Apache Kafka</span></div></div><div class="flex items-center gap-2"><a class="text-sm text-indigo-600 hover:underline" href="/log/ko/post1/">ğŸ‡°ğŸ‡· Read in Korean</a></div><div class="markdown-body" style="font-size:17px"><p>During a recent incident, our team observed message loss from a Kafka producer during an Amazon MSK rolling patch. What began as a routine upgrade quickly uncovered hidden weaknesses in our producer&#x27;s configuration.</p>
<p>As we dug into the issue, I developed a clearer picture of how Kafka producers interact with broker leaders and what it truly takes to build a production-grade, fault-tolerant producer pipeline. This post captures those insightsâ€”covering critical configuration options that influence message delivery reliability and mechanisms behind them.</p>
<p>Let&#x27;s begin by examining how message loss can occur during a rolling patchâ€”and then broaden our lens to explore other scenarios where Kafka messages might be at risk.</p>
<h2>A Successful Scenario: How Rolling Patches Should Work</h2>
<p>Amazon MSK performs &quot;rolling patches&quot; to apply updates while minimizing disruption by restarting brokers one at a time.</p>
<p>In a well-configured environment, the patching process follows a series of fault-tolerant steps that ensure message delivery remains uninterrupted:</p>
<p><strong>1. Initial State</strong>:</p>
<ul>
<li>All brokers (1, 2, and 3) are operational.</li>
<li>Partition 1 has its leader on Broker 1, and its ISR (In-Sync Replicas) includes {Broker 1, 2, 3}.</li>
<li>The producer is configured for high resilience, using settings such as a high <code node="[object Object]">retries</code> count, <code node="[object Object]">acks=all</code>, and <code node="[object Object]">enable.idempotence=true</code>.</li>
</ul>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8gb5ff7le7bqsma2ufk7.png" alt="Post image" node="[object Object]" style="max-height:450px;max-width:90%"/></p>
<p><strong>2. Patch Initiation (Target: Broker 1)</strong>:</p>
<ul>
<li>MSK initiates a controlled shutdown of Broker 1.</li>
<li>The Kafka controller detects the shutdown and reassigns leadership of Partition 1 to another ISR member, such as Broker 2.</li>
<li>This metadata change is propagated throughout the cluster.</li>
</ul>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/634g8uf9aklhre2o55lq.png" alt="Post image" node="[object Object]" style="max-height:450px;max-width:90%"/></p>
<p><strong>3. Producer&#x27;s Initial Reaction</strong>:</p>
<ul>
<li>The producer may still believe Broker 1 is the leader.</li>
<li>Send attempts to Broker 1 fail, triggering connection errors or <code node="[object Object]">NotLeaderOrFollowerException</code>.</li>
</ul>
<p><strong>4. Metadata Refresh and Retry Logic</strong>:</p>
<ul>
<li>The producer, equipped with a high retry count, continues retrying.</li>
<li>These failures trigger a metadata refresh (either reactively or via <code node="[object Object]">metadata.max.age.ms</code>).</li>
<li>The producer receives updated metadata indicating Broker 2 as the new leader and updates its internal routing.</li>
</ul>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ckjr45rpuli7cf5lb2k8.png" alt="Post image" node="[object Object]" style="max-height:450px;max-width:90%"/></p>
<p><strong>5. Successful Message Delivery</strong>:</p>
<ul>
<li>The message is retried and sent to Broker 2.</li>
<li>Broker 2 persists the message locally and replicates it to Broker 3 (Broker 1 is offline).</li>
<li>With acknowledgments from all in-sync replicas (2 and 3), and <code node="[object Object]">min.insync.replicas=2</code> satisfied, Broker 2 responds with a final ACK.</li>
</ul>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vx16uutcmelb9dcm1lny.png" alt="Post image" node="[object Object]" style="max-height:450px;max-width:90%"/></p>
<p>As a result, the message is successfully delivered, even though the original leader was taken offline. Kafka&#x27;s failover mechanism, combined with a resilient producer configuration, ensures no data is lost.</p>
<hr/>
<p>However, in our case, the <code node="[object Object]">retries</code> setting was limited to 5 (Kafka versions before 2.1 defaulted to 0), and <code node="[object Object]">retry.backoff.ms</code> was set to its default value of 100ms. This left less than a second of total retry time.</p>
<p>Leader re-election and metadata propagation didn&#x27;t complete within that narrow window. As a result, the producer exhausted all retry attempts before it became aware of the new leader.</p>
<p>Eventually, the producer gave up. If the application doesn&#x27;t explicitly handle this failureâ€”such as routing to a Dead Letter Queueâ€”the message is lost.</p>
<p>Despite the presence of other brokers, the producer failed to reach the correct leader within its constrained retry window, resulting in irreversible message loss.</p>
<hr/>
<h2>Why Leader Re-Election Isn&#x27;t Instant</h2>
<p>When a partition leader becomes unavailableâ€”such as during a rolling patchâ€”Kafka initiates a leader re-election process to maintain availability and consistency. This process is coordinated by a special broker known as the <strong>Controller</strong>.</p>
<p>To understand this process, it&#x27;s important to first review the roles brokers play in replication. Kafka topics are divided into partitions, and each partition is replicated across multiple brokers. Among these, one broker is elected as the <strong>Leader</strong>, responsible for all read and write operations for that partition. All producers and consumers interact solely with the leader. The other brokers serve as <strong>Followers</strong>, replicating data from the leader to remain synchronized.</p>
<h3>The Role of Controller Broker</h3>
<p>The Controller Broker functions as the cluster&#x27;s brain. It monitors broker health (via ZooKeeper or KRaft), detects failures, and orchestrates the leader re-election process. Importantly, the controller itself is designed to be highly available.</p>
<p>Here&#x27;s how the re-election process typically unfolds:</p>
<ol>
<li>
<p><strong>Failure Detection</strong>: The controller notices the leader is unresponsive, usually via missed heartbeats or expired sessions.</p>
</li>
<li>
<p><strong>Partition Identification</strong>: It identifies all partitions for which the failed broker was the leader.</p>
</li>
<li>
<p><strong>ISR Consultation</strong>: For each affected partition, it consults the ISR (In-Sync Replica) list to determine which followers are fully up to date.</p>
</li>
<li>
<p><strong>Safe Leader Assignment</strong>: A new leader is selected from the ISR (assuming <code node="[object Object]">unclean.leader.election.enable=false</code>), ensuring no data loss.</p>
</li>
<li>
<p><strong>Metadata Update</strong>: The controller records the leadership change in the cluster&#x27;s metadata (ZooKeeper or KRaft).</p>
</li>
<li>
<p><strong>Cluster-Wide Propagation</strong>: The new metadata is broadcast to all brokers.</p>
</li>
<li>
<p><strong>Client Refresh</strong>: Kafka clients (like producers) either detect errors like <code node="[object Object]">NotLeaderOrFollowerException</code> or refresh metadata after the <code node="[object Object]">metadata.max.age.ms</code> interval. This enables them to learn the identity of the new leader and resume operations.</p>
</li>
</ol>
<h3>The Timing Challenge</h3>
<p>Each step introduces some delay. In practice, the full processâ€”from detecting failure to clients updating their metadataâ€”can take several seconds to tens of seconds, depending on cluster size, network conditions, and whether you&#x27;re running ZooKeeper or KRaft.</p>
<p>This delay is precisely the danger window: if the producer exhausts its retries before learning about the new leader, the message will be lost.</p>
<p>Understanding this timing is critical to configuring your producer appropriatelyâ€”and is exactly what our team learned the hard way.</p>
<hr/>
<h2>Building a Resilient Kafka Producer: Key Configurations</h2>
<p>A resilient Kafka producer doesn&#x27;t happen by accidentâ€”it&#x27;s the result of carefully chosen configuration settings that account for real-world failure scenarios like broker downtime and leader re-elections.</p>
<p>Below are the key settings that significantly improve the producer&#x27;s reliability:</p>
<h3><code node="[object Object]">acks=all</code></h3>
<p>This setting ensures that the leader broker waits for acknowledgment from all in-sync replicas (ISRs) before responding to the producer. It offers the highest level of durability.</p>
<ul>
<li><strong>Benefit</strong>: Protects against data loss if the leader fails after writing but before replication.</li>
<li><strong>Risk without it</strong>: With <code node="[object Object]">acks=1</code>, the leader acknowledges after writing locally. If it fails before replication, the message is lost.</li>
</ul>
<h3><code node="[object Object]">retries=Integer.MAX_VALUE</code></h3>
<p>Allows the producer to retry failed sends indefinitely (bounded by <code node="[object Object]">delivery.timeout.ms</code>). Starting with Kafka 2.1, this is the <strong>default</strong> value.</p>
<ul>
<li>
<p><strong>Benefit</strong>: Handles transient failures like leader unavailability or network hiccups.</p>
</li>
<li>
<p><strong>Risk without it</strong>: Limited retries can exhaust before leader re-election or metadata refresh completes. (bounded by <code node="[object Object]">delivery.timeout.ms</code>).</p>
</li>
</ul>
<h3><code node="[object Object]">enable.idempotence=true</code></h3>
<p>Prevents duplicate message delivery when retries occur, while also preserving message order within a single partition.</p>
<ul>
<li>
<p><strong>How it works</strong>: When idempotence is enabled, each Kafka producer is assigned a unique <strong>Producer ID (PID)</strong>. For every partition the producer writes to, it attaches a <strong>monotonically increasing sequence number</strong> to each message. Brokers track the last successfully written sequence number for each PID/partition pair.</p>
<p>If the broker receives a message with a sequence number it has already seenâ€”or one that is out of orderâ€”it treats it as a duplicate and silently discards it.</p>
</li>
<li>
<p><strong>Guaranteeing Order</strong>: Idempotent producers also preserve message ordering per partition. This is especially critical during retries. To safely maintain this ordering, Kafka enforces that <code node="[object Object]">max.in.flight.requests.per.connection</code> must be set to <strong>5 or fewer</strong>. Higher values may cause out-of-order retries, which Kafka cannot deduplicate reliably.</p>
</li>
<li>
<p><strong>Requirements</strong>:</p>
<ul>
<li><code node="[object Object]">acks=all</code> must be enabled to ensure replication safety.</li>
<li><code node="[object Object]">retries</code> must be greater than 0 to allow resending.</li>
<li><code node="[object Object]">max.in.flight.requests.per.connection</code> must be â‰¤ 5 to keep idempotence active.</li>
</ul>
</li>
</ul>
<p>By meeting these conditions, Kafka ensures <strong>exactly-once semantics per partition</strong> within a single producer sessionâ€”without sacrificing performance or message integrity.</p>
<p>This mechanism is crucial for mission-critical systems, where even a single duplicate or out-of-order event could lead to inconsistent downstream state.</p>
<ul>
<li><strong>How it works</strong>: Assigns sequence numbers to messages and uses producer IDs to detect and discard duplicates.</li>
<li><strong>Requirement</strong>: Must be used with <code node="[object Object]">acks=all</code> and <code node="[object Object]">retries&gt;0</code>.</li>
</ul>
<h3>Additional Settings to Consider</h3>
<ul>
<li>
<p><code node="[object Object]">max.in.flight.requests.per.connection&lt;=5</code>: Controls how many messages can be sent to a broker without receiving acknowledgments.</p>
<p>When <code node="[object Object]">enable.idempotence=true</code>, Kafka requires this value to be <strong>â‰¤ 5</strong> to ensure safe deduplication. If it&#x27;s higher, Kafka disables idempotence to avoid state management complexity.</p>
</li>
<li>
<p><code node="[object Object]">request.timeout.ms</code>: Time the producer waits for a response from the broker. Should generally be less than or equal to <code node="[object Object]">delivery.timeout.ms</code>.</p>
</li>
</ul>
<p>By tuning these configurations appropriately, your producer becomes resilient to transient errors, rolling patches, and even brief leader outagesâ€”dramatically reducing the risk of message loss.</p>
<h2>When Retries Aren&#x27;t Enough: Why You Still Need a DLQ</h2>
<p>Even with idempotence enabled and retries set to the maximum, Kafka producers can still encounter unrecoverable failures. Scenarios like extended broker outages, prolonged network partitions, message serialization errors, or misconfigurations (e.g., messages exceeding size limits) can cause final send failures.</p>
<p>That&#x27;s where a <strong>Dead Letter Queue (DLQ)</strong> comes in.</p>
<h3>What Is a DLQ?</h3>
<p>A Dead Letter Queue is a secondary Kafka topic or external system where messages are routed after repeated delivery attempts have failed.</p>
<h3>Why You Still Need a DLQ</h3>
<ul>
<li><strong>Transient vs. Terminal Failures</strong>: Kafka&#x27;s retry mechanisms handle transient failures. DLQs catch terminal ones.</li>
<li><strong>Delivery Timeout</strong>: Even with <code node="[object Object]">retries=Integer.MAX_VALUE</code>, Kafka producers ultimately give up if <code node="[object Object]">delivery.timeout.ms</code> is exceeded.</li>
<li><strong>Non-Retriable Errors</strong>: Errors like schema validation failure, record size violations, or authentication issues won&#x27;t be fixed by retrying.</li>
<li><strong>Observability</strong>: DLQs give teams visibility into failed messages for postmortem analysis or manual replay.</li>
</ul>
<h3>DLQ Design Best Practices</h3>
<ol>
<li><strong>Use a Separate Kafka Topic</strong>: Isolate failed messages in a clearly named topic (e.g., <code node="[object Object]">my-topic.DLQ</code>).</li>
<li><strong>Include Contextual Metadata</strong>: Such as error reason, original topic and partition, timestamp, and message key.</li>
<li><strong>Avoid Blocking Main Flow</strong>: Ensure DLQ writes are async or decoupled so they don&#x27;t slow down the main processing path.</li>
<li><strong>Secure and Monitor</strong>: Apply appropriate ACLs and set up alerting/monitoring on DLQ volume.</li>
</ol>
<p>Implementing a DLQ is a pragmatic and necessary layer of protection. It ensures that when all else fails, your data doesn&#x27;t disappear silentlyâ€”and your team has the tooling needed to recover from unexpected edge cases.</p>
<hr/>
<h2>Testing Your Configuration: Simulating Failure Scenarios</h2>
<p>Reading documentation and tuning configurations is only part of the equationâ€”validating your Kafka producer setup through failure simulations is essential to ensure true resilience.</p>
<p>Here&#x27;s a step-by-step guide to stress-testing your producer under real-world conditions:</p>
<h3>1. <strong>Spin Up a Test Cluster</strong></h3>
<p>Set up a local Kafka cluster with at least three brokers using Docker Compose or test infrastructure.</p>
<ul>
<li>Ensure replication factor = 3 and <code node="[object Object]">min.insync.replicas = 2</code> for target topics.</li>
</ul>
<h3>2. <strong>Configure Your Producer</strong></h3>
<p>Prepare two configurations:</p>
<ul>
<li><strong>Baseline</strong>: Low retries, no idempotence (e.g., <code node="[object Object]">retries=3</code>, <code node="[object Object]">acks=1</code>).</li>
<li><strong>Resilient</strong>: Recommended settings (<code node="[object Object]">acks=all</code>, <code node="[object Object]">retries=Integer.MAX_VALUE</code>, <code node="[object Object]">enable.idempotence=true</code>, appropriate backoff and timeouts).</li>
</ul>
<h3>3. <strong>Simulate Rolling Broker Restart</strong></h3>
<p>While actively producing messages:</p>
<ul>
<li>Restart one broker at a time to mimic a rolling patch.</li>
<li>Introduce controlled shutdown and observe producer logs.</li>
</ul>
<h3>4. <strong>Observe and Compare</strong></h3>
<ul>
<li>Do messages get lost or duplicated?</li>
<li>Do retries behave as expected?</li>
<li>Are DLQ fallbacks triggered for unrecoverable failures?</li>
</ul>
<p>By rigorously testing your Kafka setup under adverse conditions, you can verify that your configuration not only looks good on paperâ€”but actually holds up under stress. This ensures peace of mind in production environments where message loss is not an option.</p>
<hr/>
<h2>Conclusion: Owning Message Reliability End-to-End</h2>
<p>Kafka&#x27;s durability guarantees are strong, but not infallible. Without a properly configured producer, even a routine rolling patch can lead to silent message lossâ€”something no team wants to discover after the fact.</p>
<p>Through this real-world failure and recovery, we learned that ensuring message safety is a shared responsibility between Kafka and the application. It requires more than just enabling replicationâ€”it demands careful attention to producer configuration, retry behavior, idempotence, DLQ design, and validation through controlled failure testing.</p>
<p>By:</p>
<ul>
<li>Setting <code node="[object Object]">acks=all</code></li>
<li>Enabling idempotence</li>
<li>Maximizing retries with meaningful backoff</li>
<li>Using DLQs for unrecoverable cases</li>
</ul>
<p>â€”you can confidently build systems that survive the unexpected.</p>
<p>Message safety isn&#x27;t a default. It&#x27;s a design choice. And with the right choices, Kafka becomes not just fast and scalable, but truly reliable.</p></div><section></section></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"post1","lang":"en","title":"Kafka Producer Stability Check: Ensuring Message Safety in Apache Kafka","section":"tech","date":"2025-05-04","tags":"Kafka, Producer, Reliability, Apache Kafka","thumbnail":"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ojt2w7s7rr9kws35r2yg.png","description":"Learn how to build fault-tolerant Kafka producers that survive rolling patches and broker failures","searchKeywords":"Kafka, producer, reliability, message safety, rolling patch, broker failure","preview":"\nDuring a recent incident, our team observed message loss from a Kafka producer during an Amazon MSK rolling patch. What began as a routine "},"detail":"\nDuring a recent incident, our team observed message loss from a Kafka producer during an Amazon MSK rolling patch. What began as a routine upgrade quickly uncovered hidden weaknesses in our producer's configuration.\n\nAs we dug into the issue, I developed a clearer picture of how Kafka producers interact with broker leaders and what it truly takes to build a production-grade, fault-tolerant producer pipeline. This post captures those insightsâ€”covering critical configuration options that influence message delivery reliability and mechanisms behind them.\n\nLet's begin by examining how message loss can occur during a rolling patchâ€”and then broaden our lens to explore other scenarios where Kafka messages might be at risk.\n\n## A Successful Scenario: How Rolling Patches Should Work\n\nAmazon MSK performs \"rolling patches\" to apply updates while minimizing disruption by restarting brokers one at a time.\n\nIn a well-configured environment, the patching process follows a series of fault-tolerant steps that ensure message delivery remains uninterrupted:\n\n**1. Initial State**:\n\n   * All brokers (1, 2, and 3) are operational.\n   * Partition 1 has its leader on Broker 1, and its ISR (In-Sync Replicas) includes {Broker 1, 2, 3}.\n   * The producer is configured for high resilience, using settings such as a high `retries` count, `acks=all`, and `enable.idempotence=true`.\n\n![kafka cluster setting](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8gb5ff7le7bqsma2ufk7.png)\n\n**2. Patch Initiation (Target: Broker 1)**:\n\n   * MSK initiates a controlled shutdown of Broker 1.\n   * The Kafka controller detects the shutdown and reassigns leadership of Partition 1 to another ISR member, such as Broker 2.\n   * This metadata change is propagated throughout the cluster.\n\n![kafka cluster](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/634g8uf9aklhre2o55lq.png)\n\n**3. Producer's Initial Reaction**:\n\n   * The producer may still believe Broker 1 is the leader.\n   * Send attempts to Broker 1 fail, triggering connection errors or `NotLeaderOrFollowerException`.\n\n**4. Metadata Refresh and Retry Logic**:\n\n   * The producer, equipped with a high retry count, continues retrying.\n   * These failures trigger a metadata refresh (either reactively or via `metadata.max.age.ms`).\n   * The producer receives updated metadata indicating Broker 2 as the new leader and updates its internal routing.\n\n![metadata](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ckjr45rpuli7cf5lb2k8.png)\n\n**5. Successful Message Delivery**:\n\n   * The message is retried and sent to Broker 2.\n   * Broker 2 persists the message locally and replicates it to Broker 3 (Broker 1 is offline).\n   * With acknowledgments from all in-sync replicas (2 and 3), and `min.insync.replicas=2` satisfied, Broker 2 responds with a final ACK.\n\n![ISR](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vx16uutcmelb9dcm1lny.png)\n\nAs a result, the message is successfully delivered, even though the original leader was taken offline. Kafka's failover mechanism, combined with a resilient producer configuration, ensures no data is lost.\n\n---\n\nHowever, in our case, the `retries` setting was limited to 5 (Kafka versions before 2.1 defaulted to 0), and `retry.backoff.ms` was set to its default value of 100ms. This left less than a second of total retry time.\n\nLeader re-election and metadata propagation didn't complete within that narrow window. As a result, the producer exhausted all retry attempts before it became aware of the new leader.\n\nEventually, the producer gave up. If the application doesn't explicitly handle this failureâ€”such as routing to a Dead Letter Queueâ€”the message is lost.\n\nDespite the presence of other brokers, the producer failed to reach the correct leader within its constrained retry window, resulting in irreversible message loss.\n\n---\n\n## Why Leader Re-Election Isn't Instant\n\nWhen a partition leader becomes unavailableâ€”such as during a rolling patchâ€”Kafka initiates a leader re-election process to maintain availability and consistency. This process is coordinated by a special broker known as the **Controller**.\n\nTo understand this process, it's important to first review the roles brokers play in replication. Kafka topics are divided into partitions, and each partition is replicated across multiple brokers. Among these, one broker is elected as the **Leader**, responsible for all read and write operations for that partition. All producers and consumers interact solely with the leader. The other brokers serve as **Followers**, replicating data from the leader to remain synchronized.\n\n### The Role of Controller Broker\n\nThe Controller Broker functions as the cluster's brain. It monitors broker health (via ZooKeeper or KRaft), detects failures, and orchestrates the leader re-election process. Importantly, the controller itself is designed to be highly available.\n\nHere's how the re-election process typically unfolds:\n\n1. **Failure Detection**: The controller notices the leader is unresponsive, usually via missed heartbeats or expired sessions.\n\n2. **Partition Identification**: It identifies all partitions for which the failed broker was the leader.\n\n3. **ISR Consultation**: For each affected partition, it consults the ISR (In-Sync Replica) list to determine which followers are fully up to date.\n\n4. **Safe Leader Assignment**: A new leader is selected from the ISR (assuming `unclean.leader.election.enable=false`), ensuring no data loss.\n\n5. **Metadata Update**: The controller records the leadership change in the cluster's metadata (ZooKeeper or KRaft).\n\n6. **Cluster-Wide Propagation**: The new metadata is broadcast to all brokers.\n\n7. **Client Refresh**: Kafka clients (like producers) either detect errors like `NotLeaderOrFollowerException` or refresh metadata after the `metadata.max.age.ms` interval. This enables them to learn the identity of the new leader and resume operations.\n\n### The Timing Challenge\n\nEach step introduces some delay. In practice, the full processâ€”from detecting failure to clients updating their metadataâ€”can take several seconds to tens of seconds, depending on cluster size, network conditions, and whether you're running ZooKeeper or KRaft.\n\nThis delay is precisely the danger window: if the producer exhausts its retries before learning about the new leader, the message will be lost.\n\nUnderstanding this timing is critical to configuring your producer appropriatelyâ€”and is exactly what our team learned the hard way.\n\n---\n\n## Building a Resilient Kafka Producer: Key Configurations\n\nA resilient Kafka producer doesn't happen by accidentâ€”it's the result of carefully chosen configuration settings that account for real-world failure scenarios like broker downtime and leader re-elections.\n\nBelow are the key settings that significantly improve the producer's reliability:\n\n### `acks=all`\n\nThis setting ensures that the leader broker waits for acknowledgment from all in-sync replicas (ISRs) before responding to the producer. It offers the highest level of durability.\n\n* **Benefit**: Protects against data loss if the leader fails after writing but before replication.\n* **Risk without it**: With `acks=1`, the leader acknowledges after writing locally. If it fails before replication, the message is lost.\n\n### `retries=Integer.MAX_VALUE`\n\nAllows the producer to retry failed sends indefinitely (bounded by `delivery.timeout.ms`). Starting with Kafka 2.1, this is the **default** value.\n\n* **Benefit**: Handles transient failures like leader unavailability or network hiccups.\n\n* **Risk without it**: Limited retries can exhaust before leader re-election or metadata refresh completes. (bounded by `delivery.timeout.ms`).\n\n### `enable.idempotence=true`\n\nPrevents duplicate message delivery when retries occur, while also preserving message order within a single partition.\n\n* **How it works**: When idempotence is enabled, each Kafka producer is assigned a unique **Producer ID (PID)**. For every partition the producer writes to, it attaches a **monotonically increasing sequence number** to each message. Brokers track the last successfully written sequence number for each PID/partition pair.\n\n  If the broker receives a message with a sequence number it has already seenâ€”or one that is out of orderâ€”it treats it as a duplicate and silently discards it.\n\n* **Guaranteeing Order**: Idempotent producers also preserve message ordering per partition. This is especially critical during retries. To safely maintain this ordering, Kafka enforces that `max.in.flight.requests.per.connection` must be set to **5 or fewer**. Higher values may cause out-of-order retries, which Kafka cannot deduplicate reliably.\n\n* **Requirements**:\n\n  * `acks=all` must be enabled to ensure replication safety.\n  * `retries` must be greater than 0 to allow resending.\n  * `max.in.flight.requests.per.connection` must be â‰¤ 5 to keep idempotence active.\n\nBy meeting these conditions, Kafka ensures **exactly-once semantics per partition** within a single producer sessionâ€”without sacrificing performance or message integrity.\n\nThis mechanism is crucial for mission-critical systems, where even a single duplicate or out-of-order event could lead to inconsistent downstream state.\n\n* **How it works**: Assigns sequence numbers to messages and uses producer IDs to detect and discard duplicates.\n* **Requirement**: Must be used with `acks=all` and `retries\u003e0`.\n\n### Additional Settings to Consider\n\n* `max.in.flight.requests.per.connection\u003c=5`: Controls how many messages can be sent to a broker without receiving acknowledgments.\n\n  When `enable.idempotence=true`, Kafka requires this value to be **â‰¤ 5** to ensure safe deduplication. If it's higher, Kafka disables idempotence to avoid state management complexity.\n\n* `request.timeout.ms`: Time the producer waits for a response from the broker. Should generally be less than or equal to `delivery.timeout.ms`.\n\nBy tuning these configurations appropriately, your producer becomes resilient to transient errors, rolling patches, and even brief leader outagesâ€”dramatically reducing the risk of message loss.\n\n## When Retries Aren't Enough: Why You Still Need a DLQ\n\nEven with idempotence enabled and retries set to the maximum, Kafka producers can still encounter unrecoverable failures. Scenarios like extended broker outages, prolonged network partitions, message serialization errors, or misconfigurations (e.g., messages exceeding size limits) can cause final send failures.\n\nThat's where a **Dead Letter Queue (DLQ)** comes in.\n\n### What Is a DLQ?\n\nA Dead Letter Queue is a secondary Kafka topic or external system where messages are routed after repeated delivery attempts have failed.\n\n### Why You Still Need a DLQ\n\n* **Transient vs. Terminal Failures**: Kafka's retry mechanisms handle transient failures. DLQs catch terminal ones.\n* **Delivery Timeout**: Even with `retries=Integer.MAX_VALUE`, Kafka producers ultimately give up if `delivery.timeout.ms` is exceeded.\n* **Non-Retriable Errors**: Errors like schema validation failure, record size violations, or authentication issues won't be fixed by retrying.\n* **Observability**: DLQs give teams visibility into failed messages for postmortem analysis or manual replay.\n\n### DLQ Design Best Practices\n\n1. **Use a Separate Kafka Topic**: Isolate failed messages in a clearly named topic (e.g., `my-topic.DLQ`).\n2. **Include Contextual Metadata**: Such as error reason, original topic and partition, timestamp, and message key.\n3. **Avoid Blocking Main Flow**: Ensure DLQ writes are async or decoupled so they don't slow down the main processing path.\n4. **Secure and Monitor**: Apply appropriate ACLs and set up alerting/monitoring on DLQ volume.\n\nImplementing a DLQ is a pragmatic and necessary layer of protection. It ensures that when all else fails, your data doesn't disappear silentlyâ€”and your team has the tooling needed to recover from unexpected edge cases.\n\n---\n\n## Testing Your Configuration: Simulating Failure Scenarios\n\nReading documentation and tuning configurations is only part of the equationâ€”validating your Kafka producer setup through failure simulations is essential to ensure true resilience.\n\nHere's a step-by-step guide to stress-testing your producer under real-world conditions:\n\n### 1. **Spin Up a Test Cluster**\n\nSet up a local Kafka cluster with at least three brokers using Docker Compose or test infrastructure.\n\n* Ensure replication factor = 3 and `min.insync.replicas = 2` for target topics.\n\n### 2. **Configure Your Producer**\n\nPrepare two configurations:\n\n* **Baseline**: Low retries, no idempotence (e.g., `retries=3`, `acks=1`).\n* **Resilient**: Recommended settings (`acks=all`, `retries=Integer.MAX_VALUE`, `enable.idempotence=true`, appropriate backoff and timeouts).\n\n### 3. **Simulate Rolling Broker Restart**\n\nWhile actively producing messages:\n\n* Restart one broker at a time to mimic a rolling patch.\n* Introduce controlled shutdown and observe producer logs.\n\n### 4. **Observe and Compare**\n\n* Do messages get lost or duplicated?\n* Do retries behave as expected?\n* Are DLQ fallbacks triggered for unrecoverable failures?\n\nBy rigorously testing your Kafka setup under adverse conditions, you can verify that your configuration not only looks good on paperâ€”but actually holds up under stress. This ensures peace of mind in production environments where message loss is not an option.\n\n---\n\n## Conclusion: Owning Message Reliability End-to-End\n\nKafka's durability guarantees are strong, but not infallible. Without a properly configured producer, even a routine rolling patch can lead to silent message lossâ€”something no team wants to discover after the fact.\n\nThrough this real-world failure and recovery, we learned that ensuring message safety is a shared responsibility between Kafka and the application. It requires more than just enabling replicationâ€”it demands careful attention to producer configuration, retry behavior, idempotence, DLQ design, and validation through controlled failure testing.\n\nBy:\n\n* Setting `acks=all`\n* Enabling idempotence\n* Maximizing retries with meaningful backoff\n* Using DLQs for unrecoverable cases\n\nâ€”you can confidently build systems that survive the unexpected.\n\nMessage safety isn't a default. It's a design choice. And with the right choices, Kafka becomes not just fast and scalable, but truly reliable.\n","lang":"en","allPostsInOtherLang":[{"id":"post10","lang":"ko","title":"10ì–µ ë‹¬ëŸ¬ì§œë¦¬ ì‹¤ìˆ˜ í•´ê²°í•˜ê¸°: JSpecifyì™€ NullAwayë¥¼ ì‚¬ìš©í•œ ìµœì‹  Java Null ì•ˆì „ì„±","date":"2025-09-24","section":"tech","tags":"Java, JSpecify, Null ì•ˆì „ì„±, NullPointerException, ì •ì  ë¶„ì„","thumbnail":"https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftotx4mdagz9nsegld5mr.png","description":"JSpecify ì–´ë…¸í…Œì´ì…˜ê³¼ NullAway ì •ì  ë¶„ì„ì„ ì‚¬ìš©í•˜ì—¬ í˜„ëŒ€ Java ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ NullPointerExceptionì„ ì œê±°í•˜ëŠ” ë°©ë²• í•™ìŠµ","searchKeywords":"JSpecify, NullAway, Java null ì•ˆì „ì„±, NullPointerException ë°©ì§€, ì •ì  ë¶„ì„","preview":"\nìë°” í”„ë¡œê·¸ë˜ë°ì„ ì²˜ìŒ ì‹œì‘í•œ ê°œë°œìë¶€í„° 20ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ê°œë°œìê¹Œì§€, ê²½ë ¥ì„ ë¶ˆë¬¸í•˜ê³  ê°œë°œìë“¤ì´ ê°€ì¥ ìì£¼ ë§ˆì£¼ì¹˜ëŠ” ì—ëŸ¬ëŠ” **NullPointerException**ì¼ ê²ƒì…ë‹ˆë‹¤.\n\n![Top Crash Reasons](https://dev-t"},{"id":"post22","lang":"ko","title":"Anthropic ì—”ì§€ë‹ˆì–´ë“¤ì˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ íŒ","date":"2025-06-24","section":"tech","tags":"AI","thumbnail":"https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdzzrfth8zqdnrhbj9538.png","description":"Anthropic ì—”ì§€ë‹ˆì–´ë“¤ì´ ê³µìœ í•œ ê°€ì¹˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ íŒê³¼ í†µì°°","searchKeywords":"í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, AI íŒ, Anthropic, LLM ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤","preview":"\n[youtube (AI prompt engineering: A deep dive)](https://www.youtube.com/watch?v=T9aRN5JkmL8\u0026t=2463s)ì—ì„œ Anthropic ì—”ì§€ë‹ˆì–´ë“¤ì´ ê·¸ë“¤ì˜ í”„ë¡¬í”„íŠ¸ ì‘ì„± íŒê³¼ ê²½í—˜ì„ ê³µ"},{"id":"post23","lang":"ko","title":"ì¡°ì¸ í…Œì´ë¸”ì´ ì™œ ìƒê¸°ì§€? @JoinColumnìœ¼ë¡œ í•´ê²°í•˜ëŠ” ì—°ê´€ê´€ê³„ ë§¤í•‘ì˜ ë¹„ë°€","section":"tech","thumbnail":"https://velog.velcdn.com/images/wooyong99/post/6fdebd14-5fe8-4959-b085-74edb6bc4d46/image.png","tags":"JPA","date":"2024-10-10 10:00","searchKeywords":"jpa, JoinColumn, ì™¸ë˜ í‚¤, ì™¸ë˜ í‚¤ ì œì•½ ì¡°ê±´","description":"JoinColumn","preview":"\n`@JoinColumn`ì€ ì™¸ë˜í‚¤ë¥¼ ë§¤í•‘í•  ë•Œ ì‚¬ìš©í•œë‹¤. ì¦‰, í•œ ì—”í‹°í‹°ì—ì„œ ë‹¤ë¥¸ ì—”í‹°í‹°ë¥¼ ì°¸ì¡°(ì¡°ì¸)í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” í•„ë“œë¥¼ ì§€ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤.\n\n```java\n@Entity  \n@Table(name = \"orders\")  \n@Getter\n@No"},{"id":"post29","lang":"ko","title":"ë‚´ë¶€ ë©”ì„œë“œ í˜¸ì¶œì‹œ íŠ¸ëœì­ì…˜ì´ ì ìš©ë˜ì§€ ì•ŠëŠ” ì´ìŠˆ","section":"tech","thumbnail":"https://velog.velcdn.com/images/uiurihappy/post/0c13062e-e5cb-45f0-9727-a9ef018b1ffc/image.png","tags":"transactional","date":"2024-10-01 10:00","searchKeywords":"transactional, spring, aop","description":"Transactional ë‚´ë¶€ ë©”ì„œë“œ í˜¸ì¶œì‹œ íŠ¸ëœì­ì…˜ì´ ì ìš©ë˜ì§€ ì•ŠëŠ” ì´ìŠˆ","preview":"\n# @Transactional ë™ì‘ ì›ë¦¬\n\n## AOPì™€ í”„ë¡ì‹œ íŒ¨í„´ì„ í†µí•œ íŠ¸ëœì­ì…˜ ê´€ë¦¬\n\nAOPëŠ” ì—”í„°í”„ë¼ì´ì¦ˆ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì—ì„œ ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°(OOP)ì˜ í•œê³„ë¥¼ ë³´ì™„í•´ì£¼ëŠ” ë³´ì¡°ì ì¸ í”„ë¡œê·¸ë˜ë° ê¸°ë²•ì´ë‹¤. ì´ë¥¼ í†µí•´ íŠ¸ëœì­ì…˜, ìºì‹±, ë¡œê¹… "},{"id":"post30","lang":"ko","title":"Gson ë¼ì´ë¸ŒëŸ¬ë¦¬ InaccessibleObjectException","thumbnail":"https://media.techmaster.vn/api/static/bq0a8rs51co78aldi4p0/lsRpW5hr","section":"tech","tags":"gson, Java","date":"2024-07-02 10:00","searchKeywords":"gson, java","description":"gson","preview":"\nSpring Boot 2.5.x ë²„ì „ì—ì„œ 3.2.x ë²„ì „ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ í•˜ëŠ” ê³¼ì •ì—ì„œ InaccessibleObjectExceptionì´ ë°œìƒí•˜ì˜€ë‹¤. Gson ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ìª½ì—ì„œ ë°œìƒí•œ ë¬¸ì œì˜€ëŠ”ë°, ì´ì— ëŒ€í•œ íŠ¸ëŸ¬ë¸” ìŠˆíŒ… ê³¼ì •ì„ ì •ë¦¬í•´ ë³´"},{"id":"post20","lang":"ko","title":"Fixture Monkey With Kotlin","section":"tech","thumbnail":"https://i.imgur.com/J5SIYtU.png","tags":"test","date":"2024-03-03 10:00","searchKeywords":"fixture monkey, test, fixture","description":"fixtureMonkey","preview":"\ní…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ë‹¤ë³´ë©´ í”„ë¡œë•ì…˜ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ì‹œê°„ë³´ë‹¤ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í”½ìŠ¤ì²˜ë¥¼ ë§Œë“œëŠ”ë° ë” ë§ì€ ì‹œê°„ì´ ì†Œìš”ë  ë•Œê°€ ìˆë‹¤.\n\ní…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ”ë° ì‹œê°„ì´ ë§ì´ ë“¤ê³  ë²ˆê±°ë¡œìš¸ ìˆ˜ë¡ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ìƒëµí•˜ê²Œ ë˜ê³  ê²°êµ­ ê²°í•¨ì— ì·¨ì•½í•œ ì‹œìŠ¤í…œì„ êµ¬í˜„í•  ìœ„í—˜ì´ "},{"id":"post19","lang":"ko","title":"ê°ì²´ì§€í–¥ê³¼ íƒˆ êµ­ì§€í™”","section":"tech","thumbnail":"https://i.imgur.com/e584gko.png","tags":"ê°ì²´ì§€í–¥","date":"2024-02-04 10:00","searchKeywords":"OOP","description":"ê°ì²´ì§€í–¥ê³¼ íƒˆ êµ­ì§€í™”","preview":"\nìµœê·¼ ë‘ ê¶Œì˜ ì±…ì„ ë³‘í–‰í•´ì„œ ì½ëŠ” ì¤‘ì¸ë° ì„œë¡œ ì¡°ê¸ˆ ìƒë°˜ë˜ëŠ” ë‚´ìš©ì„ ì½ê²Œ ë˜ì–´ ë‚´ ìƒê°ì„ ì •ë¦¬í•´ ë³´ëŠ” ì‹œê°„ì„ ê°€ì ¸ë³´ì•˜ë‹¤.\n\ní•œ ê¶Œì€ í ë¦¬ë„ˆ í—¤ë¥´ë§ŒìŠ¤ê°€ ì“´ '[í”„ë¡œê·¸ë˜ë¨¸ì˜ ë‡Œ](https://m.yes24.com/Goods/Detail/10591101"},{"id":"post18","lang":"ko","title":"JPA íŠ¸ëœì­ì…˜ê³¼ ì˜ì†ì„± ì»¨í…ìŠ¤íŠ¸","section":"tech","thumbnail":"https://images.velog.io/images/dnjscksdn98/post/14072bd8-850b-4d2b-8476-cb5385bbcd36/jpa.png","tags":"JPA","date":"2024-01-07 10:00","searchKeywords":"jpa, entitymanager, ì˜ì†ì„± ì»¨í…ìŠ¤íŠ¸, íŠ¸ëœì­ì…˜","description":"JPA íŠ¸ëœì­ì…˜ê³¼ ì˜ì†ì„± ì»¨í…ìŠ¤íŠ¸","preview":"\nìµœê·¼ êµ¬í˜„í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œì—ì„œ `@Transactional` ì—¬ë¶€ì— ë”°ë¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ëŠ” ë¬¸ì œë¥¼ ë§Œë‚˜ê²Œ ë˜ì—ˆë‹¤.\n\níƒ€ ì„œë¹„ìŠ¤ë¡œë¶€í„° ì†¡ì¥ ì ‘ìˆ˜ ê²°ê³¼ì— ëŒ€í•œ ì¹´í”„ì¹´ ë©”ì„¸ì§€ë¥¼ ì†Œë¹„í•œ ë‹¤ìŒ, ì†¡ì¥ ì ‘ìˆ˜ì— ì‹¤íŒ¨í–ˆë‹¤ë©´ íƒë°° ë“±ë¡ ì—¬ë¶€ë¥¼ ì‹¤íŒ¨ë¡œ ë³€ê²½í•˜ëŠ” ë¡œ"},{"id":"post17","lang":"ko","title":"GitHub Actionsë¥¼ í†µí•´ CI/CD êµ¬ì¶•í•˜ê¸° (feat. Docker, Jib)","thumbnail":"https://miro.medium.com/max/1400/1*DmFbJvnRIiQIyi5xBuIXlQ.png","section":"tech","tags":"tech, í”„ë¡œì íŠ¸, CI/CD, Docker","date":"2022-12-06 10:00","preview":"\nì €í¬ `Text Me` ì„œë¹„ìŠ¤ì˜ ë² íƒ€ ë²„ì „ì´ ë°°í¬ë˜ê³  ë‚œ ë’¤ì—, ì‚¬ìš©ìë“¤ë¡œ ë¶€í„° ìˆ˜ë§ì€ í”¼ë“œë°±ì„ ë°›ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. \nì‚¬ìš©ì í”¼ë“œë°±ì„ ë¹ ë¥´ê²Œ ë°˜ì˜í•˜ë‹¤ ë³´ë‹ˆ í”„ë¡œì íŠ¸ì˜ ë¹Œë“œ ë° ë°°í¬ ì£¼ê¸°ê°€ ì§§ì•„ì¡Œê³  ì´ëŸ¬í•œ ê³¼ì •ì´ ì„œì„œíˆ ë²ˆê±°ë¡­ê²Œ ëŠê»´ì§€ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤"},{"id":"post16","lang":"ko","title":"ubuntu 18.04ì— Docker ì„¤ì¹˜í•˜ê¸°","section":"tech","thumbnail":"https://www.docker.com/wp-content/uploads/2021/09/Docker-build.png","tags":"infra","date":"2022-12-03 10:00","preview":"\ní”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´ì„œ ë„ì»¤ í—ˆë¸Œì— ì˜¬ë¼ê°€ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ì„œ ë°°í¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ìš”êµ¬ì‚¬í•­ì´ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤. \n\nì´ë¥¼ ìœ„í•´ì„œ ubuntuí™˜ê²½ì— dockerë¥¼ ì„¤ì¹˜í–ˆë˜ ê³¼ì •ì„ ê³µìœ í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n## Dockerê°€ ì§€ì›í•˜ëŠ” OS\n\në‹¤ìŒì€ do"},{"id":"post15","lang":"ko","title":"[ì˜¤ë¸Œì íŠ¸] 13ì¥ - ì„œë¸Œí´ë˜ì‹±ê³¼ ì„œë¸Œíƒ€ì´í•‘","section":"tech","thumbnail":"https://wikibook.co.kr/images/cover/m/9791158391409.png","tags":"ê°ì²´ì§€í–¥","date":"2022-11-29 10:00","preview":"\nìƒì†ì´ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ìš©ë„\n\n- íƒ€ì… ê³„ì¸µì„ êµ¬í˜„í•˜ëŠ” ê²ƒ\n    - ë¶€ëª¨ í´ë˜ìŠ¤\n        - ì¼ë°˜ì ì¸ ê°œë…ì„ êµ¬í˜„\n        - ë¶€ëª¨ í´ë˜ìŠ¤ëŠ” ìì‹ í´ë˜ìŠ¤ì˜ ì¼ë°˜í™”\n    - ìì‹ í´ë˜ìŠ¤\n        - íŠ¹ìˆ˜í•œ ê°œë…ì„ êµ¬í˜„\n      "},{"id":"post14","lang":"ko","title":"[ì˜¤ë¸Œì íŠ¸] 9ì¥ - ìœ ì—°í•œ ì„¤ê³„","section":"tech","thumbnail":"https://wikibook.co.kr/images/cover/m/9791158391409.png","tags":"ê°ì²´ì§€í–¥","date":"2022-11-02 10:00","preview":"\n## 1. ê°œë°©-íì‡„ ì›ì¹™ (Open-Closed Principle)\n\n- ê°œì²´ëŠ” í™•ì¥ì— ëŒ€í•´ ì—´ë ¤ìˆì–´ì•¼ í•˜ê³ , ìˆ˜ì •ì— ëŒ€í•´ì„œëŠ” ë‹«í˜€ ìˆì–´ì•¼í•œë‹¤.\n    - í™•ì¥ì— ì—´ë ¤ìˆë‹¤.\n        - ìƒˆë¡œìš´ ë™ì‘ì„ ì¶”ê°€í•´ì„œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì˜ ê¸°ëŠ¥ì„ í™•ì¥í•  ìˆ˜ "},{"id":"post13","lang":"ko","title":"ì–´ë…¸í…Œì´ì…˜ ê¸°ë°˜ MVCë¡œ ë¦¬íŒ©í„°ë§í•˜ê¸° - MVC 2í¸","thumbnail":"https://i.imgur.com/b9vvtK7.png","section":"tech","tags":"Spring, MVC","date":"2022-10-17 10:00","preview":"\nì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” [ì´ì „ í¬ìŠ¤íŠ¸](https://headf1rst.github.io/TIL/mvc1)ì—ì„œ êµ¬í˜„í•œ MVC í”„ë ˆì„ì›Œí¬ë¥¼ \nì–´ë…¸í…Œì´ì…˜ ê¸°ë°˜ì˜ MVCë¡œ ì ì§„ì ìœ¼ë¡œ ë¦¬íŒ©í† ë§í•´ ë‚˜ê°€ëŠ” ê³¼ì •ì— ëŒ€í•´ì„œ ë‹¤ë¤„ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n## 1. ë¶ˆí¸í•¨ì„ ê°ì§€"},{"id":"post12","lang":"ko","title":"MVC í”„ë ˆì„ì›Œí¬ ë§Œë“¤ê¸° - MVC 1í¸","thumbnail":"https://i.imgur.com/b9vvtK7.png","section":"tech","tags":"Spring, MVC","date":"2022-10-08 10:00","preview":"\n7ì›”ì— [ë„¥ìŠ¤íŠ¸ ìŠ¤í…](https://edu.nextstep.camp/)ì—ì„œ ì§„í–‰í•˜ëŠ” [ë§Œë“¤ë©´ì„œ ë°°ìš°ëŠ” ìŠ¤í”„ë§ 3ê¸°](https://edu.nextstep.camp/s/I7LCaCf3)ì— ì°¸ì—¬í•˜ì˜€ìŠµë‹ˆë‹¤.\n\nì´ í¬ìŠ¤íŠ¸ëŠ” í•´ë‹¹ ê³¼ì •ì—ì„œ ìŠ¤ìŠ¤ë¡œ ê³ ë¯¼í•˜ë©° "},{"id":"post11","lang":"ko","title":"[ì˜¤ë¸Œì íŠ¸] 7ì¥ - ê°ì²´ ë¶„í•´","section":"tech","thumbnail":"https://wikibook.co.kr/images/cover/m/9791158391409.png","tags":"ê°ì²´ì§€í–¥","date":"2022-10-02 10:00","preview":"\nëª¨ë“  í”„ë¡œê·¸ë˜ë° íŒ¨ëŸ¬ë‹¤ì„ì€ ì¶”ìƒí™”ì™€ ë¶„í•´ì˜ ê´€ì ì—ì„œ ì„¤ëª… ê°€ëŠ¥\n\n## ì¶”ìƒí™” ë©”ì»¤ë‹ˆì¦˜\nì‹œìŠ¤í…œì„ ë¶„í•´í•˜ëŠ” ë°©ë²•ì„ í”„ë¡œì‹œì €ì™€ ë°ì´í„° ì¶”ìƒí™”ì¤‘ í•˜ë‚˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ì—¬ ê²°ì •í•´ì•¼í•œë‹¤.\n\n- 1. í”„ë¡œì‹œì € ì¶”ìƒí™”\n    - ì†Œí”„íŠ¸ì›¨ì–´ê°€ ë¬´ì—‡ì„ **í•´ì•¼**í•˜ëŠ”ì§€ "},{"id":"post9","lang":"ko","title":"ë‹¤ì¤‘ ìš”ì²­ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPool ì ìš©í•˜ê¸°","thumbnail":"https://i.imgur.com/mHibXLP.jpg","section":"tech","tags":"Spring, ThreadPool","date":"2022-09-20 10:00","preview":"\ní”„ë ˆì„ì›Œí¬ëŠ” ê°œë°œìê°€ ì‰½ê³  í¸í•˜ê²Œ ê°œë°œì„ í•  ìˆ˜ ìˆë„ë¡ ë§ì€ ê¸°ìˆ ì„ ì¶”ìƒí™”í•´ì„œ ì œê³µí•œë‹¤.\n\nìŠ¤í”„ë§ ë˜í•œ ë§ì€ ë¶€ë¶„ì´ ì¶”ìƒí™” ë˜ì—ˆìœ¼ë©° ê°œë°œì ìŠ¤ìŠ¤ë¡œê°€ ì˜ë¬¸ì„ ê°–ì§€ ì•ŠëŠ”ë‹¤ë©´, ëª¨ë¥¸ì±„ ë„˜ì–´ê°ˆ ê¸°ìˆ ë“¤ì´ ì—¬ëŸ¿ ì¡´ì¬í•œë‹¤.\n\nì˜¤ëŠ˜ì€ ê·¸ëŸ¬í•œ ê¸°ìˆ ë“¤ ì¤‘, ê°œë°œìë“¤ì„"},{"id":"post8","lang":"ko","title":"[ì˜¤ë¸Œì íŠ¸] 5ì¥ - ì±…ì„ í• ë‹¹í•˜ê¸°","section":"tech","thumbnail":"https://wikibook.co.kr/images/cover/m/9791158391409.png","tags":"ê°ì²´ì§€í–¥","date":"2022-09-19 10:00","preview":"\n## ì±…ì„ ì¤‘ì‹¬ ì„¤ê³„\n\n- ì–´ë–¤ ê°ì²´ì—ê²Œ ì–´ë˜ ì±…ì„ì„ í• ë‹¹í• ì§€ ê²°ì •í•´ì•¼í•œë‹¤\n- ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë‹¤ì–‘í•œ ì±…ì„ í• ë‹¹ ë°©ë²•ì´ ì¡´ì¬í•˜ë©° ì¼ì¢…ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ í™œë™ì´ë‹¤.\n- ìƒí™©ê³¼ ë¬¸ë§¥ì— ë”°ë¼ ìµœì ì˜ ì±…ì„ í• ë‹¹ ë°©ë²•ì„ ì„ íƒí•´ì•¼í•œë‹¤.\n\n- ì±…ì„ ì¤‘ì‹¬ ì„¤ê³„ë¥¼ ìœ„"},{"id":"post7","lang":"ko","title":"[ì˜¤ë¸Œì íŠ¸] 4ì¥ - ì„¤ê³„ í’ˆì§ˆê³¼ íŠ¸ë ˆì´ë“œì˜¤í”„","thumbnail":"https://wikibook.co.kr/images/cover/m/9791158391409.png","section":"tech","tags":"ê°ì²´ì§€í–¥","date":"2022-09-12 10:00","preview":"\nì¢‹ì€ ê°ì²´ì§€í–¥ ì„¤ê³„ë€ ì˜¬ë°”ë¥¸ ê°ì²´ì—ê²Œ ì˜¬ë°”ë¥¸ ì±…ì„ì„ í• ë‹¹í•˜ë©´ì„œ ìº¡ìŠí™”ë¥¼ í†µí•´ ë‚®ì€ ê²°í•©ë„ì™€ ë†’ì€ ì‘ì§‘ë„ë¥¼ ê°€ì§„ êµ¬ì¡°ë¥¼ ì°½ì¡°í•˜ëŠ” ê²ƒ\n\n- êµ¬í˜„\n    - ë³€ê²½ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ë¶€ë¶„\n- ì¸í„°í˜ì´ìŠ¤\n    - ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ë¶€ë¶„\n\n- ë³€ê²½ì˜ ì •ë„ì— "},{"id":"post6","lang":"ko","title":"[ì˜¤ë¸Œì íŠ¸] 3ì¥ - ì—­í• , ì±…ì„, í˜‘ë ¥","section":"tech","thumbnail":"https://wikibook.co.kr/images/cover/m/9791158391409.png","tags":"ê°ì²´ì§€í–¥","date":"2022-09-05 10:00","preview":"\n- ê°ì²´ì§€í–¥ì˜ ë³¸ì§ˆ : í˜‘ë ¥í•˜ëŠ” ê°ì²´ë“¤ì˜ ê³µë™ì²´ë¥¼ ì°½ì¡°í•˜ëŠ” ê²ƒ\n    - ê¸°ëŠ¥ êµ¬í˜„ì„ ìœ„í•´ ì–´ë–¤ í˜‘ë ¥ì´ í•„ìš”í•˜ê³  í˜‘ë ¥ì„ ìœ„í•´ ì–´ë–¤ ì—­í• , ì±…ì„ì´ í•„ìš”í•œì§€ íŒŒì•…\n\n- ê°ì²´ë“¤ì€ ë©”ì‹œì§€ë¥¼ ì£¼ê³  ë°›ìœ¼ë©° í˜‘ë ¥í•œë‹¤\n\n- `í˜‘ë ¥`\n    - ì–´í”Œë¦¬ì¼€ì´ì…˜ ê¸°ëŠ¥ êµ¬"},{"id":"post5","lang":"ko","title":"[ì˜¤ë¸Œì íŠ¸] 2ì¥ - ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°","section":"tech","thumbnail":"https://wikibook.co.kr/images/cover/m/9791158391409.png","tags":"ê°ì²´ì§€í–¥","date":"2022-08-29 10:00","preview":"\ní´ë˜ìŠ¤ë¥¼ ë¨¼ì € ê²°ì •í•˜ê³ , ì–´ë–¤ `ì†ì„±`ê³¼ `ë©”ì„œë“œ`ê°€ í•„ìš”í•œì§€ ê³ ë¯¼í•˜ëŠ”ê²ƒë¦¬ ì•„ë‹ˆë¼ `ê°ì²´`ì— ì´ˆì ì„ ë§ì¶°ì•¼í•œë‹¤\n\n1. ì–´ë–¤ í´ë˜ìŠ¤ê°€ í•„ìš”í•œì§€ ì´ì „ì— ì–´ë–¤ ê°ì²´ê°€ í•„ìš”í•œì§€ ê³ ë¯¼í•˜ë¼\n   í´ë˜ìŠ¤ëŠ” ê³µí†µì ì¸ ìƒíƒœ, í–‰ë™ì„ ê³µìœ í•˜ëŠ” ê°ì²´ë¥¼ ì¶”ìƒí™”í•œê²ƒ\n\n2."},{"id":"post4","lang":"ko","title":"[ì˜¤ë¸Œì íŠ¸] 1ì¥ - ê°ì²´, ì„¤ê³„","section":"tech","thumbnail":"https://wikibook.co.kr/images/cover/m/9791158391409.png","tags":"ê°ì²´ì§€í–¥","date":"2022-08-22 10:00","preview":"\n`íŒ¨ëŸ¬ë‹¤ì„` - í•œ ì‹œëŒ€ì˜ ì‚¬íšŒ ì „ì²´ê°€ ê³µìœ í•˜ëŠ” ì´ë¡  í˜¹ì€ ë°©ë²•.\nì ˆì°¨í˜• â†’ ê°ì²´ì§€í–¥ìœ¼ë¡œ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ë§ì•˜ë‹¤.\n\ní”„ë¡œê·¸ë˜ë° íŒ¨ëŸ¬ë‹¤ì„ì€ ê³¼ê±°ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ íê¸°ì‹œí‚¤ëŠ” í˜ëª…ì  íŒ¨ëŸ¬ë‹¤ì„ì´ ì•„ë‹ˆë¼ ê³¼ê±°ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ê°œì„ í•˜ëŠ” `ë°œì „ì  íŒ¨ëŸ¬ë‹¤ì„`ì´ë‹¤.\n\nê°ì²´"},{"id":"post3","lang":"ko","title":"ë‚´ê°€ ë¸”ë¡œê·¸ë¥¼ ìƒˆë¡œ ì‹œì‘í•˜ëŠ” ì´ìœ ","thumbnail":"https://i.imgur.com/HbkinoQ.jpg","section":"tech","tags":"íšŒê³ ","date":"2022-08-02 10:00","preview":"\n## ì•µë¬´ìƒˆì‹ ë¸”ë¡œê·¸ ê¸€\n\nì²˜ìŒ ë¸”ë¡œê·¸ë¥¼ ì‹œì‘í•˜ëŠ” ê²½ìš°ì— `TIL (Today I Learned)` ì„ ëª©ì ìœ¼ë¡œ í•˜ë£¨ì— í•˜ë‚˜ì˜ í¬ìŠ¤íŒ…ì„ í•˜ëŠ” ê²½ìš°ë¥¼ ì£¼ìœ„ì—ì„œ ë§ì´ ë´¤ê³  ë‚˜ ë˜í•œ ê·¸ë¬ë‹¤.\n\nê·¸ ë‹¹ì‹œì—ëŠ” í•˜ë‚˜ì˜ í¬ìŠ¤íŒ…ì„ ë§ˆì¹˜ê³  ë‚˜ë©´ ë§ˆì¹˜ í•´ë‹¹ ì£¼ì œì— ëŒ€"},{"id":"post2","lang":"ko","title":"Static ë³€ìˆ˜ ì €ì¥ìœ„ì¹˜ì™€ JVM êµ¬ì¡°ì˜ ë³€í™”","section":"tech","thumbnail":"https://i.imgur.com/5AJJwhh.png","tags":"Java, JVM","date":"2022-07-11 10:00","searchKeywords":"ìë°”, jvm, ì •ì  ë³€ìˆ˜","description":"Static ë³€ìˆ˜ ì €ì¥ìœ„ì¹˜ì™€ JVM êµ¬ì¡°ì˜ ë³€í™”","preview":"\nStatic í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ì  ë³€ìˆ˜ì™€ ì •ì  ë©”ì„œë“œë¥¼ ë§Œë“¤ìˆ˜ ìˆëŠ”ë°, ì´ë“¤ì„ ì •ì  ë©¤ë²„ (í˜¹ì€ í´ë˜ìŠ¤ ë©¤ë²„) ë¼ê³  í•©ë‹ˆë‹¤.\n\n```java\nclass Lesson {\n\t\tstatic int score = 0;\n\t\tstatic String grad"},{"id":"post1","lang":"ko","title":"CORS, ì•Œê³ ë³´ë‹ˆ ìš°ë¦¬í¸?","section":"tech","thumbnail":"https://user-images.githubusercontent.com/85024598/236966566-6127653c-7540-485e-859f-01240e7e7154.png","tags":"í”„ë¡œì íŠ¸","date":"2022-05-26 10:00","preview":"\nServer Side Template ë°©ì‹ì´ ì•„ë‹Œ í”„ë¡ íŠ¸ì™€ ë°±ìœ¼ë¡œ ë‚˜ëˆ ì„œ API í†µì‹ ì„ í•˜ëŠ”\ní”„ë¡œì íŠ¸ì˜ ê²½ìš°, ì—´ì— ì•„í™‰ì€ ë§Œë‚˜ê²Œ ë˜ëŠ”ê²Œ ë°”ë¡œ `CORS` ì…ë‹ˆë‹¤.\n\nì•„ë‹ˆë‚˜ ë‹¤ë¥¼ê¹Œ í˜„ì¬ ì§„í–‰ì¤‘ì¸ í”„ë¡œì íŠ¸ì—ì„œë„ CORS ê´€ë ¨ ì´ìŠˆê°€ ì˜¬ë¼ì™”ìŠµë‹ˆë‹¤.\n\n!"}]},"__N_SSG":true},"page":"/[lang]/[id]","query":{"lang":"en","id":"post1"},"buildId":"cw6uK4UnfZBis5R_g8eEP","assetPrefix":"/log","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>